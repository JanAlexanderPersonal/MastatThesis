@article{Maninis2018,
abstract = {This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/{\~{}}cvlsegmentation/dextr/.},
archivePrefix = {arXiv},
arxivId = {1711.09081},
author = {Maninis, K. K. and Caelles, S. and Pont-Tuset, J. and {Van Gool}, L.},
doi = {10.1109/CVPR.2018.00071},
eprint = {1711.09081},
file = {:home/jan/Downloads/1711.09081.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {616--625},
title = {{Deep Extreme Cut: From Extreme Points to Object Segmentation}},
year = {2018}
}
@article{Alam2014,
abstract = {Study Design: A cross-sectional study. Purpose: To describe the characteristics of lumbar vertebrae of Pakistani patients reporting at a tertiary care hospital and compare with studies from other populations. Overview of Literature: Several studies have been conducted to determine morphometry of lumbar vertebrae. Most of the studies involve Caucasian populations, still data on other populations still sparse. This is the first study describing lumbar morphometry of a Pakistani population. Methods: An observational study was conducted based on a review of thin-cut (3 mm) computed topographic images of lumbar vertebrae. Two-hundred and twenty vertebrae from forty-nine patients were studied, and various dimensions were analyzed. Results: Generally, the size of the vertebrae, vertebral canals and recesses were found to be greater in male patients. The difference was statistically significant for transverse and anteroposterior diameters of the vertebral bodies and sagittal diameter of pedicles on the left side (p {\textless}0.05). Comparison of populations revealed statistically significant differences in pedicle dimensions between Pakistani population and others. Conclusions: This study provides anatomical knowledge of the lumbar region in a sample population of Pakistan. There were significant differences in various dimensions of lumbar vertebrae between female and male patients. This would prove to be critical for performing a safe operation. {\textcopyright} 2014 by Korean Society of Spine Surgery.},
author = {Alam, Muhammad M. and Waqas, Muhammad and Shallwani, Hussain and Javed, Gohar},
doi = {10.4184/asj.2014.8.4.421},
file = {:home/jan/Downloads/asj-8-421.pdf:pdf},
issn = {19767846},
journal = {Asian Spine Journal},
keywords = {Lumbar vertebra,Pakistan,Pakistani population,Vertebral dimensions},
number = {4},
pages = {421--426},
title = {{Lumbar morphometry: A study of lumbar vertebrae from a Pakistani population using computed tomography scans}},
volume = {8},
year = {2014}
}
@article{sitk,
abstract = {Modern scientific endeavors increasingly require team collaborations to construct and interpret complex computational workflows. This work describes an image-analysis environment that supports the use of computational tools that facilitate reproducible research and support scientists with varying levels of software development skills. The Jupyter notebook web application is the basis of an environment that enables flexible, well-documented, and reproducible workflows via literate programming. Image-analysis software development is made accessible to scientists with varying levels of programming experience via the use of the SimpleITK toolkit, a simplified interface to the Insight Segmentation and Registration Toolkit. Additional features of the development environment include user friendly data sharing using online data repositories and a testing framework that facilitates code maintenance. SimpleITK provides a large number of examples illustrating educational and research-oriented image analysis workflows for free download from GitHub under an Apache 2.0 license: github.com/InsightSoftwareConsortium/SimpleITK-Notebooks.},
author = {Yaniv, Ziv and Lowekamp, Bradley C. and Johnson, Hans J. and Beare, Richard},
doi = {10.1007/s10278-017-0037-8},
file = {:home/jan/Downloads/Yaniv2018{\_}Article{\_}SimpleITKImage-AnalysisNoteboo.pdf:pdf},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {Image analysis,Open-source software,Python,R,Registration,Segmentation},
number = {3},
pages = {290--303},
pmid = {29181613},
publisher = {Journal of Digital Imaging},
title = {{SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research}},
volume = {31},
year = {2018}
}
@article{Vocaturo2019,
abstract = {The tumors on the skin are characterized by a high mortality rate. Research is attempting the automatic early diagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images. Automatic diagnostics provides a valid 'second opinion' to support physicians in deciding whether a skin lesion is a benign mole or a malignant melanoma. Determining effective detection methods to reduce the rate of error in diagnosis is a crucial challenge. Computer vision systems are characterized by several fundamental steps. Preprocessing is the first phase of detection and plays a fundamental role: the elimination of noise and irrelevant parts against the background of skin images to improve image quality. The purpose of this paper is to review the pre-processing approaches that can be used on skin cancer images. The current interest in the automatic analysis of images, is motivated by the possibility of being able to provide quantitative information on a lesion and to implement self diagnosis solutions.},
author = {Vocaturo, Eugenio and Zumpano, Ester and Veltri, Pierangelo},
doi = {10.1109/BIBM.2018.8621507},
file = {:home/jan/Downloads/2018{\_}Imagepre-processingincomputervisionsystemsformelanomadetection.pdf:pdf},
isbn = {9781538654880},
journal = {Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018},
keywords = {Hair Removal,Image Enhancement,Image Pre-Processing,Image Restoration,Melanoma detection},
number = {December},
pages = {2117--2124},
publisher = {IEEE},
title = {{Image pre-processing in computer vision systems for melanoma detection}},
year = {2019}
}
@misc{ECCV2020,
abstract = {Deep convolutional networks have become the go-to technique for a variety of computer vision task such as image classification, object detection, segmentation, key points detection, etc. These over-parameterized models are known to be data-hungry; tens of thousand of labelled examples are typically required. Since manual annotations are expensive, learning from “weaker” annotations (e.g. only image-level category labels to localize object instances by a bounding box) become key to expand the success of deep networks to new applications. This tutorial will provide an overview of weakly supervised learning methods in computer vision, and we will discuss the broad area of weakly supervised object recognition and its limitations of current state-of-the-art, evaluation metrics, and future ideas that will spur disruptive progress in the field of weakly supervised learning.},
author = {Bilen, Hakan and Rodrigo, Benenson and Joon oh, Seong},
booktitle = {ECCV'20 online},
title = {{ECCV 2020 Tutorial on Weakly-Supervised Learning in Computer Vision}},
url = {https://github.com/hbilen/wsl-eccv20.github.io},
year = {2020}
}
@article{Khalid2018,
abstract = {Introduction. Blood pressure (BP) has been a potential risk factor for cardiovascular diseases. BP measurement is one of the most useful parameters for early diagnosis, prevention, and treatment of cardiovascular diseases. At present, BP measurement mainly relies on cuff-based techniques that cause inconvenience and discomfort to users. Although some of the present prototype cuffless BP measurement techniques are able to reach overall acceptable accuracies, they require an electrocardiogram (ECG) and a photoplethysmograph (PPG) that make them unsuitable for true wearable applications. Therefore, developing a single PPG-based cuffless BP estimation algorithm with enough accuracy would be clinically and practically useful. Methods. The University of Queensland vital sign dataset (online database) was accessed to extract raw PPG signals and its corresponding reference BPs (systolic BP and diastolic BP). The online database consisted of PPG waveforms of 32 cases from whom 8133 (good quality) signal segments (5 s for each) were extracted, preprocessed, and normalised in both width and amplitude. Three most significant pulse features (pulse area, pulse rising time, and width 25{\%}) with their corresponding reference BPs were used to train and test three machine learning algorithms (regression tree, multiple linear regression (MLR), and support vector machine (SVM)). A 10-fold cross-validation was applied to obtain overall BP estimation accuracy, separately for the three machine learning algorithms. Their estimation accuracies were further analysed separately for three clinical BP categories (normotensive, hypertensive, and hypotensive). Finally, they were compared with the ISO standard for noninvasive BP device validation (average difference no greater than 5 mmHg and SD no greater than 8 mmHg). Results. In terms of overall estimation accuracy, the regression tree achieved the best overall accuracy for SBP (mean and SD of difference:-0.1 ± 6.5 mmHg) and DBP (mean and SD of difference:-0.6 ± 5.2 mmHg). MLR and SVM achieved the overall mean difference less than 5 mmHg for both SBP and DBP, but their SD of difference was {\textgreater}8 mmHg. Regarding the estimation accuracy in each BP categories, only the regression tree achieved acceptable ISO standard for SBP (-1.1 ± 5.7 mmHg) and DBP (-0.03 ± 5.6 mmHg) in the normotensive category. MLR and SVM did not achieve acceptable accuracies in any BP categories. Conclusion. This study developed and compared three machine learning algorithms to estimate BPs using PPG only and revealed that the regression tree algorithm was the best approach with overall acceptable accuracy to ISO standard for BP device validation. Furthermore, this study demonstrated that the regression tree algorithm achieved acceptable measurement accuracy only in the normotensive category, suggesting that future algorithm development for BP estimation should be more specific for different BP categories.},
author = {Khalid, Syed Ghufran and Zhang, Jufen and Chen, Fei and Zheng, Dingchang},
doi = {10.1155/2018/1548647},
file = {:home/jan/Downloads/JHE2018-1548647.pdf:pdf},
issn = {20402309},
journal = {Journal of Healthcare Engineering},
pmid = {30425819},
publisher = {Hindawi},
title = {{Blood Pressure Estimation Using Photoplethysmography Only: Comparison between Different Machine Learning Approaches}},
volume = {2018},
year = {2018}
}
@article{Chu2015,
abstract = {In this paper, we address the problems of fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images. We propose a learning-based, unified random forest regression and classification framework to tackle these two problems. More specifically, in the first stage, the localization of 3D vertebral bodies is solved with random forest regression where we aggregate the votes from a set of randomly sampled image patches to get a probability map of the center of a target vertebral body in a given image. The resultant probability map is then further regularized by Hidden Markov Model (HMM) to eliminate potential ambiguity caused by the neighboring vertebral bodies. The output from the first stage allows us to define a region of interest (ROI) for the segmentation step, where we use random forest classification to estimate the likelihood of a voxel in the ROI being foreground or background. The estimated likelihood is combined with the prior probability, which is learned from a set of training data, to get the posterior probability of the voxel. The segmentation of the target vertebral body is then done by a binary thresholding of the estimated probability. We evaluated the present approach on two openly available datasets: 1) 3D T2-weighted spine MR images from 23 patients and 2) 3D spine CT images from 10 patients. Taking manual segmentation as the ground truth (each MR image contains at least 7 vertebral bodies from T11 to L5 and each CT image contains 5 vertebral bodies from L1 to L5), we evaluated the present approach with leave-one-out experiments. Specifically, for the T2-weighted MR images, we achieved for localization a mean error of 1.6 mm, and for segmentation a mean Dice metric of 88.7{\%} and a mean surface distance of 1.5 mm, respectively. For the CT images we achieved for localization a mean error of 1.9 mm, and for segmentation a mean Dice metric of 91.0{\%} and a mean surface distance of 0.9 mm, respectively.},
author = {Chu, Chengwen and Belav{\'{y}}, Daniel L. and Armbrecht, Gabriele and Bansmann, Martin and Felsenberg, Dieter and Zheng, Guoyan},
doi = {10.1371/journal.pone.0143327},
editor = {Pham, Dzung},
issn = {19326203},
journal = {PLoS ONE},
month = {nov},
number = {11},
pages = {e0143327},
pmid = {26599505},
title = {{Fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images via a learning-based method}},
url = {https://dx.plos.org/10.1371/journal.pone.0143327},
volume = {10},
year = {2015}
}
@article{Korez2015,
abstract = {Automated and semi-automated detection and segmentation of spinal and vertebral structures from computed tomography (CT) images is a challenging task due to a relatively high degree of anatomical complexity, presence of unclear boundaries and articulation of vertebrae with each other, as well as due to insufficient image spatial resolution, partial volume effects, presence of image artifacts, intensity variations and low signal-to-noise ratio. In this paper, we describe a novel framework for automated spine and vertebrae detection and segmentation from 3-D CT images. A novel optimization technique based on interpolation theory is applied to detect the location of the whole spine in the 3-D image and, using the obtained location of the whole spine, to further detect the location of individual vertebrae within the spinal column. The obtained vertebra detection results represent a robust and accurate initialization for the subsequent segmentation of individual vertebrae, which is performed by an improved shape-constrained deformable model approach. The framework was evaluated on two publicly available CT spine image databases of 50 lumbar and 170 thoracolumbar vertebrae. Quantitative comparison against corresponding reference vertebra segmentations yielded an overall mean centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6{\%} for vertebra detection, and an overall mean symmetric surface distance of 0.3 mm and Dice coefficient of 94.6{\%} for vertebra segmentation. The results indicate that by applying the proposed automated detection and segmentation framework, vertebrae can be successfully detected and accurately segmented in 3-D from CT spine images.},
author = {Korez, R and Ibragimov, B and Likar, B and Pernu{\v{s}}, F and Vrtovec, T},
doi = {10.1109/TMI.2015.2389334},
issn = {1558-254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Three-Dimensional;Male;Middle Aged;Spine;Tomograp,X-Ray Computed;Young Adult,bone;computerised tomography;image segmentation;in},
month = {aug},
number = {8},
pages = {1649--1662},
title = {{A Framework for Automated Spine and Vertebrae Interpolation-Based Detection and Model-Based Segmentation}},
volume = {34},
year = {2015}
}
@article{Ibragimov2014,
abstract = {In this paper, we propose a novel approach to landmark-based shape representation that is based on transportation theory, where landmarks are considered as sources and destinations, all possible landmark connections as roads, and established landmark connections as goods transported via these roads. Landmark connections, which are selectively established, are identified through their statistical properties describing the shape of the object of interest, and indicate the least costly roads for transporting goods from sources to destinations. From such a perspective, we introduce three novel shape representations that are combined with an existing landmark detection algorithm based on game theory. To reduce computational complexity, which results from the extension from 2-D to 3-D segmentation, landmark detection is augmented by a concept known in game theory as strategy dominance. The novel shape representations, game-theoretic landmark detection and strategy dominance are combined into a segmentation framework that was evaluated on 3-D computed tomography images of lumbar vertebrae and femoral heads. The best shape representation yielded symmetric surface distance of 0.75 mm and 1.11 mm, and Dice coefficient of 93.6{\%} and 96.2{\%} for lumbar vertebrae and femoral heads, respectively. By applying strategy dominance, the computational costs were further reduced for up to three times.},
author = {Ibragimov, B and Likar, B and Pernu{\v{s}}, F and Vrtovec, T},
doi = {10.1109/TMI.2013.2296976},
issn = {1558-254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Three-Dimensional;Lumbar Vertebrae;Tomography,X-Ray Computed,bone;computerised tomography;diseases;game theory;},
month = {apr},
number = {4},
pages = {861--874},
title = {{Shape Representation for Efficient Landmark-Based Segmentation in 3-D}},
volume = {33},
year = {2014}
}
@article{10.1371/journal.pone.0143327,
abstract = {In this paper, we address the problems of fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images. We propose a learning-based, unified random forest regression and classification framework to tackle these two problems. More specifically, in the first stage, the localization of 3D vertebral bodies is solved with random forest regression where we aggregate the votes from a set of randomly sampled image patches to get a probability map of the center of a target vertebral body in a given image. The resultant probability map is then further regularized by Hidden Markov Model (HMM) to eliminate potential ambiguity caused by the neighboring vertebral bodies. The output from the first stage allows us to define a region of interest (ROI) for the segmentation step, where we use random forest classification to estimate the likelihood of a voxel in the ROI being foreground or background. The estimated likelihood is combined with the prior probability, which is learned from a set of training data, to get the posterior probability of the voxel. The segmentation of the target vertebral body is then done by a binary thresholding of the estimated probability. We evaluated the present approach on two openly available datasets: 1) 3D T2-weighted spine MR images from 23 patients and 2) 3D spine CT images from 10 patients. Taking manual segmentation as the ground truth (each MR image contains at least 7 vertebral bodies from T11 to L5 and each CT image contains 5 vertebral bodies from L1 to L5), we evaluated the present approach with leave-one-out experiments. Specifically, for the T2-weighted MR images, we achieved for localization a mean error of 1.6 mm, and for segmentation a mean Dice metric of 88.7{\%} and a mean surface distance of 1.5 mm, respectively. For the CT images we achieved for localization a mean error of 1.9 mm, and for segmentation a mean Dice metric of 91.0{\%} and a mean surface distance of 0.9 mm, respectively.},
author = {Chu, Chengwen and Belav{\'{y}}, Daniel L and Armbrecht, Gabriele and Bansmann, Martin and Felsenberg, Dieter and Zheng, Guoyan},
doi = {10.1371/journal.pone.0143327},
journal = {PLOS ONE},
number = {11},
pages = {1--22},
publisher = {Public Library of Science},
title = {{Fully Automatic Localization and Segmentation of 3D Vertebral Bodies from CT/MR Images via a Learning-Based Method}},
url = {https://doi.org/10.1371/journal.pone.0143327},
volume = {10},
year = {2015}
}
@article{Zukic2014,
author = {Zuki{\'{c}}, D{\v{z}}enan and Vlas{\'{a}}k, Ale{\v{s}} and Egger, Jan and Hoř{\'{i}}nek, Daniel and Nimsky, Christopher and Kolb, Andreas},
doi = {10.1111/cgf.12343},
journal = {Computer Graphics Forum},
title = {{Robust Detection and Segmentation for Diagnosis of Vertebral Diseases Using Routine MR Images}},
volume = {33},
year = {2014}
}
@article{Burian2019,
author = {Burian, Egon and Rohrmeier, Alexander and Schlaeger, Sarah and Dieckmeyer, Michael and Diefenbach, Maximilian and Syv{\"{a}}ri, Jan and Klupp, Elisabeth and Weidlich, Dominik and Zimmer, Claus and Rummeny, Ernst and Karampinos, Dimitrios and Kirschke, Jan and Baum, Thomas},
doi = {10.1186/s12891-019-2528-x},
journal = {BMC Musculoskeletal Disorders},
title = {{Lumbar muscle and vertebral bodies segmentation of chemical shift encoding-based water-fat MRI: The reference database MyoSegmenTUM spine}},
volume = {20},
year = {2019}
}
@phdthesis{ubsi_936,
abstract = {Die Diagnose von bestimmten Wirbels{\"{a}}ulenerkrankungen, wie z.B. Skoliose,
Spondylolisthesis oder Wirbelbr{\"{u}}che, sind Teil des Klinikalltags. H{\"{a}}ufig
werden zur Diagnose dieser Art von Erkrankungen MRT-Daten benutzt,
um zu vermeiden, dass Patienten sch{\"{a}}dlicher Strahlung, wie z.B. R{\"{o}}ntgenstrahlung,
ausgesetzt werden.

Die Entwicklung eines Segmentierungssystems f{\"{u}}r eine Reihe von Wirbeln
ist komplex. Deshalb wurde die Methode zuerst f{\"{u}}r zwei Typen von
Gehirntumoren, Glioblastoma multiforme und Hypophysenadenom, getestet.
Ein kleines Dreiecksnetz wird am ungef{\"{a}}hren Zentrum des Tumors
durch Ballon-Forces expandiert, wobei seine Struktur n{\"{a}}herungsweise sternf{\"{o}}rmig
gehalten wird. Der Datensatz wird durch diese Kr{\"{a}}fte basierend
auf den Minimum- und Maximumintensit{\"{a}}ten beim Initialisierungsschritt
implizit in ein inneres und ein {\"{a}}u{\ss}eres Segment unterteilt. Nachdem die
Segmentierung abgeschlossen ist, wird das Volumen des Tumors berechnet.

Das Segmentierungssystem f{\"{u}}r die Wirbels{\"{a}}ule benutzt einen „Bottumup“-
Ansatz zur Erkennung der Wirbel, der auf nur einer manuellen Initialisierung
basiert. Als effiziente global-zu-lokal Gl{\"{a}}ttungsbedingung
wurde eine Oberfl{\"{a}}chenunterteilungshierarchie eingef{\"{u}}hrt, die man sich
als interne Kraft vorstellen kann. Zu Beginn wurden Intensit{\"{a}}tswerte zusammen
mit „low-high“-Werten verwendet um die Ermittlung von Kanten
zu erleichtern. Aber der Kantensch{\"{a}}tzer entwickelte sich hin zu einem
Multimerkmalsansatz.

Das endg{\"{u}}ltige System benutzt einen Viola-Jones-Detektor um das Zentrum
und die ungef{\"{a}}hre Gr{\"{o}}{\ss}e von Wirbeln zu bestimmen. Dieser Ansatz
gibt dem Nutzer die M{\"{o}}glichkeit die Erkennung manuell zu korrigieren
und erm{\"{o}}glicht eine parallele Berechnung der Merkmale und Segmentierung
und stellt eine Basis f{\"{u}}r eine zuverl{\"{a}}ssige Diagnose dar.

Das System wurde an 26 lumbalen Datens{\"{a}}tzen evaluiert, welche 234
Referenzwirbel beinhalteten. Die Wirbelerkennung hat 7.1{\%} „false positives“
und 1.3{\%} „false negatives“. Der durchschnittliche Dice-Koeffizient im
Vergleich zur Handsegmentierung ist 79.3{\%} und der mittlere Abstandsfehler
betr{\"{a}}gt 1.77mm. Alle schlimmere F{\"{a}}lle der drei Erkrankungen wurde
korrekt erkannt und Fehlalarme traten selten auf – 0{\%} bei Skoliose, 3.9{\%}
bei Spondylolisthesis und 2.6{\%} bei Wirbelfrakturen.

Die Hauptvorteile dieses Systems sind die hohe Geschwindigkeit, die
robuste Handhabung von allt{\"{a}}glichen klinischen Aufnahmen und die einfache
als auch minimale Benutzerinteraktion.},
author = {Zuki{\'{c}}, D{\v{z}}enan},
keywords = {004 Informatik; MRT; Segmentation; image analysis;},
school = {Universit{\"{a}}t Siegen},
title = {{Eine effiziente Inflationsmethode zur Segmentierung von medizinischen 3D Bildern}},
url = {https://dspace.ub.uni-siegen.de/handle/ubsi/936},
year = {2015}
}
@inproceedings{Janssens2018a,
abstract = {We present a method to address the challenging problem of segmentation of lumbar vertebrae from CT images acquired with varying fields of view. Our method is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting of a localization FCN and a segmentation FCN. More specifically, in the first step we train a regression 3D FCN (we call it 'LocalizationNet') to find the bounding box of the lumbar region. After that, a 3D U-net like FCN (we call it 'Segmentation-Net') is then developed, which after training, can perform a pixel-wise multi-class segmentation to map a cropped lumber region volumetric data to its volume-wise labels. Evaluated on publicly available datasets, our method achieved an average Dice coefficient of 95.77 ± 0.81{\%} and an average symmetric surface distance of 0.37 ± 0.06 mm.},
archivePrefix = {arXiv},
arxivId = {1712.01509},
author = {Janssens, Rens and Zeng, Guodong and Zheng, Guoyan},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363715},
eprint = {1712.01509},
isbn = {9781538636367},
issn = {19458452},
keywords = {CT,Fully Convolutional Networks,Lumbar vertebrae,Segmentation},
month = {may},
pages = {893--897},
publisher = {IEEE Computer Society},
title = {{Fully automatic segmentation of lumbar vertebrae from CT images using cascaded 3D fully convolutional networks}},
volume = {2018-April},
year = {2018}
}
@article{Forsberg2017,
abstract = {The purpose of this study was to investigate the potential of using clinically provided spine label annotations stored in a single institution image archive as training data for deep learning-based vertebral detection and labeling pipelines. Lumbar and cervical magnetic resonance imaging cases with annotated spine labels were identified and exported from an image archive. Two separate pipelines were configured and trained for lumbar and cervical cases respectively, using the same setup with convolutional neural networks for detection and parts-based graphical models to label the vertebrae. The detection sensitivity, precision and accuracy rates ranged between 99.1–99.8, 99.6–100, and 98.8–99.8{\%} respectively, the average localization error ranges were 1.18–1.24 and 2.38–2.60 mm for cervical and lumbar cases respectively, and with a labeling accuracy of 96.0–97.0{\%}. Failed labeling results typically involved failed S1 detections or missed vertebrae that were not fully visible on the image. These results show that clinically annotated image data from one image archive is sufficient to train a deep learning-based pipeline for accurate detection and labeling of MR images depicting the spine. Further, these results support using deep learning to assist radiologists in their work by providing highly accurate labels that only require rapid confirmation.},
author = {Forsberg, Daniel and Sj{\"{o}}blom, Erik and Sunshine, Jeffrey L.},
doi = {10.1007/s10278-017-9945-x},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {Archive,Artificial neural networks (ANNs),Machine learning,Magnetic resonance imaging},
month = {aug},
number = {4},
pages = {406--412},
pmid = {28083827},
publisher = {Springer New York LLC},
title = {{Detection and Labeling of Vertebrae in MR Images Using Deep Learning with Clinical Annotations as Training Data}},
volume = {30},
year = {2017}
}
@article{Huang2020,
abstract = {BACKGROUND CONTEXT: Although quantitative measurements improve the assessment of disc degeneration, acquirement of quantitative measurements relies on manual segmentation on lumbar magnetic resonance images (MRIs), which may introduce subjective bias. To date, only a few semiautomatic systems have been developed to quantify important components on MRIs. PURPOSE: To develop a deep learning based program (Spine Explorer) for automated segmentation and quantification of the vertebrae and intervertebral discs on lumbar spine MRIs. STUDY DESIGN: Cross-sectional study. PATIENT SAMPLE: The study was extended on the Hangzhou Lumbar Spine Study, a population-based study of mainland Chinese with focuses on lumbar degenerative changes. From this population-based database, 50 sets lumbar MRIs were randomly selected as training dataset, and another 50 as test dataset. OUTCOME MEASURES: Regions of vertebrae and discs were manually segmented on T2W sagittal MRIs to train a convolutional neural network for automated segmentation. Intersection-over-union was calculated to evaluate segmentation performance. Computational definitions were proposed to acquire quantitative morphometric and signal measurements for lumbar vertebrae and discs. MRIs in the test dataset were automatically measured with Spine Explorer and manually with ImageJ. METHODS: Intraclass correlation coefficient (ICC) were calculated to examine inter-software agreements. Correlations between disc measurements and Pfirrmann score as well as age were examined to assess measurement validity. RESULTS: The trained Spine Explorer automatically segments and measures a lumbar MRI in half a second, with mean Intersection-over-union of 94.7{\%} and 92.6{\%} for the vertebra and disc, respectively. For both vertebra and disc measurements acquired with Spine Explorer and ImageJ, the agreements were excellent (ICC=0.81{\~{}}1.00). Disc measurements significantly correlated to Pfirrmann score, and greater age was associated with greater anterior disc bulging area (r=0.35{\~{}}0.44) and fewer signal measurements (r=−0.62{\~{}}−0.77) as automatically acquired with Spine Explorer. CONCLUSIONS: Spine Explorer is an efficient, accurate, and reliable tool to acquire comprehensive quantitative measurements for lumbar vertebra and disc. Implication of such deep learning based program can facilitate clinical studies of the lumbar spine.},
author = {Huang, Jiawei and Shen, Haotian and Wu, Jialong and Hu, Xiaojian and Zhu, Zhiwei and Lv, Xiaoqiang and Liu, Yong and Wang, Yue},
doi = {10.1016/j.spinee.2019.11.010},
issn = {18781632},
journal = {Spine Journal},
keywords = {Deep learning,Disc degeneration,II,Image processing,Level of Evidence,Lumbar spine,MRI},
month = {apr},
number = {4},
pages = {590--599},
pmid = {31759132},
publisher = {Elsevier Inc.},
title = {{Spine Explorer: a deep learning based fully automated program for efficient and reliable quantifications of the vertebrae and discs on sagittal lumbar spine MR images}},
volume = {20},
year = {2020}
}
@article{PereraTGeorgeMSGrammerGJanicakPGPascual-LeoneA2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {{doi:10.1016/j.brs.2016.03.010 Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki T S The Clinical T M S Society Consensus Review and {for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.}, Treatment Recommendations},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@misc{SemTorch76:online,
annote = {(Accessed on 02/14/2021)},
author = {Castillo, David Lacalle},
howpublished = {$\backslash$url{\{}https://pypi.org/project/SemTorch/{\}}},
month = {sep},
title = {{SemTorch {\textperiodcentered} PyPI}},
year = {2020}
}
@inproceedings{Lafarge2018,
abstract = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Deformable image registration can be time-consuming and often needs extensive parameterization to perform well on a specific application. We present a step towards a registration framework based on a three-dimensional convolutional neural network. The network directly learns transformations between pairs of three-dimensional images. The outputs of the network are three maps for the x, y, and z components of a thin plate spline transformation grid. The network is trained on synthetic random transformations, which are applied to a small set of representative images for the desired application. Training therefore does not require manually annotated ground truth deformation information. The methodology is demonstrated on public data sets of inspiration-expiration lung CT image pairs, which come with annotated corresponding landmarks for evaluation of the registration accuracy. Advantages of this methodology are its fast registration times and its minimal parameterization.},
author = {Lafarge, Maxime W. and Moeskops, Pim and Veta, Mitko and Pluim, Josien P. W. and Eppenhof, Koen A.. J.},
doi = {10.1117/12.2292443},
isbn = {9781510616370},
issn = {16057422},
month = {mar},
pages = {27},
publisher = {SPIE-Intl Soc Optical Eng},
title = {{Deformable image registration using convolutional neural networks}},
year = {2018}
}
@article{Sekuboyina2019,
abstract = {We propose an auto-encoding network architecture for point clouds (PC) capable of extracting shape signatures without supervision. Building on this, we (i) design a loss function capable of modelling data variance on PCs which are unstructured, and (ii) regularise the latent space as in a variational auto-encoder, both of which increase the auto-encoders' descriptive capacity while making them probabilistic. Evaluating the reconstruction quality of our architectures, we employ them for detecting vertebral fractures without any supervision. By learning to efficiently reconstruct only healthy vertebrae, fractures are detected as anomalous reconstructions. Evaluating on a dataset containing ∼ 1500 vertebrae, we achieve area-under-ROC curve of {\textgreater}75{\%}, without using intensity-based features.},
archivePrefix = {arXiv},
arxivId = {1907.09254},
author = {Sekuboyina, Anjany and Rempfler, Markus and Valentinitsch, Alexander and Loeffler, Maximilian and Kirschke, Jan S. and Menze, Bjoern H.},
doi = {10.1007/978-3-030-32226-7_42},
eprint = {1907.09254},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2019 - Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis.pdf:pdf},
isbn = {9783030322250},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {375--383},
title = {{Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis}},
volume = {11769 LNCS},
year = {2019}
}
@article{Jimenez-Sanchez2018,
abstract = {In this paper, we target the problem of fracture classification from clinical X-Ray images towards an automated Computer Aided Diagnosis (CAD) system. Although primarily dealing with an image classification problem, we argue that localizing the fracture in the image is crucial to make good class predictions. Therefore, we propose and thoroughly analyze several schemes for simultaneous fracture localization and classification. We show that using an auxiliary localization task, in general, improves the classification performance. Moreover, it is possible to avoid the need for additional localization annotations thanks to recent advancements in weakly-supervised deep learning approaches. Among such approaches, we investigate and adapt Spatial Transformers (ST), Self-Transfer Learning (STL), and localization from global pooling layers. We provide a detailed quantitative and qualitative validation on a dataset of 1347 femur fractures images and report high accuracy with regard to inter-expert correlation values reported in the literature. Our investigations show that i) lesion localization improves the classification outcome, ii) weakly-supervised methods improve baseline classification without any additional cost, iii) STL guides feature activations and boost performance. We plan to make both the dataset and code available.},
archivePrefix = {arXiv},
arxivId = {1809.10692},
author = {Jim{\'{e}}nez-S{\'{a}}nchez, Amelia and Kazi, Anees and Albarqouni, Shadi and Kirchhoff, Sonja and Str{\"{a}}ter, Alexandra and Biberthaler, Peter and Mateus, Diana and Navab, Nassir},
eprint = {1809.10692},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jim{\'{e}}nez-S{\'{a}}nchez et al. - 2018 - Weakly-Supervised localization and classification of proximal femur fractures.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Attention models,Deep learning,Fracture classification,Multi-task learning,Weak-supervision,X-ray},
pages = {1--7},
title = {{Weakly-Supervised localization and classification of proximal femur fractures}},
year = {2018}
}
@article{Shi2020,
abstract = {Current weakly supervised object localization and segmentation rely on class-discriminative visualization techniques to generate pseudo-labels for pixel-level training. Such visualization methods, including class activation mapping (CAM) and Grad-CAM, use only the deepest, lowest resolution convolutional layer, missing all information in intermediate layers. We propose Zoom-CAM: going beyond the last lowest resolution layer by integrating the importance maps over all activations in intermediate layers. Zoom-CAM captures fine-grained small-scale objects for various discriminative class instances, which are commonly missed by the baseline visualization methods. We focus on generating pixel-level pseudo-labels from class labels. The quality of our pseudo-labels evaluated on the ImageNet localization task exhibits more than 2.8{\%} improvement on top-1 error. For weakly supervised semantic segmentation our generated pseudo-labels improve a state of the art model by 1.1{\%}.},
archivePrefix = {arXiv},
arxivId = {2010.08644},
author = {Shi, Xiangwei and Khademi, Seyran and Li, Yunqiang and van Gemert, Jan},
eprint = {2010.08644},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2020 - Zoom-CAM Generating Fine-grained Pixel Annotations from Image Labels.pdf:pdf},
title = {{Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels}},
url = {http://arxiv.org/abs/2010.08644},
year = {2020}
}
@article{Hong2015,
abstract = {We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in PASCAL VOC dataset.},
archivePrefix = {arXiv},
arxivId = {1506.04924},
author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
eprint = {1506.04924},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Noh, Han - 2015 - Decoupled deep neural network for semi-supervised semantic segmentation.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1495--1503},
title = {{Decoupled deep neural network for semi-supervised semantic segmentation}},
volume = {2015-Janua},
year = {2015}
}
@article{Han2020,
abstract = {Automated medical report generation in spine radiology, i.e., given spinal med-ical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of arti-ficial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpo-lates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly con-ducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demon-strate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.},
archivePrefix = {arXiv},
arxivId = {2004.13577},
author = {Han, Zhongyi and Wei, Benzheng and Yin, Yilong and Li, Shuo},
eprint = {2004.13577},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2020 - Unifying neural learning and symbolic reasoning for spinal medical report generation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Adversarial training,Graph neural network,Logical reasoning,Medical image analysis,Medical report generation},
title = {{Unifying neural learning and symbolic reasoning for spinal medical report generation}},
year = {2020}
}
@article{Sekuboyina2020,
abstract = {Reliable automated processing of spinal images is expected to benefit decision-support systems for diagnosis, surgery planning, and population-based analysis on spine and bone health. Vertebral labelling and segmentation are two fundamental tasks in such an automated pipeline. Centred around these tasks, the Large Scale Vertebrae Segmentation Challenge (VerSe) was organised in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019. This work is a technical report summarising the challenge's findings. A total of 160 multi-detector CT scans closely resembling a typical spine-centred clinical setting were prepared and annotated at voxel-level by a human-machine hybrid algorithm. Both the annotation protocol and the algorithm that aided the medical experts in this annotation process are presented. Eleven fully automated algorithms of the participating teams were benchmarked on the VerSe data. A detailed performance comparison of these algorithms along with insights into their design are presented. The best-performing algorithm achieved a vertebrae identification rate of 95$\backslash${\%} and a Dice coefficient of 90{\%} on a hidden test set. As an open-call challenge, VerSe'19's annotated image data and its evaluation tools will continue to be publicly accessible through its online portal.},
archivePrefix = {arXiv},
arxivId = {2001.09193},
author = {Sekuboyina, Anjany and Bayat, Amirhossein and Husseini, Malek E. and L{\"{o}}ffler, Maximilian and Li, Hongwei and Tetteh, Giles and Kuka{\v{c}}ka, Jan and Payer, Christian and {\v{S}}tern, Darko and Urschler, Martin and Chen, Maodong and Cheng, Dalong and Lessmann, Nikolas and Hu, Yujin and Wang, Tianfu and Yang, Dong and Xu, Daguang and Ambellan, Felix and Amiranashvili, Tamaz and Ehlke, Moritz and Lamecker, Hans and Lehnert, Sebastian and Lirio, Marilia and de Olaguer, Nicol{\'{a}}s P{\'{e}}rez and Ramm, Heiko and Sahu, Manish and Tack, Alexander and Zachow, Stefan and Jiang, Tao and Ma, Xinjun and Angerman, Christoph and Wang, Xin and Wei, Qingyue and Brown, Kevin and Wolf, Matthias and Kirszenberg, Alexandre and Puybareauq, {\'{E}}lodie and Valentinitsch, Alexander and Rempfler, Markus and Menze, Bj{\"{o}}rn H. and Kirschke, Jan S.},
eprint = {2001.09193},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2020 - VerSe A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images.pdf:pdf},
number = {January},
title = {{VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images}},
url = {http://arxiv.org/abs/2001.09193},
year = {2020}
}
@article{Lu2020,
abstract = {Accurate segmentation of anatomical structures is vital for medical image analysis. The state-of-the-art accuracy is typically achieved by supervised learning methods, where gathering the requisite expert-labeled image annotations in a scalable manner remains a main obstacle. Therefore, annotation-efficient methods that permit to produce accurate anatomical structure segmentation are highly desirable. In this work, we present Contour Transformer Network (CTN), a one-shot anatomy segmentation method with a naturally built-in human-in-the-loop mechanism. We formulate anatomy segmentation as a contour evolution process and model the evolution behavior by graph convolutional networks (GCNs). Training the CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. On segmentation tasks of four different anatomies, we demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning methods. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved to surpass the fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {2012.01480},
author = {Lu, Yuhang and Zheng, Kang and Li, Weijian and Wang, Yirui and Harrison, Adam P. and Lin, Chihung and Wang, Song and Xiao, Jing and Lu, Le and Kuo, Chang Fu and Miao, Shun},
doi = {10.1109/TMI.2020.3043375},
eprint = {2012.01480},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2020 - Contour Transformer Network for One-shot Segmentation of Anatomical Structures.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Graph Convolutional Network,Human-in-the-loop,Image Segmentation,One-shot Segmentation},
number = {Xx},
pages = {1--13},
pmid = {33290215},
title = {{Contour Transformer Network for One-shot Segmentation of Anatomical Structures}},
volume = {XX},
year = {2020}
}
@article{Yang2017,
abstract = {This paper introduces Quicksilver, a fast deformable image registration method. Quicksilver registration for image-pairs works by patch-wise prediction of a deformation model based directly on image appearance. A deep encoder-decoder network is used as the prediction model. While the prediction strategy is general, we focus on predictions for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the momentum-parameterization of LDDMM, which facilitates a patch-wise prediction strategy while maintaining the theoretical properties of LDDMM, such as guaranteed diffeomorphic mappings for sufficiently strong regularization. We also provide a probabilistic version of our prediction network which can be sampled during the testing time to calculate uncertainties in the predicted deformations. Finally, we introduce a new correction network which greatly increases the prediction accuracy of an already existing prediction network. We show experimental results for uni-modal atlas-to-image as well as uni-/multi-modal image-to-image registrations. These experiments demonstrate that our method accurately predicts registrations obtained by numerical optimization, is very fast, achieves state-of-the-art registration results on four standard validation datasets, and can jointly learn an image similarity measure. Quicksilver is freely available as an open-source software.},
archivePrefix = {arXiv},
arxivId = {1703.10908},
author = {Yang, Xiao and Kwitt, Roland and Styner, Martin and Niethammer, Marc},
doi = {10.1016/j.neuroimage.2017.07.008},
eprint = {1703.10908},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2017 - Quicksilver Fast Predictive Image Registration - a Deep Learning Approach.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Brain imaging,Deep learning,Image registration},
month = {mar},
pages = {378--396},
pmid = {28705497},
title = {{Quicksilver: Fast predictive image registration – A deep learning approach}},
url = {http://arxiv.org/abs/1703.10908},
volume = {158},
year = {2017}
}
@article{Yang2018,
abstract = {Registration of multi-temporal remote sensing images has been widely applied in military and civilian fields, such as ground target identification, urban development assessment, and geographic change assessment. Ground surface change challenges feature point detection in amount and quality, which is a common dilemma faced by feature-based registration algorithms. Under severe appearance variation, detected feature points may contain a large proportion of outliers, whereas inliers may be inadequate and unevenly distributed. This paper presents a convolutional neural network (CNN) feature-based multi-temporal remote sensing image registration method with two key contributions: (i) we use a CNN to generate robust multi-scale feature descriptors and (ii) we design a gradually increasing selection of inliers to improve the robustness of feature point registration. Extensive experiments on feature matching and image registration are performed over a multi-temporal satellite image data set and a multi-temporal unmanned aerial vehicle image dataset. Our method outperforms four state-of-the-art methods in most scenarios.},
author = {Yang, Zhuoqian and Dan, Tingting and Yang, Yang},
doi = {10.1109/ACCESS.2018.2853100},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Dan, Yang - 2018 - Multi-temporal remote sensing image registration using deep convolutional features.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Remote sensing,convolutional feature,feature matching,image registration},
month = {jul},
pages = {38544--38555},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Multi-temporal remote sensing image registration using deep convolutional features}},
volume = {6},
year = {2018}
}
@article{Liao2016,
abstract = {3-D image registration, which involves aligning two or more images, is a critical step in a variety of medical applications from diagnosis to therapy. Image registration is commonly performed by optimizing an image matching metric as a cost function. However, this task is challenging due to the non-convex nature of the matching metric over the plausible registration parameter space and insufficient approaches for a robust optimization. As a result, current approaches are often customized to a specific problem and sensitive to image quality and artifacts. In this paper, we propose a completely different approach to image registration, inspired by how experts perform the task. We first cast the image registration problem as a "strategy learning" process, where the goal is to find the best sequence of motion actions (e.g. up, down, etc.) that yields image alignment. Within this approach, an artificial agent is learned, modeled using deep convolutional neural networks, with 3D raw image data as the input, and the next optimal action as the output. To cope with the dimensionality of the problem, we propose a greedy supervised approach for an end-to-end training, coupled with attention-driven hierarchical strategy. The resulting registration approach inherently encodes both a data-driven matching metric and an optimal registration strategy (policy). We demonstrate, on two 3-D/3-D medical image registration examples with drastically different nature of challenges, that the artificial agent outperforms several state-of-art registration methods by a large margin in terms of both accuracy and robustness.},
archivePrefix = {arXiv},
arxivId = {1611.10336},
author = {Liao, Rui and Miao, Shun and de Tournemire, Pierre and Grbic, Sasa and Kamen, Ali and Mansi, Tommaso and Comaniciu, Dorin},
eprint = {1611.10336},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao et al. - 2016 - An Artificial Agent for Robust Image Registration.pdf:pdf},
month = {nov},
title = {{An Artificial Agent for Robust Image Registration}},
url = {http://arxiv.org/abs/1611.10336},
year = {2016}
}
@techreport{Brunet2010,
author = {Brunet, Florent and Sarry, Laurent and R{\'{e}}my, Adrien Bartoli and Nassir, Malgouyres and Reviewers, Navab and Agapito, Lourdes and Hornegger´, Joachim Hornegger{\'{e}}tienne and M{\'{e}}min, Hornegger{\'{e}}tienne},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunet et al. - 2010 - Universit{\'{e}} d'Auvergne Contributions to Parametric Image Registration and 3D Surface Reconstruction.pdf:pdf},
title = {{Universit{\'{e}} d'Auvergne Contributions to Parametric Image Registration and 3D Surface Reconstruction}},
year = {2010}
}
@article{DeTone2016,
abstract = {We present a deep convolutional neural network for estimating the relative homography between a pair of images. Our feed-forward network has 10 layers, takes two stacked grayscale images as input, and produces an 8 degree of freedom homography which can be used to map the pixels from the first image to the second. We present two convolutional neural network architectures for HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies. We use a 4-point homography parameterization which maps the four corners from one image into the second image. Our networks are trained in an end-to-end fashion using warped MS-COCO images. Our approach works without the need for separate local feature detection and transformation estimation stages. Our deep models are compared to a traditional homography estimator based on ORB features and we highlight the scenarios where HomographyNet outperforms the traditional technique. We also describe a variety of applications powered by deep homography estimation, thus showcasing the flexibility of a deep learning approach.},
archivePrefix = {arXiv},
arxivId = {1606.03798},
author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
eprint = {1606.03798},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeTone, Malisiewicz, Rabinovich - 2016 - Deep Image Homography Estimation.pdf:pdf},
month = {jun},
title = {{Deep Image Homography Estimation}},
url = {http://arxiv.org/abs/1606.03798},
year = {2016}
}
@techreport{Lu,
abstract = {Image registration is an important task in computer vision and image processing and widely used in medical image and self-driving cars. In this paper, we reviewed popular method in deep learning for image registration, both supervised and unsupervised one.},
author = {Lu, Yiping},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu - Unknown - Deep Learning For Image Registration.pdf:pdf},
title = {{Deep Learning For Image Registration}},
url = {http://www.cvlibs.net/datasets/kitti/eval{\_}object.php.}
}
@inproceedings{Krebs2017,
abstract = {Robust image registration in medical imaging is essential for comparison or fusion of images, acquired from various perspectives, modalities or at different times. Typically, an objective function needs to be minimized assuming specific a priori deformation models and predefined or learned similarity measures. However, these approaches have difficulties to cope with large deformations or a large variability in appearance. Using modern deep learning (DL) methods with automated feature design, these limitations could be resolved by learning the intrinsic mapping solely from experience. We investigate in this paper how DL could help organ-specific (ROI-specific) deformable registration, to solve motion compensation or atlas-based segmentation problems for instance in prostate diagnosis. An artificial agent is trained to solve the task of non-rigid registration by exploring the parametric space of a statistical deformation model built from training data. Since it is difficult to extract trustworthy ground-truth deformation fields, we present a training scheme with a large number of synthetically deformed image pairs requiring only a small number of real inter-subject pairs. Our approach was tested on inter-subject registration of prostate MR data and reached a median DICE score of.88 in 2-D and.76 in 3-D, therefore showing improved results compared to state-of-the-art registration algorithms.},
author = {Krebs, Julian and Mansi, Tommaso and Delingette, Herv{\'{e}} and Zhang, Li and Ghesu, Florin C and Miao, Shun and Maier, Andreas K. and Ayache, Nicholas and Liao, Rui and Kamen, Ali},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66182-7_40},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krebs et al. - 2017 - Robust non-rigid registration through agent-based action learning.pdf:pdf},
isbn = {9783319661810},
issn = {16113349},
pages = {344--352},
title = {{Robust non-rigid registration through agent-based action learning}},
url = {https://hal.inria.fr/hal-01569447},
volume = {10433 LNCS},
year = {2017}
}
@techreport{Schwarz,
author = {Schwarz, Loren Arthur},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz - Unknown - Non-rigid Registration Using Free-form Deformations.pdf:pdf},
title = {{Non-rigid Registration Using Free-form Deformations}}
}
@article{Nguyen2017,
abstract = {Homography estimation between multiple aerial images can provide relative pose estimation for collaborative autonomous exploration and monitoring. The usage on a robotic system requires a fast and robust homography estimation algorithm. In this study, we propose an unsupervised learning algorithm that trains a Deep Convolutional Neural Network to estimate planar homographies. We compare the proposed algorithm to traditional feature-based and direct methods, as well as a corresponding supervised learning algorithm. Our empirical results demonstrate that compared to traditional approaches, the unsupervised algorithm achieves faster inference speed, while maintaining comparable or better accuracy and robustness to illumination variation. In addition, on both a synthetic dataset and representative real-world aerial dataset, our unsupervised method has superior adaptability and performance compared to the supervised deep learning method.},
archivePrefix = {arXiv},
arxivId = {1709.03966},
author = {Nguyen, Ty and Chen, Steven W. and Shivakumar, Shreyas S. and Taylor, Camillo J. and Kumar, Vijay},
eprint = {1709.03966},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2017 - Unsupervised Deep Homography A Fast and Robust Homography Estimation Model.pdf:pdf},
month = {sep},
title = {{Unsupervised Deep Homography: A Fast and Robust Homography Estimation Model}},
url = {http://arxiv.org/abs/1709.03966},
year = {2017}
}
@article{DeVos2017,
abstract = {In this work we propose a deep learning network for deformable image registration (DIRNet). The DIRNet consists of a convolutional neural network (ConvNet) regressor, a spatial transformer, and a resampler. The ConvNet analyzes a pair of fixed and moving images and outputs parameters for the spatial transformer, which generates the displacement vector field that enables the resampler to warp the moving image to the fixed image. The DIRNet is trained end-to-end by unsupervised optimization of a similarity metric between input image pairs. A trained DIRNet can be applied to perform registration on unseen image pairs in one pass, thus non-iteratively. Evaluation was performed with registration of images of handwritten digits (MNIST) and cardiac cine MR scans (Sunnybrook Cardiac Data). The results demonstrate that registration with DIRNet is as accurate as a conventional deformable image registration method with substantially shorter execution times.},
archivePrefix = {arXiv},
arxivId = {1704.06065},
author = {de Vos, Bob D. and Berendsen, Floris F. and Viergever, Max A. and Staring, Marius and I{\v{s}}gum, Ivana},
doi = {10.1007/978-3-319-67558-9_24},
eprint = {1704.06065},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Vos et al. - 2017 - End-to-End Unsupervised Deformable Image Registration with a Convolutional Neural Network.pdf:pdf},
month = {apr},
title = {{End-to-End Unsupervised Deformable Image Registration with a Convolutional Neural Network}},
url = {http://arxiv.org/abs/1704.06065 http://dx.doi.org/10.1007/978-3-319-67558-9{\_}24},
year = {2017}
}
@techreport{Li,
abstract = {We propose a novel non-rigid image registration algorithm that is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. At the same time, our method also learns FCNs for encoding the spatial transformations at the same spatial resolution of images to be registered, rather than learning coarse-grained spatial transformation information. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different resolutions with deep self-supervision through typical feedforward and backpropagation computation. Since our method simultaneously optimizes and learns spatial transformations for the image registration, our method can be directly used to register a pair of images, and the registration of a set of images is also a training procedure for FCNs so that the trained FCNs can be directly adopted to register new images by feedforward computation of the learned FCNs without any optimization. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.},
author = {Li, Hongming and Fan, Yong},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Fan - Unknown - Non-rigid image registration using fully convolutional networks with deep self-supervision.pdf:pdf},
keywords = {Non-rigid image registration,deep learning,deep self-supervision,fully convolutional networks,multi-resolution image registration,unsupervised learning},
title = {{Non-rigid image registration using fully convolutional networks with deep self-supervision}}
}
@techreport{Szeliski1994,
author = {Szeliski, Richard and Coughlan, James},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szeliski, Coughlan - 1994 - Spline-Based Image Registration.pdf:pdf},
title = {{Spline-Based Image Registration}},
year = {1994}
}
@article{Zhao2020,
abstract = {Comprehensive vertebrae tumor diagnosis (vertebrae recognition and vertebrae tumor diagnosis from MRI images) is crucial for tumor screening and preventing further metastasis. However, this task has not yet been attempted due to challenges caused by various tumor appearance, non-tumor diseases with similar appearance, irrelevant interference information, as well as diverse MRI image field of view (FOV) and/or characteristics. We purpose a discriminative dictionary-embedded network (DECIDE) that contains an elaborated enhanced-supervision recognition network (ERN) and a discerning diagnosis network (DDN). Our ERN creatively designs projection-guided dictionary learning to leverage projections of angular point coordinates onto multiple observation axes for enhanced supervision and discriminability of different vertebrae. DDN integrates a novel label consistent dictionary learning layer into a classification network to obtain more discerning sparse codes for diagnosing performance improvement. DECIDE is trained and evaluated using a very challenging dataset consisted of 600 MRI images; the evaluation results show that DECIDE achieves high performance in both recognition (accuracy: 0.928) and diagnosis (AUC: 0.96) tasks.},
author = {Zhao, Shen and Chen, Bin and Chang, Heyou and Wu, Xi and Li, Shuo},
doi = {10.1007/978-3-030-59725-2_67},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2020 - Discriminative Dictionary-Embedded Network for Comprehensive Vertebrae Tumor Diagnosis.pdf:pdf},
isbn = {9783030597245},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Dictionary embedded deep learning,Vertebrae recognition,Vertebrae tumor diagnosis},
pages = {691--701},
title = {{Discriminative Dictionary-Embedded Network for Comprehensive Vertebrae Tumor Diagnosis}},
volume = {12266 LNCS},
year = {2020}
}
@article{He2020,
abstract = {Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions' disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge show great advantages of our DeepRS that outperforms the existing state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {2008.00710},
author = {He, Yuting and Li, Tiantian and Yang, Guanyu and Kong, Youyong and Chen, Yang and Shu, Huazhong and Coatrieux, Jean-Louis and Dillenseger, Jean-Louis and Li, Shuo},
eprint = {2008.00710},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2020 - Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images.pdf:pdf},
pages = {1--17},
title = {{Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images}},
url = {http://arxiv.org/abs/2008.00710},
year = {2020}
}
@article{Lindner2017,
abstract = {We address the challenge of model transfer learning for a shape model matching (SMM) system. The goal is to adapt an existing SMM system to work effectively with new data without rebuilding the system from scratch. Recently, several SMM systems have been proposed that combine the outcome of a Random Forest (RF) regression step with shape constraints. These methods have been shown to lead to accurate and robust results when applied to the localisation of landmarks annotating skeletal structures in radiographs. However, as these methods contain a supervised learning component, their performance heavily depends on the data that was used to train the system, limiting their applicability to a new dataset with different properties. Here we show how to tune an existing SMM system by both updating the RFs with new samples and re-estimating the shape model. We demonstrate the effectiveness of tuning a cephalometric SMM system to replicate the annotation style of a new observer. Our results demonstrate that tuning an existing system leads to significant improvements in performance on new data, up to the extent of performing a well as a system that was fully rebuilt using samples from the new dataset. The proposed approach is fast and does not require access to the original training data.},
author = {Lindner, C. and Waring, D. and Thiruvenkatachari, B. and O'Brien, K. and Cootes, T. F.},
doi = {10.1007/978-3-319-66182-7_17},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindner et al. - 2017 - Adaptable landmark localisation Applying model transfer learning to a shape model matching system.pdf:pdf},
isbn = {9783319661810},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Landmark localisation,Machine learning,Model transfer learning,Model tuning,Random Forests,Statistical shape models},
pages = {144--151},
title = {{Adaptable landmark localisation: Applying model transfer learning to a shape model matching system}},
volume = {10433 LNCS},
year = {2017}
}
@article{Cai2015,
abstract = {Computer-aided diagnosis of spine problems relies on the automatic identification of spine structures in images. The task of automatic vertebra recognition is to identify the global spine and local vertebra structural information such as spine shape, vertebra location and pose. Vertebra recognition is challenging due to the large appearance variations in different image modalities/views and the high geometric distortions in spine shape. Existing vertebra recognitions are usually simplified as vertebrae detections, which mainly focuses on the identification of vertebra locations and labels but cannot support further spine quantitative assessment. In this paper, we propose a vertebra recognition method using 3D deformable hierarchical model (DHM) to achieve cross-modality local vertebra location+pose identification with accurate vertebra labeling, and global 3D spine shape recovery. We recast vertebra recognition as deformable model matching, fitting the input spine images with the 3D DHM via deformations. The 3D model-matching mechanism provides a more comprehensive vertebra location+pose+label simultaneous identification than traditional vertebra location+label detection, and also provides an articulated 3D mesh model for the input spine section. Moreover, DHM can conduct versatile recognition on volume and multi-slice data, even on single slice. Experiments show our method can successfully extract vertebra locations, labels, and poses from multi-slice T1/T2 MR and volume CT, and can reconstruct 3D spine model on different image views such as lumbar, cervical, even whole spine. The resulting vertebra information and the recovered shape can be used for quantitative diagnosis of spine problems and can be easily digitalized and integrated in modern medical PACS systems.},
author = {Cai, Yunliang and Osman, Said and Sharma, Manas and Landis, Mark and Li, Shuo},
doi = {10.1109/TMI.2015.2392054},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai et al. - 2015 - Multi-Modality Vertebra Recognition in Arbitrary Views Using 3D Deformable Hierarchical Model.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Spine recognition,vertebra detection,vertebra pose estimation,vertebra segmentation},
number = {8},
pages = {1676--1693},
pmid = {25594966},
title = {{Multi-Modality Vertebra Recognition in Arbitrary Views Using 3D Deformable Hierarchical Model}},
volume = {34},
year = {2015}
}
@article{Hong2015a,
abstract = {We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches even with much less training images with strong annotations in PASCAL VOC dataset.},
archivePrefix = {arXiv},
arxivId = {1506.04924},
author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
eprint = {1506.04924},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Noh, Han - 2015 - Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation.pdf:pdf},
month = {jun},
title = {{Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation}},
url = {http://arxiv.org/abs/1506.04924},
year = {2015}
}
@techreport{Shen,
abstract = {Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of masks with pixel level labelling, which involves a large amount of human labour and time for annotation. In contrast, web images and their image-level labels are much easier and cheaper to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method utilizes the internet to retrieve a large number of images and uses a large scale co-segmentation framework to generate masks for the retrieved images. We first retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in the dataset PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain an IoU score of 56.9 on test set of PASCAL VOC 2012, which reaches the state-of-the-art performance.},
author = {Shen, Tong and Lin, Guosheng and Liu, Lingqiao and Shen, Chunhua and Reid, Ian},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - Unknown - Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation.pdf:pdf},
title = {{Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation}}
}
@techreport{Roy,
abstract = {This paper addresses the problem of weakly supervised semantic image segmentation. Our goal is to label every pixel in a new image, given only image-level object labels associated with training images. Our problem statement differs from common semantic segmentation, where pixel-wise annotations are typically assumed available in training. We specify a novel deep architecture which fuses three distinct computation processes toward semantic segmentation-namely, (i) the bottom-up computation of neural activations in a CNN for the image-level prediction of object classes; (ii) the top-down estimation of conditional likelihoods of the CNN's activations given the predicted objects, resulting in probabilistic attention maps per object class; and (iii) the lateral attention-message passing from neighboring neurons at the same CNN layer. The fusion of (i)-(iii) is realized via a conditional random field as recurrent network aimed at generating a smooth and boundary-preserving seg-mentation. Unlike existing work, we formulate a unified end-to-end learning of all components of our deep architecture. Evaluation on the benchmark PASCAL VOC 2012 dataset demonstrates that we outperform reasonable weakly supervised baselines and state-of-the-art approaches.},
author = {Roy, Anirban and Todorovic, Sinisa},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roy, Todorovic - Unknown - Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation.pdf:pdf},
title = {{Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation}}
}
@inproceedings{Glocker,
abstract = {Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit parametric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases. {\textcopyright} 2013 Springer-Verlag.},
author = {Glocker, Ben and Zikic, Darko and Konukoglu, Ender and Haynor, David R and Criminisi, Antonio},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-40763-5_33},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - Unknown - Vertebrae Localization in Pathological Spine CT via Dense Classification from Sparse Annotations.pdf:pdf},
isbn = {9783642407628},
issn = {03029743},
number = {PART 2},
pages = {262--270},
pmid = {24579149},
title = {{Vertebrae localization in pathological spine CT via dense classification from sparse annotations}},
volume = {8150 LNCS},
year = {2013}
}
@misc{Glockera,
abstract = {This paper presents a new method for automatic localization and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81{\%}.},
author = {Glocker, Ben and Feulner, J. and Criminisi, Antonio and Haynor, D. R. and Konukoglu, E.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33454-2_73},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - Unknown - Automatic Localization and Identification of Vertebrae in Arbitrary Field-of-View CT Scans.pdf:pdf},
isbn = {9783642334535},
issn = {16113349},
pages = {590--598},
pmid = {23286179},
title = {{Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans}},
volume = {7512 LNCS},
year = {2012}
}
@inproceedings{Ilse2018,
abstract = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
archivePrefix = {arXiv},
arxivId = {1802.04712},
author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.04712},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilse, Tomczak, Welling - 2018 - Attention-based Deep Multiple Instance Learning.pdf:pdf},
isbn = {9781510867963},
month = {feb},
pages = {3376--3391},
title = {{Attention-based deep multiple instance learning}},
url = {http://arxiv.org/abs/1802.04712},
volume = {5},
year = {2018}
}
@article{Li2018,
abstract = {This work presents a deep object co-segmentation (DOCS) approach for segmenting common objects of the same class within a pair of images. This means that the method learns to ignore common, or uncommon, background stuff and focuses on objects. If multiple object classes are presented in the image pair, they are jointly extracted as foreground. To address this task, we propose a CNN-based Siamese encoder-decoder architecture. The encoder extracts high-level semantic features of the foreground objects, a mutual correlation layer detects the common objects, and finally, the decoder generates the output foreground masks for each image. To train our model, we compile a large object co-segmentation dataset consisting of image pairs from the PASCAL VOC dataset with common objects masks. We evaluate our approach on commonly used datasets for co-segmentation tasks and observe that our approach consistently outperforms competing methods, for both seen and unseen object classes.},
archivePrefix = {arXiv},
arxivId = {1804.06423},
author = {Li, Weihao and Jafari, Omid Hosseini and Rother, Carsten},
eprint = {1804.06423},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Jafari, Rother - 2018 - Deep Object Co-Segmentation.pdf:pdf},
month = {apr},
title = {{Deep Object Co-Segmentation}},
url = {http://arxiv.org/abs/1804.06423},
year = {2018}
}
@techreport{Follow,
author = {Follow, Lori Sheng},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Follow - Unknown - Multiple Instance Learning with MNIST dataset using Pytorch.pdf:pdf},
title = {{Multiple Instance Learning with MNIST dataset using Pytorch}}
}
@techreport{Carbonneau2016,
author = {Carbonneau, Marc-Andr{\'{e}} and Granger, Eric and Gagnon, Ghyslain},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Carbonneau, Granger, Gagnon - 2016 - Introduction to Multiple Instance Learning.pdf:pdf},
title = {{Introduction to Multiple Instance Learning}},
year = {2016}
}
@article{Wang2016,
abstract = {A novel interactive image cosegmentation algorithm using likelihood estimation and higher order energy optimization is proposed for extracting common foreground objects from a group of related images. Our approach introduces the higher order clique's, energy into the cosegmentation optimization process successfully. A region-based likelihood estimation procedure is first performed to provide the prior knowledge for our higher order energy function. Then, a new cosegmentation energy function using higher order cliques is developed, which can efficiently cosegment the foreground objects with large appearance variations from a group of images in complex scenes. Both the quantitative and qualitative experimental results on representative datasets demonstrate that the accuracy of our cosegmentation results is much higher than the state-of-The-Art cosegmentation methods.},
author = {Wang, Wenguan and Shen, Jianbing},
doi = {10.1109/TMM.2016.2545409},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Shen - 2016 - Higher-Order Image Co-segmentation.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Image co-segmentation,energy optimization,higher-order cliques,likelihood estimation},
month = {jun},
number = {6},
pages = {1011--1021},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Higher-Order Image Co-segmentation}},
volume = {18},
year = {2016}
}
@article{Glocker2013,
abstract = {Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit parametric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases. {\textcopyright} 2013 Springer-Verlag.},
author = {Glocker, Ben and Zikic, Darko and Konukoglu, Ender and Haynor, David R. and Criminisi, Antonio},
doi = {10.1007/978-3-642-40763-5_33},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2013 - Vertebrae localization in pathological spine CT via dense classification from sparse annotations.pdf:pdf},
isbn = {9783642407628},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {262--270},
pmid = {24579149},
title = {{Vertebrae localization in pathological spine CT via dense classification from sparse annotations}},
volume = {8150 LNCS},
year = {2013}
}
@article{Ronneberger2015a,
abstract = {Encoder-decoder networks are state-of-the-art approaches to biomedical image segmentation, but have two problems: i.e., the widely used pooling operations may discard spatial information, and therefore low-level semantics are lost. Feature fusion methods can mitigate these problems but feature maps of different scales cannot be easily fused because down- and upsampling change the spatial resolution of feature map. To address these issues, we propose INet, which enlarges receptive fields by increasing the kernel sizes of convolutional layers in steps (e.g., from 3times 3 to 7times 7 and then 15times 15 ) instead of downsampling. Inspired by an Inception module, INet extracts features by kernels of different sizes through concatenating the output feature maps of all preceding convolutional layers. We also find that the large kernel makes the network feasible for biomedical image segmentation. In addition, INet uses two overlapping max-poolings, i.e., max-poolings with stride 1, to extract the sharpest features. Fixed-size and fixed-channel feature maps enable INet to concatenate feature maps and add multiple shortcuts across layers. In this way, INet can recover low-level semantics by concatenating the feature maps of all preceding layers and expedite the training by adding multiple shortcuts. Because INet has additional residual shortcuts, we compare INet with a UNet system that also has residual shortcuts (ResUNet). To confirm INet as a backbone architecture for biomedical image segmentation, we implement dense connections on INet (called DenseINet) and compare it to a DenseUNet system with residual shortcuts (ResDenseUNet). INet and DenseINet require 16.9{\%} and 37.6{\%} fewer parameters than ResUNet and ResDenseUNet, respectively. In comparison with six encoder-decoder approaches using nine public datasets, INet and DenseINet demonstrate efficient improvements in biomedical image segmentation. INet outperforms DeepLabV3, which implementing atrous convolution instead of downsampling to increase receptive fields. INet also outperforms two recent methods (named HRNet and MS-NAS) that maintain high-resolution representations and repeatedly exchange the information across resolutions.},
archivePrefix = {arXiv},
arxivId = {2105.10559},
author = {Weng, Weihao and Zhu, Xin},
doi = {10.1109/ACCESS.2021.3053408},
eprint = {2105.10559},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation(2).pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Biomedical image,convolutional networks,encoder-decoder networks,semantic segmentation},
month = {may},
pages = {16591--16603},
title = {{INet: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {9},
year = {2021}
}
@article{Lu2018,
abstract = {The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.},
author = {Lu, Jen-Tang and Pedemonte, Stefano and Bizzo, Bernardo and Doyle, Sean and Andriole, Katherine P and Michalski, Mark H and {Gilberto Gonzalez}, R and Pomerantz, Stuart R},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2018 - DEEP SPINE AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP LE.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2018 - DEEP SPINE AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP(2).pdf:pdf},
journal = {Proceedings of Machine Learning Research},
pages = {1--16},
title = {{DEEP SPINE: AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP LEARNING}},
volume = {85},
year = {2018}
}
@inproceedings{Suzani,
abstract = {Segmentation of vertebral structures in magnetic resonance (MR) images is challenging because of poor contrast between bone surfaces and surrounding soft tissue. This paper describes a semi-automatic method for segmenting vertebral bodies in multi-slice MR images. In order to achieve a fast and reliable segmentation, the method takes advantage of the correlation between shape and pose of different vertebrae in the same patient by using a statistical multi-vertebrae anatomical shape+pose model. Given a set of MR images of the spine, we initially reduce the intensity inhomogeneity in the images by using an intensity-correction algorithm. Then a 3D anisotropic diffusion filter smooths the images. Afterwards, we extract edges from a relatively small region of the pre-processed image with a simple user interaction. Subsequently, an iterative Expectation Maximization technique is used to register the statistical multi-vertebrae anatomical model to the extracted edge points in order to achieve a fast and reliable segmentation for lumbar vertebral bodies. We evaluate our method in terms of speed and accuracy by applying it to volumetric MR images of the spine acquired from nine patients. Quantitative and visual results demonstrate that the method is promising for segmentation of vertebral bodies in volumetric MR images. {\textcopyright} 2014 SPIE.},
annote = {Data provided by Dr. Shuo Li,GE Health Research, Ontario, Canada.},
author = {Suzani, Amin and Rasoulian, Abtin and Fels, Sidney and Rohling, Robert N and Abolmaesumi, Purang},
booktitle = {Medical Imaging 2014: Image-Guided Procedures, Robotic Interventions, and Modeling},
doi = {10.1117/12.2043847},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzani et al. - Unknown - Semi-automatic Segmentation of Vertebral Bodies in Volumetric MR Images Using a Statistical ShapePose Model.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzani et al. - Unknown - Semi-automatic Segmentation of Vertebral Bodies in Volumetric MR Images Using a Statistical ShapePose Model(2).pdf:pdf},
isbn = {9780819498298},
issn = {16057422},
keywords = {Segmentation,multi-vertebrae anatomical model,vertebral body,volumetric MR image},
pages = {90360P},
title = {{Semi-automatic segmentation of vertebral bodies in volumetric MR images using a statistical shape+pose model}},
url = {https://www.researchgate.net/publication/269313420{\_}Semi-automatic{\_}segmentation{\_}of{\_}vertebral{\_}bodies{\_}in{\_}volumetric{\_}MR{\_}images{\_}using{\_}a{\_}statistical{\_}shapepose{\_}model},
volume = {9036},
year = {2014}
}
@article{Chan2020,
abstract = {Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis.},
archivePrefix = {arXiv},
arxivId = {1912.11186},
author = {Chan, Lyndon and Hosseini, Mahdi S. and Plataniotis, Konstantinos N.},
doi = {10.1007/s11263-020-01373-4},
eprint = {1912.11186},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hosseini, Plataniotis - 2020 - A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hosseini, Plataniotis - 2020 - A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains(2).pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural network,Deep learning,Digital pathology,Natural imaging,Satellite imaging,Self-supervised Learning,Weakly supervised semantic segmentation},
title = {{A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains}},
year = {2020}
}
@article{DasCLuciaMS2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {doi:10.1016/j.brs.2016.03.010 {Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki TS. The Clinical TMS Society Consensus Review and Treatment Recommendations for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@inproceedings{Nett2008,
author = {Nett, Brian and Tang, Jie and Leng, Shuai and Chen, Guang-Hong},
booktitle = {Medical Imaging 2008: Physics of Medical Imaging},
doi = {10.1117/12.771294},
editor = {Hsieh, Jiang and Samei, Ehsan},
publisher = {SPIE},
title = {{Tomosynthesis via total variation minimization reconstruction and prior image constrained compressed sensing ({\{}PICCS{\}}) on a C-arm system}},
url = {https://doi.org/10.1117/12.771294},
year = {2008}
}
@article{Hyun2018,
author = {Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
doi = {10.1088/1361-6560/aac71a},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyun et al. - 2018 - Deep learning for undersampled {\{}MRI{\}} reconstruction.pdf:pdf},
journal = {Physics in Medicine {\&} Biology},
keywords = {Reconstruction,Tomography},
mendeley-tags = {Reconstruction,Tomography},
number = {13},
pages = {135007},
publisher = {{\{}IOP{\}} Publishing},
title = {{Deep learning for undersampled {\{}MRI{\}} reconstruction}},
url = {https://doi.org/10.1088/1361-6560/aac71a},
volume = {63},
year = {2018}
}
@article{Cicek2016,
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf},
journal = {CoRR},
keywords = {Segmentation,Sparse annotation},
mendeley-tags = {Segmentation,Sparse annotation},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
volume = {abs/1606.0},
year = {2016}
}
@article{Selvaraju2016,
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1610.02391},
journal = {CoRR},
title = {{Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391},
volume = {abs/1610.0},
year = {2016}
}
@article{Vania2019,
abstract = {There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed a fully automatic segmentation method to segment the spine from CT images, and we compared our segmentation results with reference segmentations obtained by well-known methods. We use a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94$\backslash$$\backslash${\%}), Jaccard index (93$\backslash$$\backslash${\%}), volumetric similarity (96$\backslash$$\backslash${\%}), sensitivity (97$\backslash$$\backslash${\%}), specificity (99$\backslash$$\backslash${\%}), precision (over segmentation 8.3 and under segmentation 2.6), accuracy (99$\backslash$$\backslash${\%}), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.HighlightsA method to enhance the accuracy of spine segmentation from CT data was proposed.The proposed method uses Convolutional Neural Network via redundant generation of class labels.Experiments show the segmentation accuracy has been enhanced.},
author = {Vania, Malinda and Mureja, Dawit and Lee, Deukhee},
doi = {10.1016/j.jcde.2018.05.002},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vania, Mureja, Lee - 2019 - Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of c.pdf:pdf},
issn = {2288-5048},
journal = {Journal of Computational Design and Engineering},
number = {2},
pages = {224--232},
title = {{Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of class labels}},
url = {https://doi.org/10.1016/j.jcde.2018.05.002},
volume = {6},
year = {2019}
}
@article{Huang2013,
author = {Huang, Jing and Zhang, Yunwan and Ma, Jianhua and Zeng, Dong and Bian, Zhaoying and Niu, Shanzhou and Feng, Qianjin and Liang, Zhengrong and Chen, Wufan},
doi = {10.1371/journal.pone.0079709},
editor = {Wang, Ge},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2013 - Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior.pdf:pdf},
journal = {{\{}PLoS{\}} {\{}ONE{\}}},
keywords = {Tomography},
mendeley-tags = {Tomography},
month = {nov},
number = {11},
pages = {e79709},
publisher = {Public Library of Science ({\{}PLoS{\}})},
title = {{Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior}},
url = {https://doi.org/10.1371/journal.pone.0079709},
volume = {8},
year = {2013}
}
@article{Laradji2020,
abstract = {Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.06392v1},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1109/icip40778.2020.9190782},
eprint = {arXiv:1906.06392v1},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - Proposal-Based Instance Segmentation With Point Supervision.pdf:pdf},
journal = {020 IEEE International Conference on Image Processing (ICIP)},
number = {1},
pages = {2126--2130},
title = {{Proposal-Based Instance Segmentation With Point Supervision}},
year = {2020}
}
@article{Laradji2021,
abstract = {Coronavirus Disease 2019 (COVID-19) has spread aggressively across the world causing an existential health crisis. Thus, having a system that automatically detects COVID-19 in tomography (CT) images can assist in quantifying the severity of the illness. Unfortunately, labelling chest CT scans requires significant domain expertise, time, and effort. We address these labelling challenges by only requiring point annotations, a single pixel for each infected region on a CT image. This labeling scheme allows annotators to label a pixel in a likely infected region, only taking 1-3 seconds, as opposed to 10-15 seconds to segment a region. Conventionally, segmentation models train on point-level annotations using the cross-entropy loss function on these labels. However, these models often suffer from low precision. Thus, we propose a consistency-based (CB) loss function that encourages the output predictions to be consistent with spatial transformations of the input images. The experiments on 3 open-source COVID-19 datasets show that this loss function yields significant improvement over conventional point-level loss functions and almost matches the performance of models trained with full supervision with much less human effort. Code is available at: $\backslash$url{\{}https://github.com/IssamLaradji/covid19{\_}weak{\_}supervision{\}}.},
archivePrefix = {arXiv},
arxivId = {2007.02180},
author = {Laradji, Issam and Rodriguez, Pau and Ma{\~{n}}as, Oscar and Lensink, Keegan and Law, Marco and Kurzman, Lironne and Parker, William and Vazquez, David and Nowrouzezahrai, Derek},
eprint = {2007.02180},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - A Weakly Supervised Consistency-based Learning Method for COVID-19 Segmentation in CT Images.pdf:pdf},
journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = {jan},
title = {{A Weakly Supervised Consistency-based Learning Method for COVID-19 Segmentation in CT Images}},
url = {http://arxiv.org/abs/2007.02180},
year = {2021}
}
@article{McEver2020,
abstract = {Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset $\backslash$cite{\{}everingham2010pascal{\}}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.},
archivePrefix = {arXiv},
arxivId = {2007.05615},
author = {McEver, R. Austin and Manjunath, B. S.},
eprint = {2007.05615},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision(2).pdf:pdf},
title = {{PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision}},
url = {http://arxiv.org/abs/2007.05615},
year = {2020}
}
@article{Laradji2020b,
abstract = {One of the key challenges in the battle against the Coronavirus (COVID-19) pandemic is to detect and quantify the severity of the disease in a timely manner. Computed tomographies (CT) of the lungs are effective for assessing the state of the infection. Unfortunately, labeling CT scans can take a lot of time and effort, with up to 150 minutes per scan. We address this challenge introducing a scalable, fast, and accurate active learning system that accelerates the labeling of CT scan images. Conventionally, active learning methods require the labelers to annotate whole images with full supervision, but that can lead to wasted efforts as many of the annotations could be redundant. Thus, our system presents the annotator with unlabeled regions that promise high information content and low annotation cost. Further, the system allows annotators to label regions using point-level supervision, which is much cheaper to acquire than per-pixel annotations. Our experiments on open-source COVID-19 datasets show that using an entropy-based method to rank unlabeled regions yields to significantly better results than random labeling of these regions. Also, we show that labeling small regions of images is more efficient than labeling whole images. Finally, we show that with only 7{\%} of the labeling effort required to label the whole training set gives us around 90{\%} of the performance obtained by training the model on the fully annotated training set. Code is available at: https://github.com/IssamLaradji/covid19{\_}active{\_}learning.},
archivePrefix = {arXiv},
arxivId = {2007.07012},
author = {Laradji, Issam and Rodriguez, Pau and Branchaud-Charron, Frederic and Lensink, Keegan and Atighehchian, Parmida and Parker, William and Vazquez, David and Nowrouzezahrai, Derek},
eprint = {2007.07012},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - A Weakly Supervised Region-Based Active Learning Method for COVID-19 Segmentation in CT Images.pdf:pdf},
issn = {23318422},
journal = {arXiv},
month = {jul},
title = {{A Weakly Supervised Region-Based Active Learning Method for COVID-19 Segmentation in CT Images}},
url = {http://arxiv.org/abs/2007.07012},
year = {2020}
}
@article{Laradji2019,
abstract = {Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.},
archivePrefix = {arXiv},
arxivId = {1906.06392},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
eprint = {1906.06392},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2019 - Instance Segmentation with Point Supervision.pdf:pdf},
month = {jun},
title = {{Instance Segmentation with Point Supervision}},
url = {http://arxiv.org/abs/1906.06392},
year = {2019}
}
@article{Laradji2018,
abstract = {Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detection-based method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.},
archivePrefix = {arXiv},
arxivId = {1807.09856},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1007/978-3-030-01216-8_34},
eprint = {1807.09856},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2018 - Where Are the Blobs Counting by Localization with Point Supervision.pdf:pdf},
isbn = {9783030012151},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {560--576},
title = {{Where Are the Blobs: Counting by Localization with Point Supervision}},
volume = {11206 LNCS},
year = {2018}
}
@article{Janssens2018,
abstract = {We present a method to address the challenging problem of automatic segmentation of lumbar vertebrae from CT images acquired with varying fields of view. Our method is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting of a localization FCN and a segmentation FCN. More specifically, in the first step we train a regression 3D FCN (we call it “LocalizationNet”) to find the bounding box of the lumbar region. After that, a 3D U-net like FCN (we call it “SegmentationNet”) is then developed, which after training, can perform a pixel-wise multi-class segmentation to map a cropped lumber region volumetric data to its volume-wise labels. Evaluated on publicly available datasets, our method achieved an average Dice coefficient of 95.77 ± 0.81{\%} and an average symmetric surface distance of 0.37 ± 0.06 mm.},
author = {Janssens, Rens and Zheng, Guoyan},
doi = {10.29007/vt7v},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janssens, Zheng - 2018 - Deep Learning based Segmentation of Lumbar Vertebrae from CT Images.pdf:pdf},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
pages = {94--89},
title = {{Deep Learning based Segmentation of Lumbar Vertebrae from CT Images}},
volume = {2},
year = {2018}
}
@article{Chuang2019,
author = {Chuang, Cheng-Hung and Lin, Chih-Yang and Tsai, Yuan-Yu and Lian, Zhen-You and Xie, Hong-Xia and Hsu, Chih-Chao and Huang, Chung-Lin},
doi = {10.1109/access.2019.2934325},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang et al. - 2019 - Efficient Triple Output Network for Vertebral Segmentation and Identification.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
pages = {117978--117985},
publisher = {IEEE},
title = {{Efficient Triple Output Network for Vertebral Segmentation and Identification}},
volume = {7},
year = {2019}
}
@article{Lessmann2018,
abstract = {Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. The network concurrently performs multiple tasks, which are segmentation of a vertebra, regression of its anatomical label and prediction whether the vertebra is completely visible in the image, which allows to exclude incompletely visible vertebrae from further analyses. The predicted anatomical labels of the individual vertebrae are additionally refined with a maximum likelihood approach, choosing the overall most likely labeling if all detected vertebrae are taken into account. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. For vertebra segmentation, the average Dice score was 94.9 ± 2.1{\%} with an average absolute symmetric surface distance of 0.2 ± 10.1mm. The anatomical identification had an accuracy of 93{\%}, corresponding to a single case with mislabeled vertebrae. Vertebrae were classified as completely or incompletely visible with an accuracy of 97{\%}. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.},
archivePrefix = {arXiv},
arxivId = {1804.04383},
author = {Lessmann, Nikolas and van Ginneken, Bram and de Jong, Pim A. and I{\v{s}}gum, Ivana},
doi = {10.1016/j.media.2019.02.005},
eprint = {1804.04383},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification(2).pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Deep learning,Iterative instance segmentation,Spine segmentation,Vertebra identification,Vertebra segmentation},
mendeley-tags = {Spine segmentation},
month = {apr},
pages = {142--155},
pmid = {30771712},
title = {{Iterative fully convolutional neural networks for automatic vertebra segmentation and identification}},
url = {http://arxiv.org/abs/1804.04383 http://dx.doi.org/10.1016/j.media.2019.02.005},
volume = {53},
year = {2019}
}
@inproceedings{Klinder2008,
abstract = {Including prior shape in the form of anatomical models is a well-known approach for improving segmentation results in medical images. Currently, most approaches are focused on the modeling and segmentation of individual objects. In case of object constellations, a simultaneous segmentation of the ensemble that uses not only prior knowledge of individual shapes but also additional information about spatial relations between the objects is often beneficial. In this paper, we present a two-scale framework for the modeling and segmentation of the spine as an example for object constellations. The global spine shape is expressed as a consecution of local vertebra coordinate systems while individual vertebrae are modeled as triangulated surface meshes. Adaptation is performed by attracting the model to image features but restricting the attraction to a former learned shape. With the developed approach, we obtained a segmentation accuracy of 1.0 mm in average for ten thoracic CT images improving former results. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Klinder, Tobias and Wolz, Robin and Lorenz, Cristian and Franz, Astrid and Ostermann, J{\"{o}}rn},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-85988-8_28},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models(2).pdf:pdf},
isbn = {354085987X},
issn = {03029743},
number = {PART 1},
pages = {227--234},
pmid = {18979752},
title = {{Spine segmentation using articulated shape models}},
volume = {5241 LNCS},
year = {2008}
}
@article{Sekuboyina2017,
abstract = {Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90{\%} on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI'16. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis.},
archivePrefix = {arXiv},
arxivId = {1703.04347},
author = {Sekuboyina, Anjany and Valentinitsch, Alexander and Kirschke, Jan S. and Menze, Bjoern H.},
eprint = {1703.04347},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2017 - A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets.pdf:pdf},
journal = {arXiv},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
number = {1},
pages = {1--10},
title = {{A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets}},
year = {2017}
}
@inproceedings{Bearman2015,
abstract = {The semantic image segmentation task presents a trade-off between test time accuracy and training time annotation cost. Detailed per-pixel annotations enable training accurate models but are very timeconsuming to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9{\%} mIOU over image-level supervision. Further, we demonstrate that models trained with pointlevel supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.},
archivePrefix = {arXiv},
arxivId = {1506.02106},
author = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and Fei-Fei, Li},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46478-7_34},
eprint = {1506.02106},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bearman et al. - 2015 - What's the Point Semantic Segmentation with Point Supervision.pdf:pdf},
isbn = {9783319464770},
issn = {16113349},
keywords = {Data annotation,Semantic segmentation,Weak supervision},
month = {jun},
pages = {549--565},
publisher = {Springer, Cham},
title = {{What's the point: Semantic segmentation with point supervision}},
url = {http://arxiv.org/abs/1506.02106},
volume = {9911 LNCS},
year = {2016}
}
@article{Glocker2012,
abstract = {This paper presents a new method for automatic localization and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81{\%}.},
author = {Glocker, Ben and Feulner, J. and Criminisi, Antonio and Haynor, D. R. and Konukoglu, E.},
doi = {10.1007/978-3-642-33454-2_73},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2012 - Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans.pdf:pdf},
isbn = {9783642334535},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {590--598},
pmid = {23286179},
title = {{Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans}},
volume = {7512 LNCS},
year = {2012}
}
@article{Chang2020,
abstract = {Multi-vertebrae segmentation plays an important role in spine diseases diagnosis and treatment planning. Global spatial dependencies between vertebrae are essential prior information for automatic multi-vertebrae segmentation. However, due to the lack of global information, previous methods have to localize specific vertebrae regions first, then segment and recognize the vertebrae in the region, resulting in a reduction in feature reuse and increase in computation. In this paper, we propose to leverage both global spatial and label information for multi-vertebrae segmentation from arbitrary MR images in one go. Specifically, a spatial graph convolutional network (GCN) is designed to first automatically learn an adjacency matrix and construct a graph on local feature maps, then adopt stacked GCN to capture the global spatial relationships between vertebrae. A label attention network is built to predict the appearance probabilities of all vertebrae using attention mechanism to reduce the ambiguity caused by variant FOV or similar appearances of adjacent vertebrae. The proposed method is trained in an end-to-end manner and evaluated on a challenging dataset of 292 MRI scans with various fields of view, image characteristics and vertebra deformations. The experimental results show that our method achieves high performance (89.28 ± 5.21 of IDR and 85.37 ± 4.09 {\%} of mIoU) from arbitrary input images.},
author = {Chang, Heyou and Zhao, Shen and Zheng, Hao and Chen, Yang and Li, Shuo},
doi = {10.1007/978-3-030-59725-2_68},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2020 - Multi-vertebrae Segmentation from Arbitrary Spine MR Images Under Global View.pdf:pdf},
isbn = {9783030597245},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention network,Global information,Graph convolutional network,Multi-vertebrae segmentation},
number = {July},
pages = {702--711},
title = {{Multi-vertebrae Segmentation from Arbitrary Spine MR Images Under Global View}},
volume = {12266 LNCS},
year = {2020}
}
@article{Ahn2019,
abstract = {This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with inter-pixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.},
archivePrefix = {arXiv},
arxivId = {1904.05044},
author = {Ahn, Jiwoon and Cho, Sunghyun and Kwak, Suha},
doi = {10.1109/CVPR.2019.00231},
eprint = {1904.05044},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahn, Cho, Kwak - 2019 - Weakly supervised learning of instance segmentation with inter-pixel relations.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation},
pages = {2204--2213},
title = {{Weakly supervised learning of instance segmentation with inter-pixel relations}},
volume = {2019-June},
year = {2019}
}
@inproceedings{Laradji2019a,
abstract = {A major obstacle in instance segmentation is that existing methods often need many per-pixel labels in order to be effective. These labels require large human effort and for certain applications, such labels are not readily available. To address this limitation, we propose a novel framework that can effectively train with image-level labels, which are significantly cheaper to acquire. For instance, one can do an internet search for the term "car" and obtain many images where a car is present with minimal effort. Our framework consists of two stages: (1) train a classifier to generate pseudo masks for the objects of interest; (2) train a fully supervised Mask R-CNN on these pseudo masks. Our two main contribution are proposing a pipeline that is simple to implement and is amenable to different segmentation methods; and achieves new state-of-the-art results for this problem setup. Our results are based on evaluating our method on PASCAL VOC 2012, a standard dataset for weakly supervised methods, where we demonstrate major performance gains compared to existing methods with respect to mean average precision.},
archivePrefix = {arXiv},
arxivId = {1907.01430},
author = {Laradji, Issam H. and Vazquez, David and Schmidt, Mark},
booktitle = {30th British Machine Vision Conference 2019, BMVC 2019},
eprint = {1907.01430},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji, Vazquez, Schmidt - 2019 - Where are the Masks Instance Segmentation with Image-level Supervision.pdf:pdf},
month = {jul},
title = {{Where are the masks: Instance segmentation with image-level supervision}},
url = {http://arxiv.org/abs/1907.01430},
year = {2020}
}
@article{Ibragimov2012,
author = {Ibragimov, Bulat},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ibragimov - 2012 - A Game-Theoretic Framework for Landmark-Based Image Segmentation.pdf:pdf},
journal = {IEEE TRANSACTIONS ON MEDICAL IMAGING},
number = {9},
pages = {1761--1776},
title = {{A Game-Theoretic Framework for Landmark-Based Image Segmentation}},
volume = {31},
year = {2012}
}
@article{Yao2015,
author = {Yao, Jianhua and Burns, Joseph E and Forsberg, Daniel and Seitel, Alexander and Rasoulian, Abtin and Abolmaesumi, Purang and Hammernik, Kerstin and Urschler, Martin and Ibragimov, Bulat and Vrtovec, Toma{\v{z}} and Castro-mateos, Isaac and Pozo, Jose M and Frangi, Alejandro F},
doi = {10.1016/j.compmedimag.2015.12.006.A},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yao et al. - 2017 - Segmentation.pdf:pdf},
pages = {16--28},
title = {{A Multi-center Milestone Study of Clinical Vertebral CT Segmentation}},
year = {2016}
}
@article{Ronneberger2015,
annote = {Original article about U-Net.

Stress on low data volume needed.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation(2).pdf:pdf},
journal = {CoRR},
keywords = {Medical,Segmentation},
mendeley-tags = {Medical,Segmentation},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {abs/1505.0},
year = {2015}
}
@techreport{Lia,
abstract = {Effective feature representations which should not only express the images individual properties, but also reflect the interaction among group images are essentially crucial for real-world co-segmentation. This paper proposes a novel end-to-end deep learning approach for group-wise object co-segmentation with a recurrent network architecture. Specifically, the semantic features extracted from a pre-trained CNN of each image are first processed by single image representation branch to learn the unique properties. Meanwhile, a specially designed Co-Attention Recurrent Unit (CARU) recurrently explores all images to generate the final group representation by using the co-attention between images, and simultaneously suppresses noisy information. The group feature which contains synergetic information is broadcasted to each individual image and fused with multi-scale fine-resolution features to facilitate the inferring of co-segmentation. Moreover, we propose a group-wise training objective to utilize the co-object similarity and figure-ground distinctness as the additional supervision. The whole modules are collaboratively optimized in an end-to-end manner, further improving the robustness of the approach. Comprehensive experiments on three benchmarks can demonstrate the superiority of our approach in comparison with the state-of-the-art methods.},
author = {Li, Bo and Li, Qian and Wu, Yunjie and Hu, Anqi},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Group-wise Deep Object Co-Segmentation with Co-Attention Recurrent Neural Network.pdf:pdf},
title = {{Group-wise Deep Object Co-Segmentation with Co-Attention Recurrent Neural Network}}
}

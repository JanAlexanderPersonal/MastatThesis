\chapter{Model combination results\label{sec:combination}}\thispagestyle{empty}

In chapter \ref{sec:singleDimension}, the construction of single dimension models is discussed.
These models extimate the segmentation masks of $352 mm\times 352 mm$ patches of scan volume slices along one of the three main axis.
In this chapter, the results of these single dimension models are combined to form a \textit{pseudo} mask that is subsequently used as labelling to train the final segmentation network.
Details regarding this combination procedure are given on page \pageref{sec:combinationProcedure}.
Interesting to note in this procedure is that the evaluation metric of these obtained segmentation masks is improved in each of these steps.
The pseudo mask volume performs better than the single dimension model results and the final model trained on the pseudo masks performs better than the pseudo mask itself. 

\begin{SCtable}[\sidecaptionrelwidth][h]

    \begin{tabular}{l|lll}
        \hline
        \textbf{\begin{tabular}[c]{@{}l@{}}Slice \\ direction\end{tabular}} &
          \textbf{Transverse} &
          \textbf{Coronal} &
          \textbf{Sagittal} \\ \hline
        \begin{tabular}[c]{@{}l@{}}Context\\ Slices {[}mm{]}\end{tabular}           & 1      & 1      & 1      \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Points per\\ class instance\end{tabular}         & 1      & 1      & 1      \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Background \\ points\end{tabular}                & 5      & 3      & 3      \\ \cline{1-1}
        Dataset &
          \begin{tabular}[c]{@{}l@{}}PLoS\\ xVertSeg\\ USiegen\\ MyoSegmenTUM\end{tabular} &
          \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}xVertSeg\\ USiegen\\ MyoSegmenTUM\end{tabular}} \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Segmentation\\ classes\end{tabular}              & 2      & 6      & 6      \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}score \\ $dice_{wi}$\end{tabular}              & 0.46   & 0.38   & 0.38   \\ \hline
        \begin{tabular}[c]{@{}l@{}}score \\ $dice_{wi}$\\ combination\end{tabular} & \multicolumn{3}{c}{0.51} \\ \hline
        \end{tabular}
    \caption{Combination of three point supervised models with algorithm \ref{alg:combination}. 
    These models were constructed with a fixed number of background points and a fixed number of class labels per class instance.
    This test indicates that the segmentation mask obtained from the result of single dimension models with algorithm \ref{alg:combination} allows to obtain a new segmentation mask with a higher metric score, the pseudo masks.
    The inverse weighted dice scores are evaluated on the cross validation set, this causes the difference with the values in figure \ref{fig:points_influence}. \label{tab:combination_1}
    }

\end{SCtable}
\par{
    In table \ref{tab:combination_1}, the segmentation quality is calculated on the validation set.
    Since there are two hyperparameters (the number of denoise iterations and the number of erosion iterations) to optimise in the combination algorithm \ref{alg:combination}, 
    the algorithm is evaluated with a matrix of these hyperparameters to choose the optimal hyperparameter set to construct the pseudo masks for the test set.
    In figure \ref{fig:hyperparameter_combination_1}, this obtimization procedure is illustrated.
    The optimal hyperparameter set in this case was 2 denoise iterations and 1 erosion iteration.
    From this figure, it is clear that the discussed hyperparameters do not have a large impact on the final result.
    The small difference in performance between the pseudo masks generated with increasing number of denoise iteration is arguably not worth the computational effort.
}
\par{
    It might be interesting to note that \textit{0} iterations indicates the morphological operation is repeated until the result converges.
    For the erosion operation, it is trivial this results in zero performance, all class segments are eroded away. 
}

\par{
    The result of this procedure is illustrated in figures \ref{fig:comb1_1}, \ref{fig:comb1_2} and \ref{fig:comb1_3}.
    These images illustrate that the result of the combination procedure is an improvement compared to the results of the individual models.
    Now, it is possible to train a network with only point annotated labels by using the results of the pseudo mask volume.
    Mind however that the results in chapter \ref{sec:singleDimension} and the results combined in this chapter were obtained with a different set of point annotations for each dimension.
    This was done to make comparison between different models and between models trained on slices taken over a single dimension possible. The next chapter discusses a more realistic scenario.
}
\par{
    In a realistic scenario, the expert will not annotated three stacks of slices from the same volume.
    Doing this would mean the expert would generate information with a high level of correlated and redundant information, 
    while the objective of weakly supervised learning is exactly to increase the efficiency of the label generation and information extraction.
    In the following chapter, a set of pseudo masks will be generated based on a single stack of annotated sagittal slices.
}
\marginpar{
        \includegraphics[width=5cm]{images/combination_optimization_1.pdf}
        \captionof{figure}{Illustration of the hyperparameter optimization procedure for the combination detailed in \ref{tab:combination_1}}
        \label{fig:hyperparameter_combination_1}
    }
\begin{SCfigure}[][htb]
    \centering
    \includegraphics[width=.90\textwidth]{images/comb1_denoise2_erode1_MyoSegmenTUM_036.pdf}
    \caption{
        Result of the combination of the three single dimension model results for volume MyoSegmenTUM nr 36.
        The colours indicate the vertebra classes. Only one semantic class is estimated in the first row, illustrating the model trained on transversal slices.
        On the first three rows, slices of the resulting segmentations from the single dimension models are shown. 
        It is clear these masks contain some artefacts and are not always in agreement with each other.
        On the fourth row, the result after mask combination and morphological smoothing is shown. 
        This corresponds more closely to the ground truth mask, shown on the fifth row.
        This final mask, shown on the fourth row, will be used as a pseudo mask to approximate the unknown ground truth mask.
        In the last row, the corresponding images are shown. 
        \protect\input{tables/colourlegend.tex}
        \label{fig:comb1_1}
    }
\end{SCfigure}
\begin{SCfigure}[][htb]
    \centering
    \includegraphics[width=.90\textwidth]{images/comb1_denoise2_erode1_xVertSeg_009.pdf}
    \caption{
        Result of the combination of the three single dimension model results for volume xVertSeg 009. The layout of this picture is identical to figure \ref{fig:comb1_1}.
        \protect\input{tables/colourlegend.tex}
        \label{fig:comb1_2}
    }
\end{SCfigure}
\begin{SCfigure}[][htb]
    \centering
    \includegraphics[width=.95\textwidth]{images/comb1_denoise2_erode1_USiegen_004.pdf}
    \caption{
        Result of the combination of the three single dimension model results for volume USiegen 004. The layout of this picture is identical to figure \ref{fig:comb1_1}.
        \protect\input{tables/colourlegend.tex}
        \label{fig:comb1_3}
    }
\end{SCfigure}

\chapter{Pseudo mask training}\thispagestyle{empty}
\par{
    In reality, an expert will generate a single stack of annotations, not three.
    As described in chapter \ref{sec:annotationPoints} on page \pageref{sec:annotationPoints}, one of the single dimensional models will be trained on this stack of annotated slices.
    The other two however, will be trained on stacks where the same annotation points are used, but now as these are found when slicing in the two other dimensions.
    This means that for the slices along one of the geometric dimensions, a known number of annotation points will be provided.
    For the slices perpendicular to the other two geometric axis, the annotation points will have to be deduced from this original stack.
    Figures \ref{fig:inferepoints_1} and \ref{fig:inferepoints_2} illustrate this.
}
\marginpar{
        \includegraphics[width=5cm]{images/combination_optimization_precalc.pdf}
        \captionof{figure}{Illustration of the hyperparameter optimization procedure (see also table \ref{tab:combination_precalc})}
        \label{fig:hyperparameter_combination_precalc}
    }

    \marginpar{
        \begin{tabular}{l|ll}
            \textbf{\begin{tabular}[c]{@{}l@{}}labelled\\ \\ slices\end{tabular}} &
              \textbf{\begin{tabular}[c]{@{}l@{}}1 class pt\\ 1 BG pt\end{tabular}} &
              \textbf{\begin{tabular}[c]{@{}l@{}}3 class pt\\ 5 BG pt\end{tabular}} \\ \hline
            \textbf{transv.} & 4466 & 9121 \\
            \textbf{frontal}    & 2627 & 4507 \\
            \textbf{sagittal}   & 7939 & 7939
            \end{tabular}
        \captionof{table}{Available labelled slices from labelling the sagittal slice stack.\label{tab:inferredLabels}}
    }
\par{
    Inferring annotation points from labels provided for slices taken in another direction implies the number of annotation points will vary.
    For some slices, no annotation points will be available.
    As a first test, point annotations in the sagittal slices with 1 class point per class instance and 1 background point were created.
    This showed to decrease the number of labelled slices strongly (table \ref{tab:inferredLabels}). When training with these slices, the segmentation networks showed to perform very poorly
    (inversely weighted dice scores of 0.22, 0.23 and 0.33 for the models trained respectively on the transverse, frontal and sagittal slice stacks).
    As is discussed on page \pageref{sec:trainingData} the cost of additional data points when the first point is provided is very low.
    When generating 3 labels per class instance and 5 background labels, the estimated cost is only increased to about $\frac{22,1 + 6.0,9}{239.7}=11,5\%$ of the cost of a full mask label.
    It is clear that this type of reasoning cannot be stretched to extremes (100 points or such), yet, the numbers of 3 and 5 seem perfectly reasonable.
}
\begin{SCfigure}[][htb]
    \centering
    \centering
    \begin{minipage}{.75\textwidth}
        \includegraphics[width=.99\textwidth]{images/MyoSegmenTUM020_s21_points.pdf}
    \end{minipage} 
    \vspace{1 mm}
    \begin{minipage}{.60\textwidth}
        \includegraphics[width=.99\textwidth]{images/MyoSegmenTUM020_s75_front_points.pdf}
    \end{minipage} 
    \vspace{2 mm}
    \caption{Illustration of the single-stack labelling procedure.
    Since the labeller is only providing point annotations for the sagittal slices, slice 21 of MyoSegmenTUM 20 is labelled with the definded 3 class labels per class instance and 5 background point labels.
    The same annotation points show up in the frontal slice 75 of the same volume. 
    \protect\input{tables/colourlegend.tex}\label{fig:inferepoints_1}}
\end{SCfigure}

\begin{SCfigure}[][htb]
    \centering
    \centering
    \begin{minipage}{.75\textwidth}
        \includegraphics[width=.99\textwidth]{images/USiegen004_s20_points.pdf}
    \end{minipage} 
    \vspace{1 mm}
    \begin{minipage}{.60\textwidth}
        \includegraphics[width=.99\textwidth]{images/USiegen004_s250_front_points.pdf}
    \end{minipage} 
    \vspace{2 mm}
    \caption{Illustration of the single-stack labelling procedure.
    Since the labeller is only providing point annotations for the sagittal slices, slice 20 of USiegen 04 is labelled with the definded 3 class labels per class instance and 5 background point labels.
    The same annotation points show up in the frontal slice 250 of the same volume. 
    \protect\input{tables/colourlegend.tex}\label{fig:inferepoints_2}}
\end{SCfigure}



Table \ref{tab:combination_precalc} illustrates how the performance of the combination of models trained on different slice stacks results in pseudo masks which prove to be an improvement compared to the individual models.
It is remarkable how high the performance of the models trained on \textit{inferred} annotation points proves to be.
Seemingly, randomizing the number of annotation points could have a positive influence on the model performance.

\begin{SCtable}[\sidecaptionrelwidth][h]

    \begin{tabular}{l|lll}
        \hline
        \textbf{\begin{tabular}[c]{@{}l@{}}Slice \\ direction\end{tabular}} &
          \textbf{Transverse} &
          \textbf{Coronal} &
          \textbf{Sagittal} \\ \hline
        \begin{tabular}[c]{@{}l@{}}Context\\ Slices {[}mm{]}\end{tabular}           & 1        & 1        & 1    \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Points per\\ class instance\end{tabular}         & variable & variable & 3    \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Background \\ points\end{tabular}                & variable & variable & 5    \\ \cline{1-1}
        Dataset &
          \begin{tabular}[c]{@{}l@{}}PLoS\\ xVertSeg\\ USiegen\\ MyoSegmenTUM\end{tabular} &
          \multicolumn{2}{l}{\begin{tabular}[c]{@{}l@{}}xVertSeg\\ USiegen\\ MyoSegmenTUM\end{tabular}} \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}Segmentation\\ classes\end{tabular}              & 2        & 6        & 6    \\ \cline{1-1}
        \begin{tabular}[c]{@{}l@{}}score \\ $dice_{wi}$\end{tabular}              & 0.51     & 0.41     & 0.34 \\ \hline
        \begin{tabular}[c]{@{}l@{}}score \\ $dice_{wi}$\ combination\end{tabular} & \multicolumn{3}{c}{0.48}   \\ \hline
        \end{tabular}
    \caption{Combination of three point supervised models with algorithm \ref{alg:combination}. 
    These models were constructed from points labels that were provided on the sagittal slice stack.
    The labels for the other two slice stacks were derived from this first one.
    This test indicates that the segmentation mask obtained from the result of single dimension models with algorithm \ref{alg:combination} allows to obtain a new segmentation mask with a higher metric score, the pseudo masks.
    The weighted dice scores are evaluated on the cross validation set, this causes the difference with the values in figure \ref{fig:points_influence} where the values were obtained by evaluation on the test set. \label{tab:combination_precalc}
    }

\end{SCtable}

Illustration \ref{fig:fullvsPseudo} shows the final result of the procedure.
With the pseudo masks obtained from combining the three model results, a new model is trained.
This last model is only trained on the sagittal slice stack.
The final results obtained with this model are illustrated in figure \ref{fig:fullvsPseudo}.
The hyperparameters of this model were optimized in the same way as the reference model.
The unweighted cross entropy loss proves to yield better results than the weighted cross entropy loss.
For the reference model, the same result was observed.

The confusion matrix of the model trained with pseudo mask labels (table \ref{tab:pseudo_confusionMatrix}) shows the model performance is clearly inferiour to the model performance of the fully supervised model.
Mind however that this model is trained based on data of which the labelling only costs about 12\% of the labelling cost of the fully supervised model for the train set scans.
The total labelling cost will be higher since the procedure requires full labels for the model evaluation in the cross validation and the test set. 
If one takes, is was done in this thesis, both the cross validation and test set to be $\frac{1}{6}$ of the total volumes, the total cost is about 41 \% of the cost for full labels.
One could also look at it in another way: for fixed cross validation and test set, the train set can be made 8,3$\times$ larger with the same labelling cost.


\begin{SCfigure}[][htb]
    \centering
        \includegraphics[width=.99\textwidth]{images/fullvsPseudo_MyoSegmenTUM_024_023.pdf}
    \caption{These images illustrate the difference in performance for the two models generated. On the top row, an image slice is shown next to the ground truth annotation mask.
    Two models were trained: the reference model, shown left on the second row and a model trained on pseudo masks of which the result is shown on the right side. 
    These images show the final result of the procedure described in this thesis. The shown masks are not the pseudo masks themselves, they are the results obtained from the final model trained on these pseudo masks.
    This first image shows these results for slice 23 of MyoSegmenTUM volume 24, slice 24 of USiegen volume 12 and slice 260 of xVertSeg volume 14. 
    \protect\input{tables/colourlegend.tex}
    \label{fig:fullvsPseudo}}
\end{SCfigure}
\begin{SCfigure}[][htb]
    \centering
        \includegraphics[width=.99\textwidth]{images/fullvsPseudo_USiegen_012_024.pdf}
    \caption{This image follows figure \ref{fig:fullvsPseudo}. It shows slice 24 of USiegen volume 12. 
    \protect\input{tables/colourlegend.tex}}
\end{SCfigure}
\begin{SCfigure}[][htb]
    \centering
        \includegraphics[width=.99\textwidth]{images/fullvsPseudo_xVertSeg_014_260.pdf}
    \caption{This image follows figure \ref{fig:fullvsPseudo}. It shows slice 260 of xVertSeg volume 14. 
    \protect\input{tables/colourlegend.tex}
    \label{fig:fullvsPseudo}}
\end{SCfigure}

\begin{SCtable}[\sidecaptionrelwidth][h]
 
    % Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{tabular}{ll|llllll}
    \toprule
    \multicolumn{2}{l|}{\multirow{2}{*}{\textbf{values in {[}\%{]}}}} & \multicolumn{6}{l}{\textbf{Predicted}}                                            \\
    \multicolumn{2}{l|}{}                                             & \textbf{BG} & \textbf{L1} & \textbf{L2} & \textbf{L3} & \textbf{L4} & \textbf{L5} \\ \hline
    \multirow{6}{*}{\textbf{Actual}}           & \textbf{BG}          & 99.48       & 56.56       & 29.81       & 29.5        & 29.22       & 41.93       \\
     & \textbf{L1} & 0.09 & 39.65 & 3.73  & 0.08  & 0     & 0     \\
     & \textbf{L2} & 0.11 & 3.76  & 63.36 & 2.42  & 0     & 0     \\
     & \textbf{L3} & 0.11 & 0     & 3.1   & 63.49 & 2.91  & 0     \\
     & \textbf{L4} & 0.11 & 0     & 0     & 4.5   & 65.91 & 1.85  \\
     & \textbf{L5} & 0.11 & 0.03  & 0     & 0     & 1.96  & 56.22 \\ \bottomrule
    \end{tabular}

    \caption{Confusion matrix for the model trained with pseudo label masks (network VGG16-FCN8 and cross-correlation loss), evaluated on the test set.
    The values have been normalized by the total number of voxels predicted in each class.
    \label{tab:pseudo_confusionMatrix}
    }
\end{SCtable}

\begin{SCfigure}[][htb]
        \includegraphics[width=.99\textwidth]{images/PseudoSupervised.pdf}
        \caption{Inversely weighted dice score for models trained on pseudo-masks. \label{fig:PseudoSupervised_dice}}
        
\end{SCfigure}

\par{
    Since the final model is exactly the same network as the reference model, only trianed with pseudo masks instead of full masks, the inference time is exactly the same.
Preprocesing of one volume was observed to take 3.9 seconds on average (see appendix \ref{sec:hardware} for details on the hardware used).
After this, evaluation of one volume and recombination of the crops takes, on average, 8,3 seconds.
The segmentation masks illustrated in figure \ref{fig:fullvsPseudo} can both be generated in 12,2 seconds.
}
\par{
    The obtained segmentation masks are too restricted.
    While the position of the vertebrae seems to be predicted correctly, the model does not predict the full extend of the vertebra and many vertebra voxels are classified as background, hence the low recall.
    The ground cause of this phenomenon could not be discovered.
    No post-processing steps (dilation, watershed) to remedy this a-postiori were tested.
    Future research might investigate how these post-processing steps could benefit the model performance. 
}

\par{
    To investigate the performance difference observed between the different datasets, models were trained using only data from a single data source.
    Due to small differences between the datasets, both in labelling (some dataset include the laminae in the labels while others do not) and the imaging techniques used to obtain the dataset (\acrshort{ct} or \acrshort{mri}),
    one could believe that combining these datasets could cause one to influence the network performance on the other.
    After training these single source models, large differences between the performances of these single source models were observed.
    The latter calls for concluding that there are factors within each dataset determining how well the model can generalize.
}

\begin{SCtable}[\sidecaptionrelwidth][h]

    \input{tables/Summary_final.tex}
    \caption{Summary table.
    }

\end{SCtable}

\begin{SCfigure}[][htb]
    \centering
    \begin{minipage}{.99\textwidth}
        \includegraphics[width=.99\textwidth]{images/pointAnnotated_perClass_perSource_point_precision.pdf}
    \end{minipage} 
    \vspace{2 mm}
    \begin{minipage}{.99\textwidth}
        \includegraphics[width=.99\textwidth]{images/pointAnnotated_perClass_perSource_point_recall.pdf}
    \end{minipage} 
    \caption{Precision and recall for the final model. From this image and from the illustrations shown in figure \ref{fig:fullvsPseudo}, 
    it is clear that the obtained model does not seem to be able to predict the full extend of the vertebrae. The inferred masks are too restricted in dimensions.
    \protect\input{tables/colourlegend_long.tex}
    }
\end{SCfigure}

\par{
Finally, it was also observed that the model does not generalize very well to the one dataset that was not used for training the model.
Due to the lack of full annotation masks, the UWspine dataset (see page \pageref{sec:UWspine}) could not be used to construct the reference model.
This means it was not used to construct the weakly supervised model either. 
The dataset does not contain labels except for the vertebra volume centers, it would not have increased the training dataset strongly.
When evaluation the constructed model on this dataset, the model estimations do not approximate a spine segmentation.
About 50\% of the estimation volumes are empty, when the model does make an estimation, it is patchy as best.
This leads to the conclusion that the model performance depends strongly on the dataset. 
}



\begin{SCfigure}[][htb]
    \centering
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{images/UW_result.pdf}
    \end{minipage}%
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=.99\textwidth]{images/fullvsPseudo_UW_062_103.pdf}
    \end{minipage} 
    \caption{Model estimation for the UWspine volume 62. The 3D visualization of the estimation is shown on the left, while the 103$^{th}$ slice of this volume is illustrated on the right.
    \protect\input{tables/colourlegend_long.tex}
    }
\end{SCfigure}
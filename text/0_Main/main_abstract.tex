\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

% Set page layout to plain (only page number) and two-column 
\begin{multicols}{2}
\thispagestyle{plain}
\par{
    \textit{
        Doctors use \acrfull{mri} or \acrfull{ct} scans as essential components for medical diagnosis, following the course of medical conditions and the planning of medical procedures.
        There is a trend towards machine vision to support medical professionals interpreting and using these images \todo[inline]{references}.
        Building these applications requires expensive labelled datasets.
        This research investigates techniques to reduce the dataset labelling cost by working with point annotation instead of full annotation.
        Experiments are conducted on openly available datasets and demonstrate the usefulness of two new loss components and a combination technique of different model results to generate pseudo masks.
        As a final result, this work demonstrates that one can obtain xxx \% of the performance of a fully annotated model at xxx \% of the labelling cost. \todo[inline]{add numbers}
    }
}
\section*{Thesis objective \& Motivation}
\par{
    The use of radiological images is a crucial element in modern medical practice. 
    \acrfull{mri} or \acrfull{ct} scans are essential components for pre-operative and post-operative diagnosis, following the course of medical conditions and the planning of medical procedures.
    Automated interpretation of medical images can mean a gain in efficiency and a cost reduction.
}
\par{
    Machine vision - deep learning in general - tends to be very \textit{data-hungry}. Constructing a new model requires large, labelled datasets.
    Acquiring these datasets and the corresponding labels is time-consuming and expensive. 
    Maximisation of the return of a given data and labelling budget through is a goal shared by all \acrshort{ml} practitioners.
    The use of weak labels, or sometimes called \textit{hints}, is one approach to improve this.
    This approach aims to train a model capable of inferring more informative results than the information level explicitly available in the labelling.
}
\par{
    This project presents a model for the automated segmentation of the lumbar vertebrae of the human spine based on point level annotated medical scans.
    I work with both \acrshort{ct} scan volumes and \acrshort{mri} scan volumes.
    Since point level annotation is faster and cheaper than providing a complete label mask (estimated at 8.4\% of the cost by \cite{Bearman2015}), this technique provides a cost-benefit. 
    The labels only contain the true class of a mere handful of voxels. The information contained in point labels is weaker than the segmentation information inferred by the model.
}



\section*{Data sets and data preprocessing \label{sec:abstr_data}}
\par{
    All datasets used in this work are publicly available (all datasets are listed on page \pageref{sec:datasets}). 
    The scope of this work did not include medical data gathering.
    These datasets contain both \acrshort{ct} and \acrshort{mri} scans. 
    In 86 of these scans, complete volume masks of the vertebrae are available. 
    In 20 volumes, only semantic labels are available.
    For 125 volumes, point level annotation is available.
}
\par{
    The complete dataset of 231 patients consists of 112 women and 99 men. Of 23 people, no gender information is available. 
    Since a medical professional does not order a medical scan unless there is a suspicion of a medical condition, the dataset contains various patients with different pathologies.
    The dataset contains images of patients with scoliosis and with crushed and wedged vertebrae.
}
\par{
    The different datasets use different data formats and different scan resolutions. 
    The first data preprocessing step is homogenising the scan resolution by resampling the image on an $1mm\times 1mm\times 1mm$ grid. 
    Next, the image is sliced along one of the three principal axes.
    The contrast of the 2D image slices is first enhanced with the \acrfull{clahe} algorithm.
    Then the images are cropped (or padded, if needed) to form $352 px \times 352 px$ slices.
    All models are built with this image size, sufficient to contain all 5 lumbar vertebrae $L_1$ to $L_5$ in one image.
}


\section*{Methodology}
\par{
    The performances of different models are compared with the weighted dice score.
    This metric takes into account both the model precision and recall as well as the class imbalance.
}


\subsection*{Performance benchmark}
\par{
    For 86 scans, full annotation masks are available.
    As a performance benchmark, the performance of a fully supervised model trained on these images is taken.
}



\subsection*{Weakly supervised models}
\par{
    The segmentation of the volume image is based on a 2D \acrfull{cnn}, pre-trained on a large classification dataset.
    Upsampling layers are added to this network to estimate 6 segmentation channels (5 lumbar vertebrae and the background class). 
    By training three different weakly supervised models on 3 sets of 2D images sliced along the 3 main volume dimensions, three sets of segmentation masks are obtained. 
    The combination of these different segmentation masks is used as an \textit{erzatz} label set to train a fully supervised model on one volumetric dimension.
}

\subsubsection*{Loss function}
\par{
    To train a weakly supervised network, several loss components, both supervised and unsupervised, are combined.
}
\par{
    The weighted cross-entropy loss is optimised for the fully supervised reference model, a classic choice for this problem.
    It is also the point loss $\mathcal{L}_P$ component of the weakly supervised model. Then it is only evaluated on $\mathcal{Y}$, the set of available point labels.
    The function combines the six network output channels with a softmax function $\sigma$, after which the negative log-loss function is calculated, weighted with factors $w$.
}
\begin{equation} \label{eq:crossEntropy}
    \mathcal{L}_P(X_i) = -\sum_{\vec{p} \in \mathcal{I}_i} w_{\mathcal{Y}_i(\vec{p})}.\log\left[\sigma_{\mathcal{Y}_i(\vec{p})}\left(\vec{z_i(\vec{p})}\right)\right]
\end{equation}
\par{
    Then the unsupervised rotation consistency loss $\mathcal{L}_C$ \todo[inline]{cite laradji} imposes that the model output $f_\theta$ should be consistent for a transformation $t_k$ of the input image.
    In this work, the chosen transformations are image rotations over $0^\circ, 90^\circ, 180^\circ$ or $270^\circ$, combined with an image flip.
}
\begin{equation}
    \mathcal{L}_C(X_i) = \sum_{p \in \mathcal{P}_i} \left| t_k\left[f_\theta(X_i)\right]_p - f_\theta\left( t_k[X_i] \right)_p  \right|  
\end{equation}
\par{
    The second unsupervised loss term is the separation loss term.  \todo[inline]{elaborate}
}
\par{
    Finally, $\mathcal{L}_E$, the maximal extend supervised loss term, takes into account that a lumbar vertebra has a limited size.
    The Euclidian distance field $\mathbf{d}$ from the annotation point is converted to a semi-mask for each class $k$:
    \begin{eqnarray}
        \mathbf{d}_k(\vec{q}) &=& \max_{\vec{p}:\mathcal{Y}_i(\vec{p})=k}||\vec{q} - \vec{p}||\\
        \mathbf{m}_k(\vec{q}) &=& \mathbf{I}\left( (-\mathbf{d}(\vec{q}) + r) > 0 \right)
    \end{eqnarray}
    Now, $\mathbf{m}$ is 1 only for positions closer than distance $r$ from the annotation points for class $k$.
    Where $\mathbf{m}_k=0$, the model output should not indicate output class $k$. Where $\mathbf{m}_k=1$, the output class is unknown.
    The loss function is the binary cross-entropy between $\mathbf{m}_k$ and the sigmoid of the k$^{th}$ channel of the logits $z_i$ with weight vector $\{1, 0\}$.
}
\begin{equation}
    \mathcal{L}_E(X_i) = \sum_{k\in\mathcal{K}}\sum_{\vec{q}\in X_i}  (1-\mathbf{m}_k(\vec{q})) \log(\mathbf{S}(z_i(\vec{q})_k)) 
\end{equation}

\subsubsection*{Model result combination}
\par{
    Combining the results of the three models trained on the three geometric axes is a pseudo-mask of higher quality than the results of the individual models.
    After morphological smoothing, the pseudo mask is used to train the final model (on sagittal slices).
}

\thispagestyle{plain}
\section*{Results}
\subsection*{Hyperparameter optimization}


\subsection*{Final result}

\section*{Conclusion}
\todo[inline]{Complete this section}
\cleardoublepage
\end{multicols}
\chapter*{Abstract}
\begin{multicols}{2}
\section*{Thesis objective \& Motivation}
\par{
    The use of radiological images is a crucial element in modern medical practice. 
    Doctors use \acrfull{mri} or \acrfull{ct} scans as essential components for medical diagnosis, following the course of medical conditions and the planning of medical procedures.
There is a trend towards machine vision to support medical professionals interpreting and using these images \todo[inline]{references}
}
\par{
    Machine vision - deep learning in general - tends to be very \textit{data-hungry}. Constructing a new model requires large, labelled datasets.
    Acquiring these datasets and the corresponding labels is time-consuming and expensive. 
    Maximisation of the return of a given data and labelling budget through is a goal shared by all \acrshort{ml} practitioners.
    One idea to attain this is to use weak labels, or sometimes called \textit{hints}.
    This approach aims to train a model that is capable of inferring \textit{stronger} information from the labels than the information level explicitly available in the labelling.
}
\par{
    This project presents a model for the automated segmentation of the lumbar vertebrae of the human spine based on point level annotated medical scans.
    I work with both \acrshort{ct} scan volumes and \acrshort{mri} scan volumes.
    Since point level annotation is faster and cheaper than providing a complete label mask, this technique provides a cost-benefit. 
    The information contained in point labels is weaker than the segmentation information inferred by the model. the labels only contain the true class of a mere handful of voxels, hence the term \gls{weaklysupervisedl}. 
}



\section*{Data sets and data preprocessing \label{sec:abstr_data}}
\par{
    All datasets used in this work are publicly available (all datasets are listed on page \pageref{sec:datasets}). 
    The scope of this work did not include medical data gathering.
    These datasets contain both \acrshort{ct} and \acrshort{mri} scans. 
    In 86 of these scans, complete volume masks of the vertebrae are available. 
    In 20 volumes, only semantic labels are available.
    For 125 volumes, point level annotation is available. There are also five volumes for which no segmentation data is available. These were not used in this study.
}
\par{
    The complete dataset of 231 patients consists of xxx women and xxx men. 
    The average male patient in the dataset is xxx years old; the average female patient is xxx.
    Since a medical professional does not order a medical scan unless there is a suspicion of a medical condition, the dataset contains various patients with different pathologies.
    The dataset contains images of patients with scoliosis and with crushed and wedged vertebrae.
}
\par{
    The different datasets use different data formats and different scan resolutions. 
    The first data preprocessing step is homogenising the scan resolution by resampling the image on an $1mm\times 1mm\times 1mm$ grid. 
    Next, the image is sliced along one of the three principal axes.
    The contrast of the 2D image slices is first enhanced with the \acrfull{clahe} algorithm.
    Then the images are cropped (or padded, if needed) to form $352 px \times 352 px$ input image slices.
}


\section*{Methodology}

The performances of different models are compared with the weighted dice score.
This metric takes into account both the model precision and recall as well as the class imbalance.

\subsection*{Performance benchmark}
For 86 scans, full annotation masks are available.
As a performance benchmark, the performance of a fully supervised model trained on these images is taken.


\subsubsection*{Weakly supervised models}
\par{
    The segmentation of the volume image is based on a 2D \acrfull{cnn}, pre-trained on a large classification dataset.
    Several upsampling layers are added to this network to allow the network to estimate 6 segmentation channels (5 lumbar vertebrae and the background class). 
    The network provides a 2D segmentation mask for each slice, which is then combined into a volume segmentation mask. 
    The image preprocessing results in 3 sets of 2D images sliced along the 3 main volume dimensions.
    By training three different weakly supervised models on this data, three sets of segmentation masks are obtained. 
    The combination of these different segmentation masks can result in a more accurate mask than the individual ones.
    Finally, This combined segmentation mask is then used as an \textit{erzatz} label set to train a fully supervised model on one volumetric dimension.
}
\par{
    As discussed further, there are several hyperparameters in the model to be optimised.
    The performance of the models trained on \Gls{weaklysupervisedl} data is compared to the performance of a model trained on \Gls{supervisedl} data.
}

\subsection*{Loss function}
\par{
    The weighted cross-entropy loss is optimised for the fully supervised reference model, a classic choice for this problem.
    The function combines the six network output channels with a softmax function, after which the negative log-loss function is calculated.
}
\begin{equation} \label{eq:crossEntropy}
    \mathcal{L}_P(X_i) = -\sum_{\vec{p} \in \mathcal{I}_i} w_{\mathcal{Y}_i(\vec{p})}.\log\left[\sigma_{\mathcal{Y}_i(\vec{p})}\left(\vec{z_i(\vec{p})}\right)\right]
\end{equation}
\par{
    A combination of several supervised and unsupervised loss terms is optimised to train the weakly supervised model.
}
\par{
    The first of these terms is the unsupervised rotation consistency loss \todo[inline]{cite laradji}. 
    This loss term imposes that the model output should be consistent for a transformation $t_k$ of the input image.
    In this work, the chosen transformations are image rotations over $0^\circ, 90^\circ, 180^\circ$ or $270^\circ$, combined with an image flip.
}
\begin{equation}
    \mathcal{L}_C(X_i) = \sum_{p \in \mathcal{P}_i} \left| t_k\left[f_\theta(X_i)\right]_p - f_\theta\left( t_k[X_i] \right)_p  \right|  
\end{equation}
\par{
    The second unsupervised loss term is the separation loss term.  \todo[inline]{elaborate}
}
\par{
    Then a supervised loss term takes into account that a lumbar vertebra has a limited size.
    Then $\mathbf{d}$ is converted to a semi-mask:
    \begin{eqnarray}
        \mathbf{d}_k(\vec{q}) &=& \max_{\vec{p}:\mathcal{Y}_i(\vec{p})=k}||\vec{q} - \vec{p}||\\
        \mathbf{m}_k(\vec{q}) &=& \mathbf{I}\left( (-\mathbf{d}(\vec{q}) + r) > 0 \right)
    \end{eqnarray}
    Now, $\mathbf{m}$ is 1 for positions closer than distance $r$ from the points, and it is 0 for positions far from labels with value $k$.
    Where $\mathbf{m}_k=0$, the model output should not indicate output class $k$. Where $\mathbf{m}_k=1$, the output class is unknown\footnote{Only channel $k$ is concerned, since one only knows what class these points do not belong to, there is not more information about the other classes.}.

    The loss function is the binary cross-entropy between $\mathbf{m}_k$ and the sigmoid of the k$^{th}$ channel of the logits $z_i$ with weight vector $\{1, 0\}$.
}
\begin{equation}
    \mathcal{L}_E(X_i) = \sum_{k\in\mathcal{K}}\sum_{\vec{q}\in X_i}  (1-\mathbf{m}_k(\vec{q})) \log(\mathbf{S}(z_i(\vec{q})_k)) 
\end{equation}
\par{
    The final loss term for the weakly supervised model is again the cross-entropy loss $\mathcal{L}_p$, equation \ref{eq:crossEntropy}. 
    The difference with the fully supervised model is that $\mathcal{I}_i$ only covers a handful of image pixels.
}


\section*{Results}
\subsection*{Hyperparameter optimization}

\subsection*{Combination of different dimension models}

\subsection*{Final result}

\section*{Conclusion}
\todo[inline]{Complete this section}
\cleardoublepage
\end{multicols}
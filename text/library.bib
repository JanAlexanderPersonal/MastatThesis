Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{SemTorch76:online,
annote = {(Accessed on 02/14/2021)},
howpublished = {$\backslash$url{\{}https://pypi.org/project/SemTorch/{\}}},
month = {sep},
title = {{SemTorch {\textperiodcentered} PyPI}},
year = {2020}
}
@article{DasCLuciaMS2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {{doi:10.1016/j.brs.2016.03.010 Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki T S The Clinical T M S Society Consensus Review and {for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.}, Treatment Recommendations},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@article{PereraTGeorgeMSGrammerGJanicakPGPascual-LeoneA2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {{doi:10.1016/j.brs.2016.03.010 Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki T S The Clinical T M S Society Consensus Review and {for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.}, Treatment Recommendations},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@article{Sekuboyina2019,
abstract = {We propose an auto-encoding network architecture for point clouds (PC) capable of extracting shape signatures without supervision. Building on this, we (i) design a loss function capable of modelling data variance on PCs which are unstructured, and (ii) regularise the latent space as in a variational auto-encoder, both of which increase the auto-encoders' descriptive capacity while making them probabilistic. Evaluating the reconstruction quality of our architectures, we employ them for detecting vertebral fractures without any supervision. By learning to efficiently reconstruct only healthy vertebrae, fractures are detected as anomalous reconstructions. Evaluating on a dataset containing ∼ 1500 vertebrae, we achieve area-under-ROC curve of {\textgreater}75{\%}, without using intensity-based features.},
archivePrefix = {arXiv},
arxivId = {1907.09254},
author = {Sekuboyina, Anjany and Rempfler, Markus and Valentinitsch, Alexander and Loeffler, Maximilian and Kirschke, Jan S. and Menze, Bjoern H.},
doi = {10.1007/978-3-030-32226-7_42},
eprint = {1907.09254},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2019 - Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis.pdf:pdf},
isbn = {9783030322250},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {375--383},
title = {{Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis}},
volume = {11769 LNCS},
year = {2019}
}
@article{Laradji2018,
abstract = {Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detection-based method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.},
archivePrefix = {arXiv},
arxivId = {1807.09856},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1007/978-3-030-01216-8_34},
eprint = {1807.09856},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2018 - Where Are the Blobs Counting by Localization with Point Supervision.pdf:pdf},
isbn = {9783030012151},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {560--576},
title = {{Where Are the Blobs: Counting by Localization with Point Supervision}},
volume = {11206 LNCS},
year = {2018}
}
@article{McEver2020,
abstract = {Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset $\backslash$cite{\{}everingham2010pascal{\}}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.},
archivePrefix = {arXiv},
arxivId = {2007.05615},
author = {McEver, R. Austin and Manjunath, B. S.},
eprint = {2007.05615},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision(2).pdf:pdf},
title = {{PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision}},
url = {http://arxiv.org/abs/2007.05615},
year = {2020}
}
@article{Ronneberger2015,
annote = {Original article about U-Net.

Stress on low data volume needed.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation(2).pdf:pdf},
journal = {CoRR},
keywords = {Medical,Segmentation},
mendeley-tags = {Medical,Segmentation},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {abs/1505.0},
year = {2015}
}
@article{Glocker2013,
abstract = {Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit parametric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases. {\textcopyright} 2013 Springer-Verlag.},
author = {Glocker, Ben and Zikic, Darko and Konukoglu, Ender and Haynor, David R. and Criminisi, Antonio},
doi = {10.1007/978-3-642-40763-5_33},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2013 - Vertebrae localization in pathological spine CT via dense classification from sparse annotations.pdf:pdf},
isbn = {9783642407628},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {262--270},
pmid = {24579149},
title = {{Vertebrae localization in pathological spine CT via dense classification from sparse annotations}},
volume = {8150 LNCS},
year = {2013}
}
@article{Lessmann2018,
abstract = {Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.},
archivePrefix = {arXiv},
arxivId = {1804.04383},
author = {Lessmann, Nikolas and van Ginneken, Bram and de Jong, Pim A. and I{\v{s}}gum, Ivana},
doi = {10.1016/j.media.2019.02.005},
eprint = {1804.04383},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification(2).pdf:pdf},
keywords = {Spine segmentation},
mendeley-tags = {Spine segmentation},
month = {apr},
title = {{Iterative fully convolutional neural networks for automatic vertebra segmentation and identification}},
url = {http://arxiv.org/abs/1804.04383 http://dx.doi.org/10.1016/j.media.2019.02.005},
year = {2018}
}
@article{Shi2020,
abstract = {Current weakly supervised object localization and segmentation rely on class-discriminative visualization techniques to generate pseudo-labels for pixel-level training. Such visualization methods, including class activation mapping (CAM) and Grad-CAM, use only the deepest, lowest resolution convolutional layer, missing all information in intermediate layers. We propose Zoom-CAM: going beyond the last lowest resolution layer by integrating the importance maps over all activations in intermediate layers. Zoom-CAM captures fine-grained small-scale objects for various discriminative class instances, which are commonly missed by the baseline visualization methods. We focus on generating pixel-level pseudo-labels from class labels. The quality of our pseudo-labels evaluated on the ImageNet localization task exhibits more than 2.8{\%} improvement on top-1 error. For weakly supervised semantic segmentation our generated pseudo-labels improve a state of the art model by 1.1{\%}.},
archivePrefix = {arXiv},
arxivId = {2010.08644},
author = {Shi, Xiangwei and Khademi, Seyran and Li, Yunqiang and van Gemert, Jan},
eprint = {2010.08644},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - 2020 - Zoom-CAM Generating Fine-grained Pixel Annotations from Image Labels.pdf:pdf},
title = {{Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels}},
url = {http://arxiv.org/abs/2010.08644},
year = {2020}
}
@article{Hong2015,
abstract = {We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in PASCAL VOC dataset.},
archivePrefix = {arXiv},
arxivId = {1506.04924},
author = {Hong, Seunghoon and Noh, Hyeonwoo and Han, Bohyung},
eprint = {1506.04924},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Noh, Han - 2015 - Decoupled deep neural network for semi-supervised semantic segmentation.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1495--1503},
title = {{Decoupled deep neural network for semi-supervised semantic segmentation}},
volume = {2015-Janua},
year = {2015}
}
@article{Ahn2019,
abstract = {This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with inter-pixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.},
archivePrefix = {arXiv},
arxivId = {1904.05044},
author = {Ahn, Jiwoon and Cho, Sunghyun and Kwak, Suha},
doi = {10.1109/CVPR.2019.00231},
eprint = {1904.05044},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahn, Cho, Kwak - 2019 - Weakly supervised learning of instance segmentation with inter-pixel relations.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Categorization,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation},
pages = {2204--2213},
title = {{Weakly supervised learning of instance segmentation with inter-pixel relations}},
volume = {2019-June},
year = {2019}
}
@techreport{Klinder2008,
abstract = {Including prior shape in the form of anatomical models is a well-known approach for improving segmentation results in medical images. Currently, most approaches are focused on the modeling and segmentation of individual objects. In case of object constellations, a simultaneous segmentation of the ensemble that uses not only prior knowledge of individual shapes but also additional information about spatial relations between the objects is often beneficial. In this paper, we present a two-scale framework for the modeling and segmentation of the spine as an example for object constellations. The global spine shape is expressed as a consecution of local vertebra coordinate systems while individual vertebrae are modeled as triangulated surface meshes. Adaptation is performed by attracting the model to image features but restricting the attraction to a former learned shape. With the developed approach, we obtained a segmentation accuracy of 1.0 mm in average for ten thoracic CT images improving former results.},
author = {Klinder, Tobias and Wolz, Robin and Lorenz, Cristian and Franz, Astrid and Ostermann, J{\"{o}}rn},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models(2).pdf:pdf},
title = {{LNCS 5241 - Spine Segmentation Using Articulated Shape Models}},
year = {2008}
}
@article{DasCLuciaMS2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {doi:10.1016/j.brs.2016.03.010 {Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki TS. The Clinical TMS Society Consensus Review and Treatment Recommendations for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@article{Chuang2019,
author = {Chuang, Cheng-Hung and Lin, Chih-Yang and Tsai, Yuan-Yu and Lian, Zhen-You and Xie, Hong-Xia and Hsu, Chih-Chao and Huang, Chung-Lin},
doi = {10.1109/access.2019.2934325},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang et al. - 2019 - Efficient Triple Output Network for Vertebral Segmentation and Identification.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
pages = {117978--117985},
publisher = {IEEE},
title = {{Efficient Triple Output Network for Vertebral Segmentation and Identification}},
volume = {7},
year = {2019}
}
@article{Janssens2018,
abstract = {We present a method to address the challenging problem of automatic segmentation of lumbar vertebrae from CT images acquired with varying fields of view. Our method is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting of a localization FCN and a segmentation FCN. More specifically, in the first step we train a regression 3D FCN (we call it “LocalizationNet”) to find the bounding box of the lumbar region. After that, a 3D U-net like FCN (we call it “SegmentationNet”) is then developed, which after training, can perform a pixel-wise multi-class segmentation to map a cropped lumber region volumetric data to its volume-wise labels. Evaluated on publicly available datasets, our method achieved an average Dice coefficient of 95.77 ± 0.81{\%} and an average symmetric surface distance of 0.37 ± 0.06 mm.},
author = {Janssens, Rens and Zheng, Guoyan},
doi = {10.29007/vt7v},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janssens, Zheng - 2018 - Deep Learning based Segmentation of Lumbar Vertebrae from CT Images.pdf:pdf},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
pages = {94--89},
title = {{Deep Learning based Segmentation of Lumbar Vertebrae from CT Images}},
volume = {2},
year = {2018}
}
@article{Sekuboyina2017,
abstract = {Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90{\%} on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI'16. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis.},
archivePrefix = {arXiv},
arxivId = {1703.04347},
author = {Sekuboyina, Anjany and Valentinitsch, Alexander and Kirschke, Jan S. and Menze, Bjoern H.},
eprint = {1703.04347},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2017 - A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets.pdf:pdf},
journal = {arXiv},
keywords = {xvertseg},
mendeley-tags = {xvertseg},
number = {1},
pages = {1--10},
title = {{A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets}},
year = {2017}
}
@article{Han2020,
abstract = {Automated medical report generation in spine radiology, i.e., given spinal med-ical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of arti-ficial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpo-lates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly con-ducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demon-strate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.},
archivePrefix = {arXiv},
arxivId = {2004.13577},
author = {Han, Zhongyi and Wei, Benzheng and Yin, Yilong and Li, Shuo},
eprint = {2004.13577},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2020 - Unifying neural learning and symbolic reasoning for spinal medical report generation.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Adversarial training,Graph neural network,Logical reasoning,Medical image analysis,Medical report generation},
title = {{Unifying neural learning and symbolic reasoning for spinal medical report generation}},
year = {2020}
}
@article{Lu2018,
abstract = {The high prevalence of spinal stenosis results in a large volume of MRI imaging, yet interpretation can be time-consuming with high inter-reader variability even among the most specialized radiologists. In this paper, we develop an efficient methodology to leverage the subject-matter-expertise stored in large-scale archival reporting and image data for a deep-learning approach to fully-automated lumbar spinal stenosis grading. Specifically, we introduce three major contributions: (1) a natural-language-processing scheme to extract level-by-level ground-truth labels from free-text radiology reports for the various types and grades of spinal stenosis (2) accurate vertebral segmentation and disc-level localization using a U-Net architecture combined with a spine-curve fitting method, and (3) a multi-input, multi-task, and multi-class convolutional neural network to perform central canal and foraminal stenosis grading on both axial and sagittal imaging series inputs with the extracted report-derived labels applied to corresponding imaging level segments. This study uses a large dataset of 22796 disc-levels extracted from 4075 patients. We achieve state-of-the-art performance on lumbar spinal stenosis classification and expect the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.},
author = {Lu, Jen-Tang and Pedemonte, Stefano and Bizzo, Bernardo and Doyle, Sean and Andriole, Katherine P and Michalski, Mark H and {Gilberto Gonzalez}, R and Pomerantz, Stuart R},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2018 - DEEP SPINE AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP LE.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2018 - DEEP SPINE AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP(2).pdf:pdf},
journal = {Proceedings of Machine Learning Research},
pages = {1--16},
title = {{DEEP SPINE: AUTOMATED LUMBAR VERTEBRAL SEGMENTATION, DISC-LEVEL DESIGNATION, AND SPINAL STENOSIS GRADING USING DEEP LEARNING}},
volume = {85},
year = {2018}
}
@inproceedings{Nett2008,
author = {Nett, Brian and Tang, Jie and Leng, Shuai and Chen, Guang-Hong},
booktitle = {Medical Imaging 2008: Physics of Medical Imaging},
doi = {10.1117/12.771294},
editor = {Hsieh, Jiang and Samei, Ehsan},
publisher = {SPIE},
title = {{Tomosynthesis via total variation minimization reconstruction and prior image constrained compressed sensing ({\{}PICCS{\}}) on a C-arm system}},
url = {https://doi.org/10.1117/12.771294},
year = {2008}
}
@article{Cicek2016,
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf},
journal = {CoRR},
keywords = {Segmentation,Sparse annotation},
mendeley-tags = {Segmentation,Sparse annotation},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
volume = {abs/1606.0},
year = {2016}
}
@article{Chang2020,
abstract = {Multi-vertebrae segmentation plays an important role in spine diseases diagnosis and treatment planning. Global spatial dependencies between vertebrae are essential prior information for automatic multi-vertebrae segmentation. However, due to the lack of global information, previous methods have to localize specific vertebrae regions first, then segment and recognize the vertebrae in the region, resulting in a reduction in feature reuse and increase in computation. In this paper, we propose to leverage both global spatial and label information for multi-vertebrae segmentation from arbitrary MR images in one go. Specifically, a spatial graph convolutional network (GCN) is designed to first automatically learn an adjacency matrix and construct a graph on local feature maps, then adopt stacked GCN to capture the global spatial relationships between vertebrae. A label attention network is built to predict the appearance probabilities of all vertebrae using attention mechanism to reduce the ambiguity caused by variant FOV or similar appearances of adjacent vertebrae. The proposed method is trained in an end-to-end manner and evaluated on a challenging dataset of 292 MRI scans with various fields of view, image characteristics and vertebra deformations. The experimental results show that our method achieves high performance (89.28 ± 5.21 of IDR and 85.37 ± 4.09 {\%} of mIoU) from arbitrary input images.},
author = {Chang, Heyou and Zhao, Shen and Zheng, Hao and Chen, Yang and Li, Shuo},
doi = {10.1007/978-3-030-59725-2_68},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - 2020 - Multi-vertebrae Segmentation from Arbitrary Spine MR Images Under Global View.pdf:pdf},
isbn = {9783030597245},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Attention network,Global information,Graph convolutional network,Multi-vertebrae segmentation},
number = {July},
pages = {702--711},
title = {{Multi-vertebrae Segmentation from Arbitrary Spine MR Images Under Global View}},
volume = {12266 LNCS},
year = {2020}
}
@techreport{Suzani,
abstract = {Segmentation of vertebral structures in magnetic resonance (MR) images is challenging because of poor contrast between bone surfaces and surrounding soft tissue. This paper describes a semi-automatic method for segmenting vertebral bodies in multi-slice MR images. In order to achieve a fast and reliable segmentation, the method takes advantage of the correlation between shape and pose of different vertebrae in the same patient by using a statistical multi-vertebrae anatomical shape+pose model. Given a set of MR images of the spine, we initially reduce the intensity inhomogeneity in the images by using an intensity-correction algorithm. Then a 3D anisotropic diffusion filter smooths the images. Afterwards, we extract edges from a relatively small region of the pre-processed image with a simple user interaction. Subsequently, an iterative Expectation Maximization technique is used to register the statistical multi-vertebrae anatomical model to the extracted edge points in order to achieve a fast and reliable segmentation for lumbar vertebral bodies. We evaluate our method in terms of speed and accuracy by applying it to volumetric MR images of the spine acquired from nine patients. Quantitative and visual results demonstrate that the method is promising for segmentation of vertebral bodies in volumetric MR images.},
annote = {Data provided by Dr. Shuo Li,GE Health Research, Ontario, Canada.},
author = {Suzani, Amin and Rasoulian, Abtin and Fels, Sidney and Rohling, Robert N and Abolmaesumi, Purang},
doi = {https://doi.org/10.1117/12.2043847},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzani et al. - Unknown - Semi-automatic Segmentation of Vertebral Bodies in Volumetric MR Images Using a Statistical ShapePose Model.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suzani et al. - Unknown - Semi-automatic Segmentation of Vertebral Bodies in Volumetric MR Images Using a Statistical ShapePose Model(2).pdf:pdf},
keywords = {Segmentation,multi-vertebrae anatomical model,vertebral body,volumetric MR image},
title = {{Semi-automatic Segmentation of Vertebral Bodies in Volumetric MR Images Using a Statistical Shape+Pose Model}},
url = {https://www.researchgate.net/publication/269313420{\_}Semi-automatic{\_}segmentation{\_}of{\_}vertebral{\_}bodies{\_}in{\_}volumetric{\_}MR{\_}images{\_}using{\_}a{\_}statistical{\_}shapepose{\_}model}
}
@article{Zhao2020,
abstract = {Comprehensive vertebrae tumor diagnosis (vertebrae recognition and vertebrae tumor diagnosis from MRI images) is crucial for tumor screening and preventing further metastasis. However, this task has not yet been attempted due to challenges caused by various tumor appearance, non-tumor diseases with similar appearance, irrelevant interference information, as well as diverse MRI image field of view (FOV) and/or characteristics. We purpose a discriminative dictionary-embedded network (DECIDE) that contains an elaborated enhanced-supervision recognition network (ERN) and a discerning diagnosis network (DDN). Our ERN creatively designs projection-guided dictionary learning to leverage projections of angular point coordinates onto multiple observation axes for enhanced supervision and discriminability of different vertebrae. DDN integrates a novel label consistent dictionary learning layer into a classification network to obtain more discerning sparse codes for diagnosing performance improvement. DECIDE is trained and evaluated using a very challenging dataset consisted of 600 MRI images; the evaluation results show that DECIDE achieves high performance in both recognition (accuracy: 0.928) and diagnosis (AUC: 0.96) tasks.},
author = {Zhao, Shen and Chen, Bin and Chang, Heyou and Wu, Xi and Li, Shuo},
doi = {10.1007/978-3-030-59725-2_67},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2020 - Discriminative Dictionary-Embedded Network for Comprehensive Vertebrae Tumor Diagnosis.pdf:pdf},
isbn = {9783030597245},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Dictionary embedded deep learning,Vertebrae recognition,Vertebrae tumor diagnosis},
pages = {691--701},
title = {{Discriminative Dictionary-Embedded Network for Comprehensive Vertebrae Tumor Diagnosis}},
volume = {12266 LNCS},
year = {2020}
}
@article{Selvaraju2016,
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1610.02391},
journal = {CoRR},
title = {{Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391},
volume = {abs/1610.0},
year = {2016}
}
@article{Sekuboyina2020,
abstract = {Reliable automated processing of spinal images is expected to benefit decision-support systems for diagnosis, surgery planning, and population-based analysis on spine and bone health. Vertebral labelling and segmentation are two fundamental tasks in such an automated pipeline. Centred around these tasks, the Large Scale Vertebrae Segmentation Challenge (VerSe) was organised in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019. This work is a technical report summarising the challenge's findings. A total of 160 multi-detector CT scans closely resembling a typical spine-centred clinical setting were prepared and annotated at voxel-level by a human-machine hybrid algorithm. Both the annotation protocol and the algorithm that aided the medical experts in this annotation process are presented. Eleven fully automated algorithms of the participating teams were benchmarked on the VerSe data. A detailed performance comparison of these algorithms along with insights into their design are presented. The best-performing algorithm achieved a vertebrae identification rate of 95$\backslash${\%} and a Dice coefficient of 90{\%} on a hidden test set. As an open-call challenge, VerSe'19's annotated image data and its evaluation tools will continue to be publicly accessible through its online portal.},
archivePrefix = {arXiv},
arxivId = {2001.09193},
author = {Sekuboyina, Anjany and Bayat, Amirhossein and Husseini, Malek E. and L{\"{o}}ffler, Maximilian and Li, Hongwei and Tetteh, Giles and Kuka{\v{c}}ka, Jan and Payer, Christian and {\v{S}}tern, Darko and Urschler, Martin and Chen, Maodong and Cheng, Dalong and Lessmann, Nikolas and Hu, Yujin and Wang, Tianfu and Yang, Dong and Xu, Daguang and Ambellan, Felix and Amiranashvili, Tamaz and Ehlke, Moritz and Lamecker, Hans and Lehnert, Sebastian and Lirio, Marilia and de Olaguer, Nicol{\'{a}}s P{\'{e}}rez and Ramm, Heiko and Sahu, Manish and Tack, Alexander and Zachow, Stefan and Jiang, Tao and Ma, Xinjun and Angerman, Christoph and Wang, Xin and Wei, Qingyue and Brown, Kevin and Wolf, Matthias and Kirszenberg, Alexandre and Puybareauq, {\'{E}}lodie and Valentinitsch, Alexander and Rempfler, Markus and Menze, Bj{\"{o}}rn H. and Kirschke, Jan S.},
eprint = {2001.09193},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2020 - VerSe A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images.pdf:pdf},
number = {January},
title = {{VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images}},
url = {http://arxiv.org/abs/2001.09193},
year = {2020}
}
@article{Vania2019,
abstract = {There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed a fully automatic segmentation method to segment the spine from CT images, and we compared our segmentation results with reference segmentations obtained by well-known methods. We use a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94$\backslash$$\backslash${\%}), Jaccard index (93$\backslash$$\backslash${\%}), volumetric similarity (96$\backslash$$\backslash${\%}), sensitivity (97$\backslash$$\backslash${\%}), specificity (99$\backslash$$\backslash${\%}), precision (over segmentation 8.3 and under segmentation 2.6), accuracy (99$\backslash$$\backslash${\%}), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.HighlightsA method to enhance the accuracy of spine segmentation from CT data was proposed.The proposed method uses Convolutional Neural Network via redundant generation of class labels.Experiments show the segmentation accuracy has been enhanced.},
author = {Vania, Malinda and Mureja, Dawit and Lee, Deukhee},
doi = {10.1016/j.jcde.2018.05.002},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vania, Mureja, Lee - 2019 - Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of c.pdf:pdf},
issn = {2288-5048},
journal = {Journal of Computational Design and Engineering},
number = {2},
pages = {224--232},
title = {{Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of class labels}},
url = {https://doi.org/10.1016/j.jcde.2018.05.002},
volume = {6},
year = {2019}
}
@article{Chan2020,
abstract = {Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis.},
archivePrefix = {arXiv},
arxivId = {1912.11186},
author = {Chan, Lyndon and Hosseini, Mahdi S. and Plataniotis, Konstantinos N.},
doi = {10.1007/s11263-020-01373-4},
eprint = {1912.11186},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hosseini, Plataniotis - 2020 - A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hosseini, Plataniotis - 2020 - A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains(2).pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural network,Deep learning,Digital pathology,Natural imaging,Satellite imaging,Self-supervised Learning,Weakly supervised semantic segmentation},
title = {{A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains}},
year = {2020}
}
@article{Cai2015,
abstract = {Computer-aided diagnosis of spine problems relies on the automatic identification of spine structures in images. The task of automatic vertebra recognition is to identify the global spine and local vertebra structural information such as spine shape, vertebra location and pose. Vertebra recognition is challenging due to the large appearance variations in different image modalities/views and the high geometric distortions in spine shape. Existing vertebra recognitions are usually simplified as vertebrae detections, which mainly focuses on the identification of vertebra locations and labels but cannot support further spine quantitative assessment. In this paper, we propose a vertebra recognition method using 3D deformable hierarchical model (DHM) to achieve cross-modality local vertebra location+pose identification with accurate vertebra labeling, and global 3D spine shape recovery. We recast vertebra recognition as deformable model matching, fitting the input spine images with the 3D DHM via deformations. The 3D model-matching mechanism provides a more comprehensive vertebra location+pose+label simultaneous identification than traditional vertebra location+label detection, and also provides an articulated 3D mesh model for the input spine section. Moreover, DHM can conduct versatile recognition on volume and multi-slice data, even on single slice. Experiments show our method can successfully extract vertebra locations, labels, and poses from multi-slice T1/T2 MR and volume CT, and can reconstruct 3D spine model on different image views such as lumbar, cervical, even whole spine. The resulting vertebra information and the recovered shape can be used for quantitative diagnosis of spine problems and can be easily digitalized and integrated in modern medical PACS systems.},
author = {Cai, Yunliang and Osman, Said and Sharma, Manas and Landis, Mark and Li, Shuo},
doi = {10.1109/TMI.2015.2392054},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai et al. - 2015 - Multi-Modality Vertebra Recognition in Arbitrary Views Using 3D Deformable Hierarchical Model.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Spine recognition,vertebra detection,vertebra pose estimation,vertebra segmentation},
number = {8},
pages = {1676--1693},
pmid = {25594966},
title = {{Multi-Modality Vertebra Recognition in Arbitrary Views Using 3D Deformable Hierarchical Model}},
volume = {34},
year = {2015}
}
@article{Lu2020,
abstract = {Accurate segmentation of anatomical structures is vital for medical image analysis. The state-of-the-art accuracy is typically achieved by supervised learning methods, where gathering the requisite expert-labeled image annotations in a scalable manner remains a main obstacle. Therefore, annotation-efficient methods that permit to produce accurate anatomical structure segmentation are highly desirable. In this work, we present Contour Transformer Network (CTN), a one-shot anatomy segmentation method with a naturally built-in human-in-the-loop mechanism. We formulate anatomy segmentation as a contour evolution process and model the evolution behavior by graph convolutional networks (GCNs). Training the CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. On segmentation tasks of four different anatomies, we demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning methods. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved to surpass the fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {2012.01480},
author = {Lu, Yuhang and Zheng, Kang and Li, Weijian and Wang, Yirui and Harrison, Adam P. and Lin, Chihung and Wang, Song and Xiao, Jing and Lu, Le and Kuo, Chang Fu and Miao, Shun},
doi = {10.1109/TMI.2020.3043375},
eprint = {2012.01480},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2020 - Contour Transformer Network for One-shot Segmentation of Anatomical Structures.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Graph Convolutional Network,Human-in-the-loop,Image Segmentation,One-shot Segmentation},
number = {Xx},
pages = {1--13},
pmid = {33290215},
title = {{Contour Transformer Network for One-shot Segmentation of Anatomical Structures}},
volume = {XX},
year = {2020}
}
@article{Lindner2017,
abstract = {We address the challenge of model transfer learning for a shape model matching (SMM) system. The goal is to adapt an existing SMM system to work effectively with new data without rebuilding the system from scratch. Recently, several SMM systems have been proposed that combine the outcome of a Random Forest (RF) regression step with shape constraints. These methods have been shown to lead to accurate and robust results when applied to the localisation of landmarks annotating skeletal structures in radiographs. However, as these methods contain a supervised learning component, their performance heavily depends on the data that was used to train the system, limiting their applicability to a new dataset with different properties. Here we show how to tune an existing SMM system by both updating the RFs with new samples and re-estimating the shape model. We demonstrate the effectiveness of tuning a cephalometric SMM system to replicate the annotation style of a new observer. Our results demonstrate that tuning an existing system leads to significant improvements in performance on new data, up to the extent of performing a well as a system that was fully rebuilt using samples from the new dataset. The proposed approach is fast and does not require access to the original training data.},
author = {Lindner, C. and Waring, D. and Thiruvenkatachari, B. and O'Brien, K. and Cootes, T. F.},
doi = {10.1007/978-3-319-66182-7_17},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindner et al. - 2017 - Adaptable landmark localisation Applying model transfer learning to a shape model matching system.pdf:pdf},
isbn = {9783319661810},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Landmark localisation,Machine learning,Model transfer learning,Model tuning,Random Forests,Statistical shape models},
pages = {144--151},
title = {{Adaptable landmark localisation: Applying model transfer learning to a shape model matching system}},
volume = {10433 LNCS},
year = {2017}
}
@article{He2020,
abstract = {Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions' disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge show great advantages of our DeepRS that outperforms the existing state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {2008.00710},
author = {He, Yuting and Li, Tiantian and Yang, Guanyu and Kong, Youyong and Chen, Yang and Shu, Huazhong and Coatrieux, Jean-Louis and Dillenseger, Jean-Louis and Li, Shuo},
eprint = {2008.00710},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2020 - Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images.pdf:pdf},
pages = {1--17},
title = {{Deep Complementary Joint Model for Complex Scene Registration and Few-shot Segmentation on Medical Images}},
url = {http://arxiv.org/abs/2008.00710},
year = {2020}
}
@article{Huang2013,
author = {Huang, Jing and Zhang, Yunwan and Ma, Jianhua and Zeng, Dong and Bian, Zhaoying and Niu, Shanzhou and Feng, Qianjin and Liang, Zhengrong and Chen, Wufan},
doi = {10.1371/journal.pone.0079709},
editor = {Wang, Ge},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2013 - Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior.pdf:pdf},
journal = {{\{}PLoS{\}} {\{}ONE{\}}},
keywords = {Tomography},
mendeley-tags = {Tomography},
month = {nov},
number = {11},
pages = {e79709},
publisher = {Public Library of Science ({\{}PLoS{\}})},
title = {{Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior}},
url = {https://doi.org/10.1371/journal.pone.0079709},
volume = {8},
year = {2013}
}
@article{Glocker2012,
abstract = {This paper presents a new method for automatic localization and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81{\%}.},
author = {Glocker, Ben and Feulner, J. and Criminisi, Antonio and Haynor, D. R. and Konukoglu, E.},
doi = {10.1007/978-3-642-33454-2_73},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2012 - Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans.pdf:pdf},
isbn = {9783642334535},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {590--598},
pmid = {23286179},
title = {{Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans}},
volume = {7512 LNCS},
year = {2012}
}
@article{Hyun2018,
author = {Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
doi = {10.1088/1361-6560/aac71a},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyun et al. - 2018 - Deep learning for undersampled {\{}MRI{\}} reconstruction.pdf:pdf},
journal = {Physics in Medicine {\&} Biology},
keywords = {Reconstruction,Tomography},
mendeley-tags = {Reconstruction,Tomography},
number = {13},
pages = {135007},
publisher = {{\{}IOP{\}} Publishing},
title = {{Deep learning for undersampled {\{}MRI{\}} reconstruction}},
url = {https://doi.org/10.1088/1361-6560/aac71a},
volume = {63},
year = {2018}
}
@article{Jimenez-Sanchez2018,
abstract = {In this paper, we target the problem of fracture classification from clinical X-Ray images towards an automated Computer Aided Diagnosis (CAD) system. Although primarily dealing with an image classification problem, we argue that localizing the fracture in the image is crucial to make good class predictions. Therefore, we propose and thoroughly analyze several schemes for simultaneous fracture localization and classification. We show that using an auxiliary localization task, in general, improves the classification performance. Moreover, it is possible to avoid the need for additional localization annotations thanks to recent advancements in weakly-supervised deep learning approaches. Among such approaches, we investigate and adapt Spatial Transformers (ST), Self-Transfer Learning (STL), and localization from global pooling layers. We provide a detailed quantitative and qualitative validation on a dataset of 1347 femur fractures images and report high accuracy with regard to inter-expert correlation values reported in the literature. Our investigations show that i) lesion localization improves the classification outcome, ii) weakly-supervised methods improve baseline classification without any additional cost, iii) STL guides feature activations and boost performance. We plan to make both the dataset and code available.},
archivePrefix = {arXiv},
arxivId = {1809.10692},
author = {Jim{\'{e}}nez-S{\'{a}}nchez, Amelia and Kazi, Anees and Albarqouni, Shadi and Kirchhoff, Sonja and Str{\"{a}}ter, Alexandra and Biberthaler, Peter and Mateus, Diana and Navab, Nassir},
eprint = {1809.10692},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jim{\'{e}}nez-S{\'{a}}nchez et al. - 2018 - Weakly-Supervised localization and classification of proximal femur fractures.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Attention models,Deep learning,Fracture classification,Multi-task learning,Weak-supervision,X-ray},
pages = {1--7},
title = {{Weakly-Supervised localization and classification of proximal femur fractures}},
year = {2018}
}
@article{Laradji2020,
abstract = {Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.06392v1},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1109/icip40778.2020.9190782},
eprint = {arXiv:1906.06392v1},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - Proposal-Based Instance Segmentation With Point Supervision.pdf:pdf},
number = {1},
pages = {2126--2130},
title = {{Proposal-Based Instance Segmentation With Point Supervision}},
year = {2020}
}
@article{McEver2020,
abstract = {Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset $\backslash$cite{\{}everingham2010pascal{\}}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.},
archivePrefix = {arXiv},
arxivId = {2007.05615},
author = {McEver, R. Austin and Manjunath, B. S.},
eprint = {2007.05615},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision.pdf:pdf},
title = {{PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision}},
url = {http://arxiv.org/abs/2007.05615},
year = {2020}
}
@article{Ronneberger2015,
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation(2).pdf:pdf},
journal = {CoRR},
keywords = {Medical,Segmentation},
mendeley-tags = {Medical,Segmentation},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {abs/1505.0},
year = {2015}
}
@article{Huang2013,
author = {Huang, Jing and Zhang, Yunwan and Ma, Jianhua and Zeng, Dong and Bian, Zhaoying and Niu, Shanzhou and Feng, Qianjin and Liang, Zhengrong and Chen, Wufan},
doi = {10.1371/journal.pone.0079709},
editor = {Wang, Ge},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2013 - Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior.pdf:pdf},
journal = {{\{}PLoS{\}} {\{}ONE{\}}},
keywords = {Tomography},
mendeley-tags = {Tomography},
month = {nov},
number = {11},
pages = {e79709},
publisher = {Public Library of Science ({\{}PLoS{\}})},
title = {{Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior}},
url = {https://doi.org/10.1371/journal.pone.0079709},
volume = {8},
year = {2013}
}
@article{DasCLuciaMS2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {doi:10.1016/j.brs.2016.03.010 {Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki TS. The Clinical TMS Society Consensus Review and Treatment Recommendations for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surg(2).pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@article{Hyun2018,
author = {Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
doi = {10.1088/1361-6560/aac71a},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyun et al. - 2018 - Deep learning for undersampled {\{}MRI{\}} reconstruction.pdf:pdf},
journal = {Physics in Medicine {\&} Biology},
keywords = {Reconstruction,Tomography},
mendeley-tags = {Reconstruction,Tomography},
number = {13},
pages = {135007},
publisher = {{\{}IOP{\}} Publishing},
title = {{Deep learning for undersampled {\{}MRI{\}} reconstruction}},
url = {https://doi.org/10.1088/1361-6560/aac71a},
volume = {63},
year = {2018}
}
@article{Laradji2020,
abstract = {Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.06392v1},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1109/icip40778.2020.9190782},
eprint = {arXiv:1906.06392v1},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - Proposal-Based Instance Segmentation With Point Supervision.pdf:pdf},
number = {1},
pages = {2126--2130},
title = {{Proposal-Based Instance Segmentation With Point Supervision}},
year = {2020}
}
@article{Vania2019,
abstract = {There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed a fully automatic segmentation method to segment the spine from CT images, and we compared our segmentation results with reference segmentations obtained by well-known methods. We use a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94$\backslash$$\backslash${\%}), Jaccard index (93$\backslash$$\backslash${\%}), volumetric similarity (96$\backslash$$\backslash${\%}), sensitivity (97$\backslash$$\backslash${\%}), specificity (99$\backslash$$\backslash${\%}), precision (over segmentation 8.3 and under segmentation 2.6), accuracy (99$\backslash$$\backslash${\%}), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.HighlightsA method to enhance the accuracy of spine segmentation from CT data was proposed.The proposed method uses Convolutional Neural Network via redundant generation of class labels.Experiments show the segmentation accuracy has been enhanced.},
author = {Vania, Malinda and Mureja, Dawit and Lee, Deukhee},
doi = {10.1016/j.jcde.2018.05.002},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vania, Mureja, Lee - 2019 - Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of c.pdf:pdf},
issn = {2288-5048},
journal = {Journal of Computational Design and Engineering},
number = {2},
pages = {224--232},
title = {{Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of class labels}},
url = {https://doi.org/10.1016/j.jcde.2018.05.002},
volume = {6},
year = {2019}
}
@inproceedings{Nett2008,
author = {Nett, Brian and Tang, Jie and Leng, Shuai and Chen, Guang-Hong},
booktitle = {Medical Imaging 2008: Physics of Medical Imaging},
doi = {10.1117/12.771294},
editor = {Hsieh, Jiang and Samei, Ehsan},
publisher = {SPIE},
title = {{Tomosynthesis via total variation minimization reconstruction and prior image constrained compressed sensing ({\{}PICCS{\}}) on a C-arm system}},
url = {https://doi.org/10.1117/12.771294},
year = {2008}
}
@article{Lessmann2018,
abstract = {Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.},
archivePrefix = {arXiv},
arxivId = {1804.04383},
author = {Lessmann, Nikolas and van Ginneken, Bram and de Jong, Pim A. and I{\v{s}}gum, Ivana},
doi = {10.1016/j.media.2019.02.005},
eprint = {1804.04383},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.pdf:pdf},
keywords = {Spine segmentation},
mendeley-tags = {Spine segmentation},
month = {apr},
title = {{Iterative fully convolutional neural networks for automatic vertebra segmentation and identification}},
url = {http://arxiv.org/abs/1804.04383 http://dx.doi.org/10.1016/j.media.2019.02.005},
year = {2018}
}
@article{Cicek2016,
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf},
journal = {CoRR},
keywords = {Segmentation,Sparse annotation},
mendeley-tags = {Segmentation,Sparse annotation},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
volume = {abs/1606.0},
year = {2016}
}
@article{Selvaraju2016,
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1610.02391},
journal = {CoRR},
title = {{Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391},
volume = {abs/1610.0},
year = {2016}
}
@article{Chan2020,
abstract = {Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis.},
archivePrefix = {arXiv},
arxivId = {1912.11186},
author = {Chan, Lyndon and Hosseini, Mahdi S. and Plataniotis, Konstantinos N.},
doi = {10.1007/s11263-020-01373-4},
eprint = {1912.11186},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan, Hosseini, Plataniotis - 2020 - A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural network,Deep learning,Digital pathology,Natural imaging,Satellite imaging,Self-supervised Learning,Weakly supervised semantic segmentation},
title = {{A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains}},
year = {2020}
}

% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.9 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{Ahn2019}{article}{}
      \name{author}{3}{}{%
        {{hash=c4d97e02fad399ecbb41e726d316376d}{%
           family={Ahn},
           familyi={A\bibinitperiod},
           given={Jiwoon},
           giveni={J\bibinitperiod}}}%
        {{hash=8e63c907a56984c6c13caf552f5b31f5}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Sunghyun},
           giveni={S\bibinitperiod}}}%
        {{hash=ab763a8974b0572d036873fd5f58d1aa}{%
           family={Kwak},
           familyi={K\bibinitperiod},
           given={Suha},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{ed526662f7e636666e2e754a830c158c}
      \strng{fullhash}{ed526662f7e636666e2e754a830c158c}
      \strng{bibnamehash}{ed526662f7e636666e2e754a830c158c}
      \strng{authorbibnamehash}{ed526662f7e636666e2e754a830c158c}
      \strng{authornamehash}{ed526662f7e636666e2e754a830c158c}
      \strng{authorfullhash}{ed526662f7e636666e2e754a830c158c}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with inter-pixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781728132938}
      \field{issn}{10636919}
      \field{journaltitle}{Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
      \field{title}{{Weakly supervised learning of instance segmentation with inter-pixel relations}}
      \field{volume}{2019-June}
      \field{year}{2019}
      \field{pages}{2204\bibrangedash 2213}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2019.00231
      \endverb
      \verb{eprint}
      \verb 1904.05044
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahn, Cho, Kwak - 2019 - Weakly supervised learning of instance segmentation with inter-pixel relations.pdf:pdf
      \endverb
      \keyw{Categorization,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation}
    \endentry
    \entry{Alam2014}{article}{}
      \name{author}{4}{}{%
        {{hash=411c9c5da5d4b93c37c58a32c1b39624}{%
           family={Alam},
           familyi={A\bibinitperiod},
           given={Muhammad\bibnamedelima M.},
           giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=085cff9695139fd8612829bd18c01d91}{%
           family={Waqas},
           familyi={W\bibinitperiod},
           given={Muhammad},
           giveni={M\bibinitperiod}}}%
        {{hash=87e4f858fd62d1f2eeb08f302f4d2630}{%
           family={Shallwani},
           familyi={S\bibinitperiod},
           given={Hussain},
           giveni={H\bibinitperiod}}}%
        {{hash=b6cb988aea32dfe48fb402c2c28e17b2}{%
           family={Javed},
           familyi={J\bibinitperiod},
           given={Gohar},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{9a418e2a1f3bf1d2daa6d490864e0e28}
      \strng{fullhash}{280a2480b8aa5ed3b775cd5da0964bda}
      \strng{bibnamehash}{9a418e2a1f3bf1d2daa6d490864e0e28}
      \strng{authorbibnamehash}{9a418e2a1f3bf1d2daa6d490864e0e28}
      \strng{authornamehash}{9a418e2a1f3bf1d2daa6d490864e0e28}
      \strng{authorfullhash}{280a2480b8aa5ed3b775cd5da0964bda}
      \field{sortinit}{A}
      \field{sortinithash}{d77c7cdd82ff690d4c3ef13216f92f0b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Study Design: A cross-sectional study. Purpose: To describe the characteristics of lumbar vertebrae of Pakistani patients reporting at a tertiary care hospital and compare with studies from other populations. Overview of Literature: Several studies have been conducted to determine morphometry of lumbar vertebrae. Most of the studies involve Caucasian populations, still data on other populations still sparse. This is the first study describing lumbar morphometry of a Pakistani population. Methods: An observational study was conducted based on a review of thin-cut (3 mm) computed topographic images of lumbar vertebrae. Two-hundred and twenty vertebrae from forty-nine patients were studied, and various dimensions were analyzed. Results: Generally, the size of the vertebrae, vertebral canals and recesses were found to be greater in male patients. The difference was statistically significant for transverse and anteroposterior diameters of the vertebral bodies and sagittal diameter of pedicles on the left side (p {<}0.05). Comparison of populations revealed statistically significant differences in pedicle dimensions between Pakistani population and others. Conclusions: This study provides anatomical knowledge of the lumbar region in a sample population of Pakistan. There were significant differences in various dimensions of lumbar vertebrae between female and male patients. This would prove to be critical for performing a safe operation. {©} 2014 by Korean Society of Spine Surgery.}
      \field{issn}{19767846}
      \field{journaltitle}{Asian Spine Journal}
      \field{number}{4}
      \field{title}{{Lumbar morphometry: A study of lumbar vertebrae from a Pakistani population using computed tomography scans}}
      \field{volume}{8}
      \field{year}{2014}
      \field{pages}{421\bibrangedash 426}
      \range{pages}{6}
      \verb{doi}
      \verb 10.4184/asj.2014.8.4.421
      \endverb
      \verb{file}
      \verb :home/jan/Downloads/asj-8-421.pdf:pdf
      \endverb
      \keyw{Lumbar vertebra,Pakistan,Pakistani population,Vertebral dimensions}
    \endentry
    \entry{Bearman2015}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=5155a3548e01d929465f663f1b3297e1}{%
           family={Bearman},
           familyi={B\bibinitperiod},
           given={Amy},
           giveni={A\bibinitperiod}}}%
        {{hash=2a74ec28b731b015747e2af5f0b519e1}{%
           family={Russakovsky},
           familyi={R\bibinitperiod},
           given={Olga},
           giveni={O\bibinitperiod}}}%
        {{hash=2d9b2bb140830bf8f7f642c17692a8e4}{%
           family={Ferrari},
           familyi={F\bibinitperiod},
           given={Vittorio},
           giveni={V\bibinitperiod}}}%
        {{hash=cd00ce5bc45f687c432e52e0fa1a7aa6}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer, Cham}%
      }
      \strng{namehash}{9229c14a9d01f41add2565e35739f764}
      \strng{fullhash}{01bc8b1db84fa6b0d17a3db8d16e8d3a}
      \strng{bibnamehash}{9229c14a9d01f41add2565e35739f764}
      \strng{authorbibnamehash}{9229c14a9d01f41add2565e35739f764}
      \strng{authornamehash}{9229c14a9d01f41add2565e35739f764}
      \strng{authorfullhash}{01bc8b1db84fa6b0d17a3db8d16e8d3a}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The semantic image segmentation task presents a trade-off between test time accuracy and training time annotation cost. Detailed per-pixel annotations enable training accurate models but are very timeconsuming to obtain; image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9{\%} mIOU over image-level supervision. Further, we demonstrate that models trained with pointlevel supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{eprinttype}{arXiv}
      \field{isbn}{9783319464770}
      \field{issn}{16113349}
      \field{month}{6}
      \field{title}{{What's the point: Semantic segmentation with point supervision}}
      \field{volume}{9911 LNCS}
      \field{year}{2016}
      \field{pages}{549\bibrangedash 565}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-319-46478-7_34
      \endverb
      \verb{eprint}
      \verb 1506.02106
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bearman et al. - 2015 - What's the Point Semantic Segmentation with Point Supervision.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1506.02106
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1506.02106
      \endverb
      \keyw{Data annotation,Semantic segmentation,Weak supervision}
    \endentry
    \entry{ECCV2020}{misc}{}
      \name{author}{3}{}{%
        {{hash=031a77665be90cf046c10d5a9e190bf0}{%
           family={Bilen},
           familyi={B\bibinitperiod},
           given={Hakan},
           giveni={H\bibinitperiod}}}%
        {{hash=b5880fea5be303703b95e8314f4e40f8}{%
           family={Rodrigo},
           familyi={R\bibinitperiod},
           given={Benenson},
           giveni={B\bibinitperiod}}}%
        {{hash=a040c8bc71c6007bc28e913f51c37d03}{%
           family={Joon\bibnamedelima oh},
           familyi={J\bibinitperiod\bibinitdelim o\bibinitperiod},
           given={Seong},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \strng{fullhash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \strng{bibnamehash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \strng{authorbibnamehash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \strng{authornamehash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \strng{authorfullhash}{5b4b0c2a3b1a2c46ccc511611c94ce9d}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep convolutional networks have become the go-to technique for a variety of computer vision task such as image classification, object detection, segmentation, key points detection, etc. These over-parameterized models are known to be data-hungry; tens of thousand of labelled examples are typically required. Since manual annotations are expensive, learning from “weaker” annotations (e.g. only image-level category labels to localize object instances by a bounding box) become key to expand the success of deep networks to new applications. This tutorial will provide an overview of weakly supervised learning methods in computer vision, and we will discuss the broad area of weakly supervised object recognition and its limitations of current state-of-the-art, evaluation metrics, and future ideas that will spur disruptive progress in the field of weakly supervised learning.}
      \field{booktitle}{ECCV'20 online}
      \field{title}{{ECCV 2020 Tutorial on Weakly-Supervised Learning in Computer Vision}}
      \field{year}{2020}
      \verb{urlraw}
      \verb https://github.com/hbilen/wsl-eccv20.github.io
      \endverb
      \verb{url}
      \verb https://github.com/hbilen/wsl-eccv20.github.io
      \endverb
    \endentry
    \entry{Burian2019}{article}{}
      \name{author}{13}{}{%
        {{hash=2c081a2d137a6aa9a3e8485f93c6375a}{%
           family={Burian},
           familyi={B\bibinitperiod},
           given={Egon},
           giveni={E\bibinitperiod}}}%
        {{hash=fc2e5085616a4a70218d1c1d928d885e}{%
           family={Rohrmeier},
           familyi={R\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=f370a991ac7ad7444c883031c198288a}{%
           family={Schlaeger},
           familyi={S\bibinitperiod},
           given={Sarah},
           giveni={S\bibinitperiod}}}%
        {{hash=9f932b979cd81aadefe27abde9ff2174}{%
           family={Dieckmeyer},
           familyi={D\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=1e6132391503376c49f7ea54f7c2b64a}{%
           family={Diefenbach},
           familyi={D\bibinitperiod},
           given={Maximilian},
           giveni={M\bibinitperiod}}}%
        {{hash=c0d7d2d4224d4dddd6946d7001e67502}{%
           family={Syv{ä}ri},
           familyi={S\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod}}}%
        {{hash=57660d8ee6017bc4d543e365bbf5a93c}{%
           family={Klupp},
           familyi={K\bibinitperiod},
           given={Elisabeth},
           giveni={E\bibinitperiod}}}%
        {{hash=c5615b164a9577009017da7272021450}{%
           family={Weidlich},
           familyi={W\bibinitperiod},
           given={Dominik},
           giveni={D\bibinitperiod}}}%
        {{hash=7fbc102b699525d3dcdac0b0ca540926}{%
           family={Zimmer},
           familyi={Z\bibinitperiod},
           given={Claus},
           giveni={C\bibinitperiod}}}%
        {{hash=9760597902256c25c37d4326242f8d8e}{%
           family={Rummeny},
           familyi={R\bibinitperiod},
           given={Ernst},
           giveni={E\bibinitperiod}}}%
        {{hash=f9e78f78315b065719043fa67de7ab06}{%
           family={Karampinos},
           familyi={K\bibinitperiod},
           given={Dimitrios},
           giveni={D\bibinitperiod}}}%
        {{hash=b54b06bd29d6f66d7eedc4ac70d177d4}{%
           family={Kirschke},
           familyi={K\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod}}}%
        {{hash=4aa23cdaa77efa2a8df343ea8c32cb8c}{%
           family={Baum},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{e07f2da765d59f6f8cf7b0b617aef0b8}
      \strng{fullhash}{b0883c2871e46c1b83d99fcf94e72085}
      \strng{bibnamehash}{e07f2da765d59f6f8cf7b0b617aef0b8}
      \strng{authorbibnamehash}{e07f2da765d59f6f8cf7b0b617aef0b8}
      \strng{authornamehash}{e07f2da765d59f6f8cf7b0b617aef0b8}
      \strng{authorfullhash}{b0883c2871e46c1b83d99fcf94e72085}
      \field{sortinit}{B}
      \field{sortinithash}{276475738cc058478c1677046f857703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{BMC Musculoskeletal Disorders}
      \field{title}{{Lumbar muscle and vertebral bodies segmentation of chemical shift encoding-based water-fat MRI: The reference database MyoSegmenTUM spine}}
      \field{volume}{20}
      \field{year}{2019}
      \verb{doi}
      \verb 10.1186/s12891-019-2528-x
      \endverb
    \endentry
    \entry{SemTorch76:online}{misc}{}
      \name{author}{1}{}{%
        {{hash=4e146af181e32fe72488e89287fd8333}{%
           family={Castillo},
           familyi={C\bibinitperiod},
           given={David\bibnamedelima Lacalle},
           giveni={D\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \strng{namehash}{4e146af181e32fe72488e89287fd8333}
      \strng{fullhash}{4e146af181e32fe72488e89287fd8333}
      \strng{bibnamehash}{4e146af181e32fe72488e89287fd8333}
      \strng{authorbibnamehash}{4e146af181e32fe72488e89287fd8333}
      \strng{authornamehash}{4e146af181e32fe72488e89287fd8333}
      \strng{authorfullhash}{4e146af181e32fe72488e89287fd8333}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{(Accessed on 02/14/2021)}
      \field{howpublished}{$\backslash$url{\{}https://pypi.org/project/SemTorch/{\}}}
      \field{month}{9}
      \field{title}{{SemTorch {·} PyPI}}
      \field{year}{2020}
    \endentry
    \entry{Chu2015}{article}{}
      \name{author}{6}{}{%
        {{hash=08764fdfe0301b3f68d0f8029d4d6609}{%
           family={Chu},
           familyi={C\bibinitperiod},
           given={Chengwen},
           giveni={C\bibinitperiod}}}%
        {{hash=aa5283efb0b397fdb814042d1712d3ab}{%
           family={Belav{ý}},
           familyi={B\bibinitperiod},
           given={Daniel\bibnamedelima L.},
           giveni={D\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=fc72db0bd8a1dd6be99aa3afe2d0e099}{%
           family={Armbrecht},
           familyi={A\bibinitperiod},
           given={Gabriele},
           giveni={G\bibinitperiod}}}%
        {{hash=6bf67daf9697d3c0a6a20a87d9fe2de2}{%
           family={Bansmann},
           familyi={B\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=cbade18cb8332c11c5978c2b9625a46e}{%
           family={Felsenberg},
           familyi={F\bibinitperiod},
           given={Dieter},
           giveni={D\bibinitperiod}}}%
        {{hash=8cef56fce3f93d97dde7ee25db4fafd2}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Guoyan},
           giveni={G\bibinitperiod}}}%
      }
      \name{editor}{1}{}{%
        {{hash=36fb6c75447c4923037f380ea0c5a595}{%
           family={Pham},
           familyi={P\bibinitperiod},
           given={Dzung},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{141fd706fb4b8b5c5be56a25792e868a}
      \strng{fullhash}{b9db084604d6a68b74960fea0661c97c}
      \strng{bibnamehash}{141fd706fb4b8b5c5be56a25792e868a}
      \strng{authorbibnamehash}{141fd706fb4b8b5c5be56a25792e868a}
      \strng{authornamehash}{141fd706fb4b8b5c5be56a25792e868a}
      \strng{authorfullhash}{b9db084604d6a68b74960fea0661c97c}
      \strng{editorbibnamehash}{36fb6c75447c4923037f380ea0c5a595}
      \strng{editornamehash}{36fb6c75447c4923037f380ea0c5a595}
      \strng{editorfullhash}{36fb6c75447c4923037f380ea0c5a595}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we address the problems of fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images. We propose a learning-based, unified random forest regression and classification framework to tackle these two problems. More specifically, in the first stage, the localization of 3D vertebral bodies is solved with random forest regression where we aggregate the votes from a set of randomly sampled image patches to get a probability map of the center of a target vertebral body in a given image. The resultant probability map is then further regularized by Hidden Markov Model (HMM) to eliminate potential ambiguity caused by the neighboring vertebral bodies. The output from the first stage allows us to define a region of interest (ROI) for the segmentation step, where we use random forest classification to estimate the likelihood of a voxel in the ROI being foreground or background. The estimated likelihood is combined with the prior probability, which is learned from a set of training data, to get the posterior probability of the voxel. The segmentation of the target vertebral body is then done by a binary thresholding of the estimated probability. We evaluated the present approach on two openly available datasets: 1) 3D T2-weighted spine MR images from 23 patients and 2) 3D spine CT images from 10 patients. Taking manual segmentation as the ground truth (each MR image contains at least 7 vertebral bodies from T11 to L5 and each CT image contains 5 vertebral bodies from L1 to L5), we evaluated the present approach with leave-one-out experiments. Specifically, for the T2-weighted MR images, we achieved for localization a mean error of 1.6 mm, and for segmentation a mean Dice metric of 88.7{\%} and a mean surface distance of 1.5 mm, respectively. For the CT images we achieved for localization a mean error of 1.9 mm, and for segmentation a mean Dice metric of 91.0{\%} and a mean surface distance of 0.9 mm, respectively.}
      \field{issn}{19326203}
      \field{journaltitle}{PLoS ONE}
      \field{month}{11}
      \field{number}{11}
      \field{title}{{Fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images via a learning-based method}}
      \field{volume}{10}
      \field{year}{2015}
      \field{pages}{e0143327}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1371/journal.pone.0143327
      \endverb
      \verb{urlraw}
      \verb https://dx.plos.org/10.1371/journal.pone.0143327
      \endverb
      \verb{url}
      \verb https://dx.plos.org/10.1371/journal.pone.0143327
      \endverb
    \endentry
    \entry{Chuang2019}{article}{}
      \name{author}{7}{}{%
        {{hash=36b2026f35fd6ae4f964412d4f7a29c9}{%
           family={Chuang},
           familyi={C\bibinitperiod},
           given={Cheng-Hung},
           giveni={C\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=b47e78275b6052c248fa36c6492723a0}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Chih-Yang},
           giveni={C\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=e9302cebe51672860d539c6e0cdf9834}{%
           family={Tsai},
           familyi={T\bibinitperiod},
           given={Yuan-Yu},
           giveni={Y\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=062a7b4f39aec9653a04c054c12eccc1}{%
           family={Lian},
           familyi={L\bibinitperiod},
           given={Zhen-You},
           giveni={Z\bibinithyphendelim Y\bibinitperiod}}}%
        {{hash=467359811287156e8c343a45995a7b05}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Hong-Xia},
           giveni={H\bibinithyphendelim X\bibinitperiod}}}%
        {{hash=ca5a8961417a9637cf9cd4274eff846b}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Chih-Chao},
           giveni={C\bibinithyphendelim C\bibinitperiod}}}%
        {{hash=fbdffd961969457cee0a306c55c73268}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Chung-Lin},
           giveni={C\bibinithyphendelim L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{001052b4641808dfe0e8179bac582b98}
      \strng{fullhash}{95c0618d7b7c829d2e7dd54de5679198}
      \strng{bibnamehash}{001052b4641808dfe0e8179bac582b98}
      \strng{authorbibnamehash}{001052b4641808dfe0e8179bac582b98}
      \strng{authornamehash}{001052b4641808dfe0e8179bac582b98}
      \strng{authorfullhash}{95c0618d7b7c829d2e7dd54de5679198}
      \field{sortinit}{C}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2169-3536}
      \field{journaltitle}{IEEE Access}
      \field{title}{{Efficient Triple Output Network for Vertebral Segmentation and Identification}}
      \field{volume}{7}
      \field{year}{2019}
      \field{pages}{117978\bibrangedash 117985}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/access.2019.2934325
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chuang et al. - 2019 - Efficient Triple Output Network for Vertebral Segmentation and Identification.pdf:pdf
      \endverb
      \keyw{xvertseg}
    \endentry
    \entry{Cicek2016}{article}{}
      \name{author}{5}{}{%
        {{hash=692aa062895572e4b4cac3bee5e0d92e}{%
           family={{Ç}i{ç}ek},
           familyi={Ç\bibinitperiod},
           given={{Ö}zg{ü}n},
           giveni={Ö\bibinitperiod}}}%
        {{hash=dd1cd1962dcb1b6e7ba1d5e3269c1a70}{%
           family={Abdulkadir},
           familyi={A\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
        {{hash=a816c7a659778b123f218184c05118ce}{%
           family={Lienkamp},
           familyi={L\bibinitperiod},
           given={Soeren\bibnamedelima S},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=b452a32296958371572717940f900884}{%
           family={Brox},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=8e46da9de9e53ea5d37089897d69cdd9}{%
           family={Ronneberger},
           familyi={R\bibinitperiod},
           given={Olaf},
           giveni={O\bibinitperiod}}}%
      }
      \strng{namehash}{8bdd3e5bb2f12ba1680d902ab2a03c3c}
      \strng{fullhash}{297baf9bc3bb377d5e47e1e03ae1af2f}
      \strng{bibnamehash}{8bdd3e5bb2f12ba1680d902ab2a03c3c}
      \strng{authorbibnamehash}{8bdd3e5bb2f12ba1680d902ab2a03c3c}
      \strng{authornamehash}{8bdd3e5bb2f12ba1680d902ab2a03c3c}
      \strng{authorfullhash}{297baf9bc3bb377d5e47e1e03ae1af2f}
      \field{sortinit}{Ç}
      \field{sortinithash}{963e9d84a3da2344e8833203de5aed05}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{CoRR}
      \field{title}{{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}}
      \field{volume}{abs/1606.0}
      \field{year}{2016}
      \verb{eprint}
      \verb 1606.06650
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1606.06650
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1606.06650
      \endverb
      \keyw{Segmentation,Sparse annotation}
    \endentry
    \entry{Glocker2012}{article}{}
      \name{author}{5}{}{%
        {{hash=5bc19448e90c8f3e88e5fbd50f12ac15}{%
           family={Glocker},
           familyi={G\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=77e631b58207ed2f797b21379eb230c1}{%
           family={Feulner},
           familyi={F\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=51503dff68ec68a34224e2be9281b7d3}{%
           family={Criminisi},
           familyi={C\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
        {{hash=c0d6dd730568373a4ddc4c05418d8ccc}{%
           family={Haynor},
           familyi={H\bibinitperiod},
           given={D.\bibnamedelimi R.},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=2cde383cb914affdd6f47e22b93fc4dc}{%
           family={Konukoglu},
           familyi={K\bibinitperiod},
           given={E.},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{fullhash}{d353744dd0769fb6405d2bb6ad568240}
      \strng{bibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorbibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authornamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorfullhash}{d353744dd0769fb6405d2bb6ad568240}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents a new method for automatic localization and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81{\%}.}
      \field{isbn}{9783642334535}
      \field{issn}{16113349}
      \field{journaltitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{title}{{Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans}}
      \field{volume}{7512 LNCS}
      \field{year}{2012}
      \field{pages}{590\bibrangedash 598}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1007/978-3-642-33454-2_73
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2012 - Automatic localization and identification of vertebrae in arbitrary field-of-view CT scans.pdf:pdf
      \endverb
    \endentry
    \entry{Glocker}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=5bc19448e90c8f3e88e5fbd50f12ac15}{%
           family={Glocker},
           familyi={G\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=13b07c284d6b6f094f24f10c7d10a565}{%
           family={Zikic},
           familyi={Z\bibinitperiod},
           given={Darko},
           giveni={D\bibinitperiod}}}%
        {{hash=f225ea6221588341169080776608bf01}{%
           family={Konukoglu},
           familyi={K\bibinitperiod},
           given={Ender},
           giveni={E\bibinitperiod}}}%
        {{hash=4f4576afef67d89b9b1099129d2e0fb7}{%
           family={Haynor},
           familyi={H\bibinitperiod},
           given={David\bibnamedelima R},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=51503dff68ec68a34224e2be9281b7d3}{%
           family={Criminisi},
           familyi={C\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{fullhash}{0c5520e260b8069d9f43812053dcb3de}
      \strng{bibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorbibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authornamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorfullhash}{0c5520e260b8069d9f43812053dcb3de}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit parametric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases. {©} 2013 Springer-Verlag.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783642407628}
      \field{issn}{03029743}
      \field{number}{PART 2}
      \field{title}{{Vertebrae localization in pathological spine CT via dense classification from sparse annotations}}
      \field{volume}{8150 LNCS}
      \field{year}{2013}
      \field{pages}{262\bibrangedash 270}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1007/978-3-642-40763-5_33
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - Unknown - Vertebrae Localization in Pathological Spine CT via Dense Classification from Sparse Annotations.pdf:pdf
      \endverb
    \endentry
    \entry{Glocker2013}{article}{}
      \name{author}{5}{}{%
        {{hash=5bc19448e90c8f3e88e5fbd50f12ac15}{%
           family={Glocker},
           familyi={G\bibinitperiod},
           given={Ben},
           giveni={B\bibinitperiod}}}%
        {{hash=13b07c284d6b6f094f24f10c7d10a565}{%
           family={Zikic},
           familyi={Z\bibinitperiod},
           given={Darko},
           giveni={D\bibinitperiod}}}%
        {{hash=f225ea6221588341169080776608bf01}{%
           family={Konukoglu},
           familyi={K\bibinitperiod},
           given={Ender},
           giveni={E\bibinitperiod}}}%
        {{hash=1e65673d35249bca32d78892c0d86c2c}{%
           family={Haynor},
           familyi={H\bibinitperiod},
           given={David\bibnamedelima R.},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=51503dff68ec68a34224e2be9281b7d3}{%
           family={Criminisi},
           familyi={C\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{fullhash}{d76b50729776bad1333c7d03b59baad2}
      \strng{bibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorbibnamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authornamehash}{2ec50a23da7ada82bf31fcb6223977a1}
      \strng{authorfullhash}{d76b50729776bad1333c7d03b59baad2}
      \field{sortinit}{G}
      \field{sortinithash}{5e8d2bf9d38de41b1528bd307546008f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit parametric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases. {©} 2013 Springer-Verlag.}
      \field{isbn}{9783642407628}
      \field{issn}{03029743}
      \field{journaltitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{number}{PART 2}
      \field{title}{{Vertebrae localization in pathological spine CT via dense classification from sparse annotations}}
      \field{volume}{8150 LNCS}
      \field{year}{2013}
      \field{pages}{262\bibrangedash 270}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1007/978-3-642-40763-5_33
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glocker et al. - 2013 - Vertebrae localization in pathological spine CT via dense classification from sparse annotations.pdf:pdf
      \endverb
    \endentry
    \entry{Ibragimov2014}{article}{}
      \name{author}{4}{}{%
        {{hash=10c514dc35b82df56949ff26f7b5bc8b}{%
           family={Ibragimov},
           familyi={I\bibinitperiod},
           given={B},
           giveni={B\bibinitperiod}}}%
        {{hash=fd04df0991f4cbf18f1e9328a52c35b5}{%
           family={Likar},
           familyi={L\bibinitperiod},
           given={B},
           giveni={B\bibinitperiod}}}%
        {{hash=98624dd31fb217e4eb0f667449be6328}{%
           family={Pernu{š}},
           familyi={P\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod}}}%
        {{hash=75e783dd75b0d6ee51ca3208f6bd23d5}{%
           family={Vrtovec},
           familyi={V\bibinitperiod},
           given={T},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{1be2547761e6c5f40f9f33a61608cb9f}
      \strng{fullhash}{a37370da4084792473ae77ca71790091}
      \strng{bibnamehash}{1be2547761e6c5f40f9f33a61608cb9f}
      \strng{authorbibnamehash}{1be2547761e6c5f40f9f33a61608cb9f}
      \strng{authornamehash}{1be2547761e6c5f40f9f33a61608cb9f}
      \strng{authorfullhash}{a37370da4084792473ae77ca71790091}
      \field{sortinit}{I}
      \field{sortinithash}{320bc8fe8101b9376f9f21cd507de0e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we propose a novel approach to landmark-based shape representation that is based on transportation theory, where landmarks are considered as sources and destinations, all possible landmark connections as roads, and established landmark connections as goods transported via these roads. Landmark connections, which are selectively established, are identified through their statistical properties describing the shape of the object of interest, and indicate the least costly roads for transporting goods from sources to destinations. From such a perspective, we introduce three novel shape representations that are combined with an existing landmark detection algorithm based on game theory. To reduce computational complexity, which results from the extension from 2-D to 3-D segmentation, landmark detection is augmented by a concept known in game theory as strategy dominance. The novel shape representations, game-theoretic landmark detection and strategy dominance are combined into a segmentation framework that was evaluated on 3-D computed tomography images of lumbar vertebrae and femoral heads. The best shape representation yielded symmetric surface distance of 0.75 mm and 1.11 mm, and Dice coefficient of 93.6{\%} and 96.2{\%} for lumbar vertebrae and femoral heads, respectively. By applying strategy dominance, the computational costs were further reduced for up to three times.}
      \field{issn}{1558-254X}
      \field{journaltitle}{IEEE Transactions on Medical Imaging}
      \field{month}{4}
      \field{number}{4}
      \field{title}{{Shape Representation for Efficient Landmark-Based Segmentation in 3-D}}
      \field{volume}{33}
      \field{year}{2014}
      \field{pages}{861\bibrangedash 874}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1109/TMI.2013.2296976
      \endverb
      \keyw{Three-Dimensional;Lumbar Vertebrae;Tomography,X-Ray Computed,bone;computerised tomography;diseases;game theory;}
    \endentry
    \entry{Janssens2018}{article}{}
      \name{author}{2}{}{%
        {{hash=69c228be9563c042a8399c40ad259cb1}{%
           family={Janssens},
           familyi={J\bibinitperiod},
           given={Rens},
           giveni={R\bibinitperiod}}}%
        {{hash=8cef56fce3f93d97dde7ee25db4fafd2}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Guoyan},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{3a621fae88479fd7c4e308404c4722c7}
      \strng{fullhash}{3a621fae88479fd7c4e308404c4722c7}
      \strng{bibnamehash}{3a621fae88479fd7c4e308404c4722c7}
      \strng{authorbibnamehash}{3a621fae88479fd7c4e308404c4722c7}
      \strng{authornamehash}{3a621fae88479fd7c4e308404c4722c7}
      \strng{authorfullhash}{3a621fae88479fd7c4e308404c4722c7}
      \field{sortinit}{J}
      \field{sortinithash}{fce5f8d0bd05e8d93f3dbe21c78897ca}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a method to address the challenging problem of automatic segmentation of lumbar vertebrae from CT images acquired with varying fields of view. Our method is based on cascaded 3D Fully Convolutional Networks (FCNs) consisting of a localization FCN and a segmentation FCN. More specifically, in the first step we train a regression 3D FCN (we call it “LocalizationNet”) to find the bounding box of the lumbar region. After that, a 3D U-net like FCN (we call it “SegmentationNet”) is then developed, which after training, can perform a pixel-wise multi-class segmentation to map a cropped lumber region volumetric data to its volume-wise labels. Evaluated on publicly available datasets, our method achieved an average Dice coefficient of 95.77 ± 0.81{\%} and an average symmetric surface distance of 0.37 ± 0.06 mm.}
      \field{title}{{Deep Learning based Segmentation of Lumbar Vertebrae from CT Images}}
      \field{volume}{2}
      \field{year}{2018}
      \field{pages}{94\bibrangedash 89}
      \range{pages}{-4}
      \verb{doi}
      \verb 10.29007/vt7v
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janssens, Zheng - 2018 - Deep Learning based Segmentation of Lumbar Vertebrae from CT Images.pdf:pdf
      \endverb
      \keyw{xvertseg}
    \endentry
    \entry{Khalid2018}{article}{}
      \name{author}{4}{}{%
        {{hash=2c5e5050a8f4c67d4a6004b3d1c901a8}{%
           family={Khalid},
           familyi={K\bibinitperiod},
           given={Syed\bibnamedelima Ghufran},
           giveni={S\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2fdea873c55e615949ffaf9203a9ac65}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jufen},
           giveni={J\bibinitperiod}}}%
        {{hash=fa91db7fe68f65a4947983b89f69fd65}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
        {{hash=d440ffc9f92e07c80eb4d85c1a705680}{%
           family={Zheng},
           familyi={Z\bibinitperiod},
           given={Dingchang},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Hindawi}%
      }
      \strng{namehash}{be39a08ade6e3b75dd8647df6e69a503}
      \strng{fullhash}{72e1ec7aabbd452f0dd288f8fd0db0a7}
      \strng{bibnamehash}{be39a08ade6e3b75dd8647df6e69a503}
      \strng{authorbibnamehash}{be39a08ade6e3b75dd8647df6e69a503}
      \strng{authornamehash}{be39a08ade6e3b75dd8647df6e69a503}
      \strng{authorfullhash}{72e1ec7aabbd452f0dd288f8fd0db0a7}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Introduction. Blood pressure (BP) has been a potential risk factor for cardiovascular diseases. BP measurement is one of the most useful parameters for early diagnosis, prevention, and treatment of cardiovascular diseases. At present, BP measurement mainly relies on cuff-based techniques that cause inconvenience and discomfort to users. Although some of the present prototype cuffless BP measurement techniques are able to reach overall acceptable accuracies, they require an electrocardiogram (ECG) and a photoplethysmograph (PPG) that make them unsuitable for true wearable applications. Therefore, developing a single PPG-based cuffless BP estimation algorithm with enough accuracy would be clinically and practically useful. Methods. The University of Queensland vital sign dataset (online database) was accessed to extract raw PPG signals and its corresponding reference BPs (systolic BP and diastolic BP). The online database consisted of PPG waveforms of 32 cases from whom 8133 (good quality) signal segments (5 s for each) were extracted, preprocessed, and normalised in both width and amplitude. Three most significant pulse features (pulse area, pulse rising time, and width 25{\%}) with their corresponding reference BPs were used to train and test three machine learning algorithms (regression tree, multiple linear regression (MLR), and support vector machine (SVM)). A 10-fold cross-validation was applied to obtain overall BP estimation accuracy, separately for the three machine learning algorithms. Their estimation accuracies were further analysed separately for three clinical BP categories (normotensive, hypertensive, and hypotensive). Finally, they were compared with the ISO standard for noninvasive BP device validation (average difference no greater than 5 mmHg and SD no greater than 8 mmHg). Results. In terms of overall estimation accuracy, the regression tree achieved the best overall accuracy for SBP (mean and SD of difference:-0.1 ± 6.5 mmHg) and DBP (mean and SD of difference:-0.6 ± 5.2 mmHg). MLR and SVM achieved the overall mean difference less than 5 mmHg for both SBP and DBP, but their SD of difference was {>}8 mmHg. Regarding the estimation accuracy in each BP categories, only the regression tree achieved acceptable ISO standard for SBP (-1.1 ± 5.7 mmHg) and DBP (-0.03 ± 5.6 mmHg) in the normotensive category. MLR and SVM did not achieve acceptable accuracies in any BP categories. Conclusion. This study developed and compared three machine learning algorithms to estimate BPs using PPG only and revealed that the regression tree algorithm was the best approach with overall acceptable accuracy to ISO standard for BP device validation. Furthermore, this study demonstrated that the regression tree algorithm achieved acceptable measurement accuracy only in the normotensive category, suggesting that future algorithm development for BP estimation should be more specific for different BP categories.}
      \field{issn}{20402309}
      \field{journaltitle}{Journal of Healthcare Engineering}
      \field{title}{{Blood Pressure Estimation Using Photoplethysmography Only: Comparison between Different Machine Learning Approaches}}
      \field{volume}{2018}
      \field{year}{2018}
      \verb{doi}
      \verb 10.1155/2018/1548647
      \endverb
      \verb{file}
      \verb :home/jan/Downloads/JHE2018-1548647.pdf:pdf
      \endverb
    \endentry
    \entry{Klinder2008}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=84e60d8ad5883e24a6a6cb6c16e1c48b}{%
           family={Klinder},
           familyi={K\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod}}}%
        {{hash=5014b87aaa035dcc7023d6388b29ade0}{%
           family={Wolz},
           familyi={W\bibinitperiod},
           given={Robin},
           giveni={R\bibinitperiod}}}%
        {{hash=bb37f204cceb13d141afbefd3a7d39e3}{%
           family={Lorenz},
           familyi={L\bibinitperiod},
           given={Cristian},
           giveni={C\bibinitperiod}}}%
        {{hash=4acfc6a24f6b3ceb917a95ea7cb3ce0c}{%
           family={Franz},
           familyi={F\bibinitperiod},
           given={Astrid},
           giveni={A\bibinitperiod}}}%
        {{hash=5927753e2129f489ee298ab393d1017e}{%
           family={Ostermann},
           familyi={O\bibinitperiod},
           given={J{ö}rn},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ea8cb929774858bfbc80a90e89df57ae}
      \strng{fullhash}{558622815486bcf860ef85bfcf6c6b8e}
      \strng{bibnamehash}{ea8cb929774858bfbc80a90e89df57ae}
      \strng{authorbibnamehash}{ea8cb929774858bfbc80a90e89df57ae}
      \strng{authornamehash}{ea8cb929774858bfbc80a90e89df57ae}
      \strng{authorfullhash}{558622815486bcf860ef85bfcf6c6b8e}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Including prior shape in the form of anatomical models is a well-known approach for improving segmentation results in medical images. Currently, most approaches are focused on the modeling and segmentation of individual objects. In case of object constellations, a simultaneous segmentation of the ensemble that uses not only prior knowledge of individual shapes but also additional information about spatial relations between the objects is often beneficial. In this paper, we present a two-scale framework for the modeling and segmentation of the spine as an example for object constellations. The global spine shape is expressed as a consecution of local vertebra coordinate systems while individual vertebrae are modeled as triangulated surface meshes. Adaptation is performed by attracting the model to image features but restricting the attraction to a former learned shape. With the developed approach, we obtained a segmentation accuracy of 1.0 mm in average for ten thoracic CT images improving former results. {©} 2008 Springer-Verlag Berlin Heidelberg.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{354085987X}
      \field{issn}{03029743}
      \field{number}{PART 1}
      \field{title}{{Spine segmentation using articulated shape models}}
      \field{volume}{5241 LNCS}
      \field{year}{2008}
      \field{pages}{227\bibrangedash 234}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1007/978-3-540-85988-8_28
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klinder et al. - 2008 - LNCS 5241 - Spine Segmentation Using Articulated Shape Models(2).pdf:pdf
      \endverb
    \endentry
    \entry{Korez2015}{article}{}
      \name{author}{5}{}{%
        {{hash=4742f8796ecff5fca6bed43001ce0a06}{%
           family={Korez},
           familyi={K\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod}}}%
        {{hash=10c514dc35b82df56949ff26f7b5bc8b}{%
           family={Ibragimov},
           familyi={I\bibinitperiod},
           given={B},
           giveni={B\bibinitperiod}}}%
        {{hash=fd04df0991f4cbf18f1e9328a52c35b5}{%
           family={Likar},
           familyi={L\bibinitperiod},
           given={B},
           giveni={B\bibinitperiod}}}%
        {{hash=98624dd31fb217e4eb0f667449be6328}{%
           family={Pernu{š}},
           familyi={P\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod}}}%
        {{hash=75e783dd75b0d6ee51ca3208f6bd23d5}{%
           family={Vrtovec},
           familyi={V\bibinitperiod},
           given={T},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{70ee8230ada707143a15e890ab7406ed}
      \strng{fullhash}{128d05ce87b31508839759cbdc5a1ba5}
      \strng{bibnamehash}{70ee8230ada707143a15e890ab7406ed}
      \strng{authorbibnamehash}{70ee8230ada707143a15e890ab7406ed}
      \strng{authornamehash}{70ee8230ada707143a15e890ab7406ed}
      \strng{authorfullhash}{128d05ce87b31508839759cbdc5a1ba5}
      \field{sortinit}{K}
      \field{sortinithash}{9fd838a31ba64d981e8f44562bd33f89}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Automated and semi-automated detection and segmentation of spinal and vertebral structures from computed tomography (CT) images is a challenging task due to a relatively high degree of anatomical complexity, presence of unclear boundaries and articulation of vertebrae with each other, as well as due to insufficient image spatial resolution, partial volume effects, presence of image artifacts, intensity variations and low signal-to-noise ratio. In this paper, we describe a novel framework for automated spine and vertebrae detection and segmentation from 3-D CT images. A novel optimization technique based on interpolation theory is applied to detect the location of the whole spine in the 3-D image and, using the obtained location of the whole spine, to further detect the location of individual vertebrae within the spinal column. The obtained vertebra detection results represent a robust and accurate initialization for the subsequent segmentation of individual vertebrae, which is performed by an improved shape-constrained deformable model approach. The framework was evaluated on two publicly available CT spine image databases of 50 lumbar and 170 thoracolumbar vertebrae. Quantitative comparison against corresponding reference vertebra segmentations yielded an overall mean centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6{\%} for vertebra detection, and an overall mean symmetric surface distance of 0.3 mm and Dice coefficient of 94.6{\%} for vertebra segmentation. The results indicate that by applying the proposed automated detection and segmentation framework, vertebrae can be successfully detected and accurately segmented in 3-D from CT spine images.}
      \field{issn}{1558-254X}
      \field{journaltitle}{IEEE Transactions on Medical Imaging}
      \field{month}{8}
      \field{number}{8}
      \field{title}{{A Framework for Automated Spine and Vertebrae Interpolation-Based Detection and Model-Based Segmentation}}
      \field{volume}{34}
      \field{year}{2015}
      \field{pages}{1649\bibrangedash 1662}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1109/TMI.2015.2389334
      \endverb
      \keyw{Three-Dimensional;Male;Middle Aged;Spine;Tomograp,X-Ray Computed;Young Adult,bone;computerised tomography;image segmentation;in}
    \endentry
    \entry{Laradji2019}{article}{}
      \name{author}{5}{}{%
        {{hash=63c226b42b5e888366eb8a53382941f1}{%
           family={Laradji},
           familyi={L\bibinitperiod},
           given={Issam\bibnamedelima H.},
           giveni={I\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=34b5faceb84f7cef004dc1f2f64f02b1}{%
           family={Rostamzadeh},
           familyi={R\bibinitperiod},
           given={Negar},
           giveni={N\bibinitperiod}}}%
        {{hash=bf69365008ca29e92cfd56a656c9e0bd}{%
           family={Pinheiro},
           familyi={P\bibinitperiod},
           given={Pedro\bibnamedelima O.},
           giveni={P\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=93ee720c9a96ab0f1d67654be0d6ee24}{%
           family={Vazquez},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{fullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \strng{bibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorbibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authornamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorfullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{title}{{Instance Segmentation with Point Supervision}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1906.06392
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2019 - Instance Segmentation with Point Supervision.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1906.06392
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1906.06392
      \endverb
    \endentry
    \entry{Laradji2020}{article}{}
      \name{author}{5}{}{%
        {{hash=63c226b42b5e888366eb8a53382941f1}{%
           family={Laradji},
           familyi={L\bibinitperiod},
           given={Issam\bibnamedelima H.},
           giveni={I\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=34b5faceb84f7cef004dc1f2f64f02b1}{%
           family={Rostamzadeh},
           familyi={R\bibinitperiod},
           given={Negar},
           giveni={N\bibinitperiod}}}%
        {{hash=bf69365008ca29e92cfd56a656c9e0bd}{%
           family={Pinheiro},
           familyi={P\bibinitperiod},
           given={Pedro\bibnamedelima O.},
           giveni={P\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=93ee720c9a96ab0f1d67654be0d6ee24}{%
           family={Vazquez},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{fullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \strng{bibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorbibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authornamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorfullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{020 IEEE International Conference on Image Processing (ICIP)}
      \field{number}{1}
      \field{title}{{Proposal-Based Instance Segmentation With Point Supervision}}
      \field{year}{2020}
      \field{pages}{2126\bibrangedash 2130}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/icip40778.2020.9190782
      \endverb
      \verb{eprint}
      \verb arXiv:1906.06392v1
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2020 - Proposal-Based Instance Segmentation With Point Supervision.pdf:pdf
      \endverb
    \endentry
    \entry{Laradji2018}{article}{}
      \name{author}{5}{}{%
        {{hash=63c226b42b5e888366eb8a53382941f1}{%
           family={Laradji},
           familyi={L\bibinitperiod},
           given={Issam\bibnamedelima H.},
           giveni={I\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=34b5faceb84f7cef004dc1f2f64f02b1}{%
           family={Rostamzadeh},
           familyi={R\bibinitperiod},
           given={Negar},
           giveni={N\bibinitperiod}}}%
        {{hash=bf69365008ca29e92cfd56a656c9e0bd}{%
           family={Pinheiro},
           familyi={P\bibinitperiod},
           given={Pedro\bibnamedelima O.},
           giveni={P\bibinitperiod\bibinitdelim O\bibinitperiod}}}%
        {{hash=93ee720c9a96ab0f1d67654be0d6ee24}{%
           family={Vazquez},
           familyi={V\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{fullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \strng{bibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorbibnamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authornamehash}{59dc60e92d4a3d0ac02e1fa11828c090}
      \strng{authorfullhash}{7b8896cae1766981ed5d6ab9db99c040}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detection-based method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9783030012151}
      \field{issn}{16113349}
      \field{journaltitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{title}{{Where Are the Blobs: Counting by Localization with Point Supervision}}
      \field{volume}{11206 LNCS}
      \field{year}{2018}
      \field{pages}{560\bibrangedash 576}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-030-01216-8_34
      \endverb
      \verb{eprint}
      \verb 1807.09856
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laradji et al. - 2018 - Where Are the Blobs Counting by Localization with Point Supervision.pdf:pdf
      \endverb
    \endentry
    \entry{Lessmann2018}{article}{}
      \name{author}{4}{}{%
        {{hash=8ebe9ad407b31087fe4e2d286e081609}{%
           family={Lessmann},
           familyi={L\bibinitperiod},
           given={Nikolas},
           giveni={N\bibinitperiod}}}%
        {{hash=aa138a7545c0c5a0727a81295dd50b75}{%
           family={Ginneken},
           familyi={G\bibinitperiod},
           given={Bram},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=14e924050495f963d1b6d5cdff35d00d}{%
           family={Jong},
           familyi={J\bibinitperiod},
           given={Pim\bibnamedelima A.},
           giveni={P\bibinitperiod\bibinitdelim A\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=8bb2c54c8074419186652d90d0350b66}{%
           family={I{š}gum},
           familyi={I\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{ea6484a19ae5a43b9c22126a892d259f}
      \strng{fullhash}{852f981d9d018b8f0f5bcedd770c67a9}
      \strng{bibnamehash}{ea6484a19ae5a43b9c22126a892d259f}
      \strng{authorbibnamehash}{ea6484a19ae5a43b9c22126a892d259f}
      \strng{authornamehash}{ea6484a19ae5a43b9c22126a892d259f}
      \strng{authorfullhash}{852f981d9d018b8f0f5bcedd770c67a9}
      \field{sortinit}{L}
      \field{sortinithash}{2c7981aaabc885868aba60f0c09ee20f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. The network concurrently performs multiple tasks, which are segmentation of a vertebra, regression of its anatomical label and prediction whether the vertebra is completely visible in the image, which allows to exclude incompletely visible vertebrae from further analyses. The predicted anatomical labels of the individual vertebrae are additionally refined with a maximum likelihood approach, choosing the overall most likely labeling if all detected vertebrae are taken into account. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. For vertebra segmentation, the average Dice score was 94.9 ± 2.1{\%} with an average absolute symmetric surface distance of 0.2 ± 10.1mm. The anatomical identification had an accuracy of 93{\%}, corresponding to a single case with mislabeled vertebrae. Vertebrae were classified as completely or incompletely visible with an accuracy of 97{\%}. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.}
      \field{eprinttype}{arXiv}
      \field{issn}{13618423}
      \field{journaltitle}{Medical Image Analysis}
      \field{month}{4}
      \field{title}{{Iterative fully convolutional neural networks for automatic vertebra segmentation and identification}}
      \field{volume}{53}
      \field{year}{2019}
      \field{pages}{142\bibrangedash 155}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1016/j.media.2019.02.005
      \endverb
      \verb{eprint}
      \verb 1804.04383
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lessmann et al. - 2018 - Iterative fully convolutional neural networks for automatic vertebra segmentation and identification(2).pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1804.04383 http://dx.doi.org/10.1016/j.media.2019.02.005
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1804.04383%20http://dx.doi.org/10.1016/j.media.2019.02.005
      \endverb
      \keyw{Deep learning,Iterative instance segmentation,Spine segmentation,Vertebra identification,Vertebra segmentation}
    \endentry
    \entry{Maninis2018}{article}{}
      \name{author}{4}{}{%
        {{hash=5ce839454526d07b6f1bf581db015b15}{%
           family={Maninis},
           familyi={M\bibinitperiod},
           given={K.\bibnamedelimi K.},
           giveni={K\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=b03f8faf35fa97e390bf1dad33f9975f}{%
           family={Caelles},
           familyi={C\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=4b484fe1e091cdb1954da7fda351cdaf}{%
           family={Pont-Tuset},
           familyi={P\bibinithyphendelim T\bibinitperiod},
           given={J.},
           giveni={J\bibinitperiod}}}%
        {{hash=b7b12884e34fc715e60fab1ba670bc7d}{%
           family={{Van Gool}},
           familyi={V\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{df91051199f22170e12612659ebf621c}
      \strng{fullhash}{48b8c81cc027c9abf1cbdfed87a19261}
      \strng{bibnamehash}{df91051199f22170e12612659ebf621c}
      \strng{authorbibnamehash}{df91051199f22170e12612659ebf621c}
      \strng{authornamehash}{df91051199f22170e12612659ebf621c}
      \strng{authorfullhash}{48b8c81cc027c9abf1cbdfed87a19261}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/{\~{}}cvlsegmentation/dextr/.}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781538664209}
      \field{issn}{10636919}
      \field{journaltitle}{Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
      \field{title}{{Deep Extreme Cut: From Extreme Points to Object Segmentation}}
      \field{year}{2018}
      \field{pages}{616\bibrangedash 625}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/CVPR.2018.00071
      \endverb
      \verb{eprint}
      \verb 1711.09081
      \endverb
      \verb{file}
      \verb :home/jan/Downloads/1711.09081.pdf:pdf
      \endverb
    \endentry
    \entry{McEver2020}{article}{}
      \name{author}{2}{}{%
        {{hash=3a0dac7456b26753f2e36e6cb7c40bf5}{%
           family={McEver},
           familyi={M\bibinitperiod},
           given={R.\bibnamedelimi Austin},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=661a8b53aa48439e1b916b8c28ab623c}{%
           family={Manjunath},
           familyi={M\bibinitperiod},
           given={B.\bibnamedelimi S.},
           giveni={B\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{5f8def70fe55163216096eb9632bb8c2}
      \strng{fullhash}{5f8def70fe55163216096eb9632bb8c2}
      \strng{bibnamehash}{5f8def70fe55163216096eb9632bb8c2}
      \strng{authorbibnamehash}{5f8def70fe55163216096eb9632bb8c2}
      \strng{authornamehash}{5f8def70fe55163216096eb9632bb8c2}
      \strng{authorfullhash}{5f8def70fe55163216096eb9632bb8c2}
      \field{sortinit}{M}
      \field{sortinithash}{cfd219b90152c06204fab207bc6c7cab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset $\backslash$cite{\{}everingham2010pascal{\}}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.}
      \field{eprinttype}{arXiv}
      \field{title}{{PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision}}
      \field{year}{2020}
      \verb{eprint}
      \verb 2007.05615
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McEver, Manjunath - 2020 - PCAMs Weakly Supervised Semantic Segmentation Using Point Supervision(2).pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2007.05615
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2007.05615
      \endverb
    \endentry
    \entry{Ronneberger2015}{article}{}
      \name{author}{3}{}{%
        {{hash=8e46da9de9e53ea5d37089897d69cdd9}{%
           family={Ronneberger},
           familyi={R\bibinitperiod},
           given={Olaf},
           giveni={O\bibinitperiod}}}%
        {{hash=168e84ce3582cbc6bac5e2ebc3ef8442}{%
           family={Fischer},
           familyi={F\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
        {{hash=b452a32296958371572717940f900884}{%
           family={Brox},
           familyi={B\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{f23ccc5160c62c7866172335b2c76e11}
      \strng{fullhash}{f23ccc5160c62c7866172335b2c76e11}
      \strng{bibnamehash}{f23ccc5160c62c7866172335b2c76e11}
      \strng{authorbibnamehash}{f23ccc5160c62c7866172335b2c76e11}
      \strng{authornamehash}{f23ccc5160c62c7866172335b2c76e11}
      \strng{authorfullhash}{f23ccc5160c62c7866172335b2c76e11}
      \field{sortinit}{R}
      \field{sortinithash}{da6b42bd3ab22fee61abed031ee405f7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{Original article about U-Net. Stress on low data volume needed.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{CoRR}
      \field{title}{{U-Net: Convolutional Networks for Biomedical Image Segmentation}}
      \field{volume}{abs/1505.0}
      \field{year}{2015}
      \verb{eprint}
      \verb 1505.04597
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf;:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation(2).pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1505.04597
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1505.04597
      \endverb
      \keyw{Medical,Segmentation}
    \endentry
    \entry{Sekuboyina2017}{article}{}
      \name{author}{4}{}{%
        {{hash=650d6c9b26a8d087e1f03a1b3c69fc78}{%
           family={Sekuboyina},
           familyi={S\bibinitperiod},
           given={Anjany},
           giveni={A\bibinitperiod}}}%
        {{hash=552d560aea76cdf2929016c636ea8b23}{%
           family={Valentinitsch},
           familyi={V\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=9149207edf6be6297f99f9d9e5fa02de}{%
           family={Kirschke},
           familyi={K\bibinitperiod},
           given={Jan\bibnamedelima S.},
           giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=4e43347b4618f5271c4b6d861e7824c8}{%
           family={Menze},
           familyi={M\bibinitperiod},
           given={Bjoern\bibnamedelima H.},
           giveni={B\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{5c93b3ca4dcd3979ad94eb4c85b4818b}
      \strng{fullhash}{9a8cc08213557e1fce7e909d4b2d69dc}
      \strng{bibnamehash}{5c93b3ca4dcd3979ad94eb4c85b4818b}
      \strng{authorbibnamehash}{5c93b3ca4dcd3979ad94eb4c85b4818b}
      \strng{authornamehash}{5c93b3ca4dcd3979ad94eb4c85b4818b}
      \strng{authorfullhash}{9a8cc08213557e1fce7e909d4b2d69dc}
      \field{sortinit}{S}
      \field{sortinithash}{322b1d5276f2f6c1bccdcd15920dbee6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90{\%} on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI'16. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{arXiv}
      \field{number}{1}
      \field{title}{{A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets}}
      \field{year}{2017}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{eprint}
      \verb 1703.04347
      \endverb
      \verb{file}
      \verb :home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sekuboyina et al. - 2017 - A localisation-segmentation approach for multi-label annotation of lumbar vertebrae using deep nets.pdf:pdf
      \endverb
      \keyw{xvertseg}
    \endentry
    \entry{Vocaturo2019}{article}{}
      \name{author}{3}{}{%
        {{hash=a5f26af388381662b1f69536ac72f408}{%
           family={Vocaturo},
           familyi={V\bibinitperiod},
           given={Eugenio},
           giveni={E\bibinitperiod}}}%
        {{hash=d6cc0d745a4efb416e00d1c6c28578a4}{%
           family={Zumpano},
           familyi={Z\bibinitperiod},
           given={Ester},
           giveni={E\bibinitperiod}}}%
        {{hash=aa535e3bd624001dd808d00ae0494cc6}{%
           family={Veltri},
           familyi={V\bibinitperiod},
           given={Pierangelo},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{11c089c6a5894f1553190725dca1629a}
      \strng{fullhash}{11c089c6a5894f1553190725dca1629a}
      \strng{bibnamehash}{11c089c6a5894f1553190725dca1629a}
      \strng{authorbibnamehash}{11c089c6a5894f1553190725dca1629a}
      \strng{authornamehash}{11c089c6a5894f1553190725dca1629a}
      \strng{authorfullhash}{11c089c6a5894f1553190725dca1629a}
      \field{sortinit}{V}
      \field{sortinithash}{75dd7385c90b2252c3ae853a80ca853b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The tumors on the skin are characterized by a high mortality rate. Research is attempting the automatic early diagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images. Automatic diagnostics provides a valid 'second opinion' to support physicians in deciding whether a skin lesion is a benign mole or a malignant melanoma. Determining effective detection methods to reduce the rate of error in diagnosis is a crucial challenge. Computer vision systems are characterized by several fundamental steps. Preprocessing is the first phase of detection and plays a fundamental role: the elimination of noise and irrelevant parts against the background of skin images to improve image quality. The purpose of this paper is to review the pre-processing approaches that can be used on skin cancer images. The current interest in the automatic analysis of images, is motivated by the possibility of being able to provide quantitative information on a lesion and to implement self diagnosis solutions.}
      \field{isbn}{9781538654880}
      \field{journaltitle}{Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018}
      \field{number}{December}
      \field{title}{{Image pre-processing in computer vision systems for melanoma detection}}
      \field{year}{2019}
      \field{pages}{2117\bibrangedash 2124}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/BIBM.2018.8621507
      \endverb
      \verb{file}
      \verb :home/jan/Downloads/2018{\_}Imagepre-processingincomputervisionsystemsformelanomadetection.pdf:pdf
      \endverb
      \keyw{Hair Removal,Image Enhancement,Image Pre-Processing,Image Restoration,Melanoma detection}
    \endentry
    \entry{sitk}{article}{}
      \name{author}{4}{}{%
        {{hash=5fa7bcf1bcc4a0ec9b20b5785c4509ba}{%
           family={Yaniv},
           familyi={Y\bibinitperiod},
           given={Ziv},
           giveni={Z\bibinitperiod}}}%
        {{hash=5eb812f700ffc102621214172d127e2e}{%
           family={Lowekamp},
           familyi={L\bibinitperiod},
           given={Bradley\bibnamedelima C.},
           giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=e842ecd73de47cf6df104bb197a15b82}{%
           family={Johnson},
           familyi={J\bibinitperiod},
           given={Hans\bibnamedelima J.},
           giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=ebc1460bb7721644829ec0c320a72041}{%
           family={Beare},
           familyi={B\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Journal of Digital Imaging}%
      }
      \strng{namehash}{775b747e72dacf90cbf4c6d9f3be2e42}
      \strng{fullhash}{f8f983102f58e4160b3b691085925b97}
      \strng{bibnamehash}{775b747e72dacf90cbf4c6d9f3be2e42}
      \strng{authorbibnamehash}{775b747e72dacf90cbf4c6d9f3be2e42}
      \strng{authornamehash}{775b747e72dacf90cbf4c6d9f3be2e42}
      \strng{authorfullhash}{f8f983102f58e4160b3b691085925b97}
      \field{sortinit}{Y}
      \field{sortinithash}{b8d711a035f7be9840c721c82920477e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Modern scientific endeavors increasingly require team collaborations to construct and interpret complex computational workflows. This work describes an image-analysis environment that supports the use of computational tools that facilitate reproducible research and support scientists with varying levels of software development skills. The Jupyter notebook web application is the basis of an environment that enables flexible, well-documented, and reproducible workflows via literate programming. Image-analysis software development is made accessible to scientists with varying levels of programming experience via the use of the SimpleITK toolkit, a simplified interface to the Insight Segmentation and Registration Toolkit. Additional features of the development environment include user friendly data sharing using online data repositories and a testing framework that facilitates code maintenance. SimpleITK provides a large number of examples illustrating educational and research-oriented image analysis workflows for free download from GitHub under an Apache 2.0 license: github.com/InsightSoftwareConsortium/SimpleITK-Notebooks.}
      \field{issn}{1618727X}
      \field{journaltitle}{Journal of Digital Imaging}
      \field{number}{3}
      \field{title}{{SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research}}
      \field{volume}{31}
      \field{year}{2018}
      \field{pages}{290\bibrangedash 303}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/s10278-017-0037-8
      \endverb
      \verb{file}
      \verb :home/jan/Downloads/Yaniv2018{\_}Article{\_}SimpleITKImage-AnalysisNoteboo.pdf:pdf
      \endverb
      \keyw{Image analysis,Open-source software,Python,R,Registration,Segmentation}
    \endentry
    \entry{Zukic2014}{article}{}
      \name{author}{6}{}{%
        {{hash=e9d7ad0bd2ad273709e2f190c1408bb2}{%
           family={Zuki{ć}},
           familyi={Z\bibinitperiod},
           given={D{ž}enan},
           giveni={D\bibinitperiod}}}%
        {{hash=04c9f25e03c7141244164f0ddd01b547}{%
           family={Vlas{á}k},
           familyi={V\bibinitperiod},
           given={Ale{š}},
           giveni={A\bibinitperiod}}}%
        {{hash=c9a59ad74e0ba8e8c16df239e9af56b6}{%
           family={Egger},
           familyi={E\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod}}}%
        {{hash=d30540f2cd70c44652c4b53589f12658}{%
           family={Hoř{í}nek},
           familyi={H\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=9e92eb7346264cabbf7b856ed6e7a8df}{%
           family={Nimsky},
           familyi={N\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=f4195dc26f57b130791772dba2a6d789}{%
           family={Kolb},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e8f4de5ce2cfb7b1d05d1169e6fd6679}
      \strng{fullhash}{15889e9aebe1180e001e70f77701565b}
      \strng{bibnamehash}{e8f4de5ce2cfb7b1d05d1169e6fd6679}
      \strng{authorbibnamehash}{e8f4de5ce2cfb7b1d05d1169e6fd6679}
      \strng{authornamehash}{e8f4de5ce2cfb7b1d05d1169e6fd6679}
      \strng{authorfullhash}{15889e9aebe1180e001e70f77701565b}
      \field{sortinit}{Z}
      \field{sortinithash}{156173bd08b075d7295bc3e0f4735a04}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Computer Graphics Forum}
      \field{title}{{Robust Detection and Segmentation for Diagnosis of Vertebral Diseases Using Routine MR Images}}
      \field{volume}{33}
      \field{year}{2014}
      \verb{doi}
      \verb 10.1111/cgf.12343
      \endverb
    \endentry
  \enddatalist
  \missing{Laradji}
  \missing{Laradji2020a}
\endrefsection
\endinput


<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<b:Sources xmlns:b="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" xmlns="http://schemas.openxmlformats.org/officeDocument/2006/bibliography" SelectedStyle="">
    <b:Source>
        <b:Year>2015</b:Year>
        <b:Volume>34</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>A Framework for Automated Spine and Vertebrae Interpolation-Based Detection and Model-Based Segmentation</b:Title>
        <b:BIBTEX_Abstract>Automated and semi-automated detection and segmentation of spinal and vertebral structures from computed tomography (CT) images is a challenging task due to a relatively high degree of anatomical complexity, presence of unclear boundaries and articulation of vertebrae with each other, as well as due to insufficient image spatial resolution, partial volume effects, presence of image artifacts, intensity variations and low signal-to-noise ratio. In this paper, we describe a novel framework for automated spine and vertebrae detection and segmentation from 3-D CT images. A novel optimization technique based on interpolation theory is applied to detect the location of the whole spine in the 3-D image and, using the obtained location of the whole spine, to further detect the location of individual vertebrae within the spinal column. The obtained vertebra detection results represent a robust and accurate initialization for the subsequent segmentation of individual vertebrae, which is performed by an improved shape-constrained deformable model approach. The framework was evaluated on two publicly available CT spine image databases of 50 lumbar and 170 thoracolumbar vertebrae. Quantitative comparison against corresponding reference vertebra segmentations yielded an overall mean centroid-to-centroid distance of 1.1 mm and Dice coefficient of 83.6% for vertebra detection, and an overall mean symmetric surface distance of 0.3 mm and Dice coefficient of 94.6% for vertebra segmentation. The results indicate that by applying the proposed automated detection and segmentation framework, vertebrae can be successfully detected and accurately segmented in 3-D from CT spine images.</b:BIBTEX_Abstract>
        <b:Tag>Korez2015</b:Tag>
        <b:BIBTEX_KeyWords>bone;computerised tomography;image segmentation;interpolation;medical image processing;neurophysiology;object detection;optimisation;physiological models;automated spine interpolation-based detection;automated vertebrae interpolation-based detection;automated spinal structure segmentation;automated vertebral structure segmentation;computed tomography;signal-to-noise ratio;3D CT image spatial resolution;optimization technique;spinal column;improved shape-constrained deformable model approach;thoracolumbar vertebrae;Dice coefficient;distance 1.1 mm;distance 0.3 mm;Interpolation;Image segmentation;Shape;Computed tomography;Optimization;Three-dimensional displays;Polynomials;Computed tomography;deformable models;image segmentation;interpolation theory;object detection;spine;vertebra;Adult;Algorithms;Female;Humans;Imaging, Three-Dimensional;Male;Middle Aged;Spine;Tomography, X-Ray Computed;Young Adult</b:BIBTEX_KeyWords>
        <b:DOI>10.1109/TMI.2015.2389334</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Korez</b:Last>
                        <b:First>R.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Ibragimov</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Likar</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pernuš</b:Last>
                        <b:First>F.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vrtovec</b:Last>
                        <b:First>T.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>1649-1662</b:Pages>
        <b:Month>Aug</b:Month>
        <b:JournalName>IEEE Transactions on Medical Imaging</b:JournalName>
        <b:Number>8</b:Number>
        <b:StandardNumber> ISSN: 1558-254X</b:StandardNumber>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>techreport</b:BIBTEX_Entry>
        <b:SourceType>Report</b:SourceType>
        <b:Title>Vertebrae Localization in Pathological Spine CT via Dense Classification from Sparse Annotations</b:Title>
        <b:BIBTEX_Abstract>Accurate localization and identification of vertebrae in spinal imaging is crucial for the clinical tasks of diagnosis, surgical planning, and post-operative assessment. The main difficulties for automatic methods arise from the frequent presence of abnormal spine curvature, small field of view, and image artifacts caused by surgical implants. Many previous methods rely on parametric models of appearance and shape whose performance can substantially degrade for pathological cases. We propose a robust localization and identification algorithm which builds upon supervised classification forests and avoids an explicit para-metric model of appearance. We overcome the tedious requirement for dense annotations by a semi-automatic labeling strategy. Sparse centroid annotations are transformed into dense probabilistic labels which capture the inherent identification uncertainty. Using the dense labels, we learn a discriminative centroid classifier based on local and contextual intensity features which is robust to typical characteristics of spinal pathologies and image artifacts. Extensive evaluation is performed on a challenging dataset of 224 spine CT scans of patients with varying pathologies including high-grade scoliosis, kyphosis, and presence of surgical implants. Additionally, we test our method on a heterogeneous dataset of another 200, mostly abdominal, CTs. Quantitative evaluation is carried out with respect to localization errors and identification rates, and compared to a recently proposed method. Our approach is efficient and outperforms state-of-the-art on pathological cases.</b:BIBTEX_Abstract>
        <b:Tag>Glockera</b:Tag>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Glocker</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zikic</b:Last>
                        <b:First>D.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Konukoglu</b:Last>
                        <b:First>E.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Haynor</b:Last>
                        <b:Middle>R.</b:Middle>
                        <b:First>D.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Criminisi</b:Last>
                        <b:First>A.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:ThesisType>Tech. rep.</b:ThesisType>
    </b:Source>
    <b:Source>
        <b:Year>2015</b:Year>
        <b:Volume>10</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Fully Automatic Localization and Segmentation of 3D Vertebral Bodies from CT/MR Images via a Learning-Based Method</b:Title>
        <b:BIBTEX_Abstract>In this paper, we address the problems of fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images. We propose a learning-based, unified random forest regression and classification framework to tackle these two problems. More specifically, in the first stage, the localization of 3D vertebral bodies is solved with random forest regression where we aggregate the votes from a set of randomly sampled image patches to get a probability map of the center of a target vertebral body in a given image. The resultant probability map is then further regularized by Hidden Markov Model (HMM) to eliminate potential ambiguity caused by the neighboring vertebral bodies. The output from the first stage allows us to define a region of interest (ROI) for the segmentation step, where we use random forest classification to estimate the likelihood of a voxel in the ROI being foreground or background. The estimated likelihood is combined with the prior probability, which is learned from a set of training data, to get the posterior probability of the voxel. The segmentation of the target vertebral body is then done by a binary thresholding of the estimated probability. We evaluated the present approach on two openly available datasets: 1) 3D T2-weighted spine MR images from 23 patients and 2) 3D spine CT images from 10 patients. Taking manual segmentation as the ground truth (each MR image contains at least 7 vertebral bodies from T11 to L5 and each CT image contains 5 vertebral bodies from L1 to L5), we evaluated the present approach with leave-one-out experiments. Specifically, for the T2-weighted MR images, we achieved for localization a mean error of 1.6 mm, and for segmentation a mean Dice metric of 88.7% and a mean surface distance of 1.5 mm, respectively. For the CT images we achieved for localization a mean error of 1.9 mm, and for segmentation a mean Dice metric of 91.0% and a mean surface distance of 0.9 mm, respectively.</b:BIBTEX_Abstract>
        <b:Tag>10.1371/journal.pone.0143327</b:Tag>
        <b:Publisher>Public Library of Science</b:Publisher>
        <b:URL>https://doi.org/10.1371/journal.pone.0143327</b:URL>
        <b:DOI>10.1371/journal.pone.0143327</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Chu</b:Last>
                        <b:First>Chengwen</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Belavý</b:Last>
                        <b:Middle>L.</b:Middle>
                        <b:First>Daniel</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Armbrecht</b:Last>
                        <b:First>Gabriele</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Bansmann</b:Last>
                        <b:First>Martin</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Felsenberg</b:Last>
                        <b:First>Dieter</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zheng</b:Last>
                        <b:First>Guoyan</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>1-22</b:Pages>
        <b:Month>11</b:Month>
        <b:JournalName>PLOS ONE</b:JournalName>
        <b:Number>11</b:Number>
    </b:Source>
    <b:Source>
        <b:Year>2015</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Whatś the Point: Semantic Segmentation with Point Supervision</b:Title>
        <b:BIBTEX_Abstract>The semantic image segmentation task presents a trade-off between test time accuracy and training-time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain, image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9% mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.</b:BIBTEX_Abstract>
        <b:Tag>Bearman2015</b:Tag>
        <b:URL>http://arxiv.org/abs/1506.02106</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Bearman</b:Last>
                        <b:First>Amy</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Russakovsky</b:Last>
                        <b:First>Olga</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Ferrari</b:Last>
                        <b:First>Vittorio</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Fei-Fei</b:Last>
                        <b:First>Li</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>jun</b:Month>
    </b:Source>
    <b:Source>
        <b:Year>2019</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Where are the Masks: Instance Segmentation with Image-level Supervision</b:Title>
        <b:BIBTEX_Abstract>A major obstacle in instance segmentation is that existing methods often need many per-pixel labels in order to be effective. These labels require large human effort and for certain applications, such labels are not readily available. To address this limitation, we propose a novel framework that can effectively train with image-level labels, which are significantly cheaper to acquire. For instance, one can do an internet search for the term "car" and obtain many images where a car is present with minimal effort. Our framework consists of two stages: (1) train a classifier to generate pseudo masks for the objects of interest; (2) train a fully supervised Mask R-CNN on these pseudo masks. Our two main contribution are proposing a pipeline that is simple to implement and is amenable to different segmentation methods; and achieves new state-of-the-art results for this problem setup. Our results are based on evaluating our method on PASCAL VOC 2012, a standard dataset for weakly supervised methods, where we demonstrate major performance gains compared to existing methods with respect to mean average precision.</b:BIBTEX_Abstract>
        <b:Tag>Laradji2019a</b:Tag>
        <b:URL>http://arxiv.org/abs/1907.01430</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Laradji</b:Last>
                        <b:Middle>H.</b:Middle>
                        <b:First>Issam</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vazquez</b:Last>
                        <b:First>David</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Schmidt</b:Last>
                        <b:First>Mark</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>jul</b:Month>
    </b:Source>
    <b:Source>
        <b:Year>2019</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Instance Segmentation with Point Supervision</b:Title>
        <b:BIBTEX_Abstract>Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.</b:BIBTEX_Abstract>
        <b:Tag>Laradji2019</b:Tag>
        <b:URL>http://arxiv.org/abs/1906.06392</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Laradji</b:Last>
                        <b:Middle>H.</b:Middle>
                        <b:First>Issam</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Rostamzadeh</b:Last>
                        <b:First>Negar</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pinheiro</b:Last>
                        <b:Middle>O.</b:Middle>
                        <b:First>Pedro</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vazquez</b:Last>
                        <b:First>David</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Schmidt</b:Last>
                        <b:First>Mark</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>jun</b:Month>
    </b:Source>
    <b:Source>
        <b:Year>2014</b:Year>
        <b:Volume>33</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Shape Representation for Efficient Landmark-Based Segmentation in 3-D</b:Title>
        <b:BIBTEX_Abstract>In this paper, we propose a novel approach to landmark-based shape representation that is based on transportation theory, where landmarks are considered as sources and destinations, all possible landmark connections as roads, and established landmark connections as goods transported via these roads. Landmark connections, which are selectively established, are identified through their statistical properties describing the shape of the object of interest, and indicate the least costly roads for transporting goods from sources to destinations. From such a perspective, we introduce three novel shape representations that are combined with an existing landmark detection algorithm based on game theory. To reduce computational complexity, which results from the extension from 2-D to 3-D segmentation, landmark detection is augmented by a concept known in game theory as strategy dominance. The novel shape representations, game-theoretic landmark detection and strategy dominance are combined into a segmentation framework that was evaluated on 3-D computed tomography images of lumbar vertebrae and femoral heads. The best shape representation yielded symmetric surface distance of 0.75 mm and 1.11 mm, and Dice coefficient of 93.6% and 96.2% for lumbar vertebrae and femoral heads, respectively. By applying strategy dominance, the computational costs were further reduced for up to three times.</b:BIBTEX_Abstract>
        <b:Tag>Ibragimov2014</b:Tag>
        <b:BIBTEX_KeyWords>bone;computerised tomography;diseases;game theory;image segmentation;medical image processing;orthopaedics;landmark-based 3D segmentation;land-mark-based shape representation;transportation theory;landmark connections;roads;good transportation;statistical properties;landmark detection algorithm;game theory;computational complexity;2D segmentation;game-theoretic landmark detection;3D computed tomography imaging;lumbar vertebrae;femoral heads;symmetric surface distance;Dice coefficient;Shape;Image segmentation;Training;Three-dimensional displays;Roads;Materials;Graph theory;landmark-based segmentation;shape representation;transportation theory;Algorithms;Anatomic Landmarks;Femur Head;Game Theory;Humans;Imaging, Three-Dimensional;Lumbar Vertebrae;Tomography, X-Ray Computed</b:BIBTEX_KeyWords>
        <b:DOI>10.1109/TMI.2013.2296976</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Ibragimov</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Likar</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pernuš</b:Last>
                        <b:First>F.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vrtovec</b:Last>
                        <b:First>T.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>861-874</b:Pages>
        <b:Month>April</b:Month>
        <b:JournalName>IEEE Transactions on Medical Imaging</b:JournalName>
        <b:Number>4</b:Number>
        <b:StandardNumber> ISSN: 1558-254X</b:StandardNumber>
    </b:Source>
    <b:Source>
        <b:Year>2018</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Iterative fully convolutional neural networks for automatic vertebra segmentation and identification</b:Title>
        <b:BIBTEX_Abstract>Precise segmentation and anatomical identification of the vertebrae provides the basis for automatic analysis of the spine, such as detection of vertebral compression fractures or other abnormalities. Most dedicated spine CT and MR scans as well as scans of the chest, abdomen or neck cover only part of the spine. Segmentation and identification should therefore not rely on the visibility of certain vertebrae or a certain number of vertebrae. We propose an iterative instance segmentation approach that uses a fully convolutional neural network to segment and label vertebrae one after the other, independently of the number of visible vertebrae. This instance-by-instance segmentation is enabled by combining the network with a memory component that retains information about already segmented vertebrae. The network iteratively analyzes image patches, using information from both image and memory to search for the next vertebra. To efficiently traverse the image, we include the prior knowledge that the vertebrae are always located next to each other, which is used to follow the vertebral column. This method was evaluated with five diverse datasets, including multiple modalities (CT and MR), various fields of view and coverages of different sections of the spine, and a particularly challenging set of low-dose chest CT scans. The proposed iterative segmentation method compares favorably with state-of-the-art methods and is fast, flexible and generalizable.</b:BIBTEX_Abstract>
        <b:Tag>Lessmann2018</b:Tag>
        <b:BIBTEX_KeyWords>Spine segmentation</b:BIBTEX_KeyWords>
        <b:URL>http://arxiv.org/abs/1804.04383 http://dx.doi.org/10.1016/j.media.2019.02.005</b:URL>
        <b:DOI>10.1016/j.media.2019.02.005</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Lessmann</b:Last>
                        <b:First>Nikolas</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>van Ginneken</b:Last>
                        <b:First>Bram</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>de Jong and Pim</b:Last>
                        <b:First>A.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Išgum</b:Last>
                        <b:First>Ivana</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>apr</b:Month>
    </b:Source>
    <b:Source>
        <b:Year>2020</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Proposal-Based Instance Segmentation With Point Supervision</b:Title>
        <b:BIBTEX_Abstract>Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.</b:BIBTEX_Abstract>
        <b:Tag>Laradji2020a</b:Tag>
        <b:DOI>10.1109/icip40778.2020.9190782</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Laradji</b:Last>
                        <b:Middle>H.</b:Middle>
                        <b:First>Issam</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Rostamzadeh</b:Last>
                        <b:First>Negar</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Pinheiro</b:Last>
                        <b:Middle>O.</b:Middle>
                        <b:First>Pedro</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vazquez</b:Last>
                        <b:First>David</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Schmidt</b:Last>
                        <b:First>Mark</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Pages>2126-2130</b:Pages>
        <b:Number>1</b:Number>
    </b:Source>
    <b:Source>
        <b:Year>2019</b:Year>
        <b:Volume>20</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Lumbar muscle and vertebral bodies segmentation of chemical shift encoding-based water-fat MRI: The reference database MyoSegmenTUM spine</b:Title>
        <b:Tag>Burian2019</b:Tag>
        <b:DOI>10.1186/s12891-019-2528-x</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Burian</b:Last>
                        <b:First>Egon</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Rohrmeier</b:Last>
                        <b:First>Alexander</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Schlaeger</b:Last>
                        <b:First>Sarah</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Dieckmeyer</b:Last>
                        <b:First>Michael</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Diefenbach</b:Last>
                        <b:First>Maximilian</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Syväri</b:Last>
                        <b:First>Jan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Klupp</b:Last>
                        <b:First>Elisabeth</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Weidlich</b:Last>
                        <b:First>Dominik</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Zimmer</b:Last>
                        <b:First>Claus</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Rummeny</b:Last>
                        <b:First>Ernst</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Karampinos</b:Last>
                        <b:First>Dimitrios</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Kirschke</b:Last>
                        <b:First>Jan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Baum</b:Last>
                        <b:First>Thomas</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>04</b:Month>
        <b:JournalName>BMC Musculoskeletal Disorders</b:JournalName>
    </b:Source>
    <b:Source>
        <b:Year>2020</b:Year>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>A Weakly Supervised Consistency-based Learning Method for COVID-19 Segmentation in CT Images</b:Title>
        <b:BIBTEX_Abstract>Coronavirus Disease 2019 (COVID-19) has spread aggressively across the world causing an existential health crisis. Thus, having a system that automatically detects COVID-19 in tomography (CT) images can assist in quantifying the severity of the illness. Unfortunately, labelling chest CT scans requires significant domain expertise, time, and effort. We address these labelling challenges by only requiring point annotations, a single pixel for each infected region on a CT image. This labeling scheme allows annotators to label a pixel in a likely infected region, only taking 1-3 seconds, as opposed to 10-15 seconds to segment a region. Conventionally, segmentation models train on point-level annotations using the cross-entropy loss function on these labels. However, these models often suffer from low precision. Thus, we propose a consistency-based (CB) loss function that encourages the output predictions to be consistent with spatial transformations of the input images. The experiments on 3 open-source COVID-19 datasets show that this loss function yields significant improvement over conventional point-level loss functions and almost matches the performance of models trained with full supervision with much less human effort. Code is available at: backslashurlhttps://github.com/IssamLaradji/covid19weaksupervision.</b:BIBTEX_Abstract>
        <b:Tag>Laradji2020</b:Tag>
        <b:URL>http://arxiv.org/abs/2007.02180</b:URL>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Laradji</b:Last>
                        <b:First>Issam</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Rodriguez</b:Last>
                        <b:First>Pau</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Mañas</b:Last>
                        <b:First>Oscar</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Lensink</b:Last>
                        <b:First>Keegan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Law</b:Last>
                        <b:First>Marco</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Kurzman</b:Last>
                        <b:First>Lironne</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Parker</b:Last>
                        <b:First>William</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vazquez</b:Last>
                        <b:First>David</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Nowrouzezahrai</b:Last>
                        <b:First>Derek</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>jul</b:Month>
    </b:Source>
    <b:Source>
        <b:Year>2014</b:Year>
        <b:Volume>33</b:Volume>
        <b:BIBTEX_Entry>article</b:BIBTEX_Entry>
        <b:SourceType>JournalArticle</b:SourceType>
        <b:Title>Robust Detection and Segmentation for Diagnosis of Vertebral Diseases Using Routine MR Images</b:Title>
        <b:Tag>Zukic2014</b:Tag>
        <b:DOI>10.1111/cgf.12343</b:DOI>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Zukić</b:Last>
                        <b:First>Dženan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Vlasák</b:Last>
                        <b:First>Aleš</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Egger</b:Last>
                        <b:First>Jan</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Hořínek</b:Last>
                        <b:First>Daniel</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Nimsky</b:Last>
                        <b:First>Christopher</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Kolb</b:Last>
                        <b:First>Andreas</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:Month>03</b:Month>
        <b:JournalName>Computer Graphics Forum</b:JournalName>
    </b:Source>
    <b:Source>
        <b:BIBTEX_Entry>techreport</b:BIBTEX_Entry>
        <b:SourceType>Report</b:SourceType>
        <b:Title>Automatic Localization and Identification of Vertebrae in Arbitrary Field-of-View CT Scans</b:Title>
        <b:BIBTEX_Abstract>This paper presents a new method for automatic localiza-tion and identification of vertebrae in arbitrary field-of-view CT scans. No assumptions are made about which section of the spine is visible or to which extent. Thus, our approach is more general than previous work while being computationally efficient. Our algorithm is based on regression forests and probabilistic graphical models. The discriminative, regression part aims at roughly detecting the visible part of the spine. Accurate localization and identification of individual vertebrae is achieved through a generative model capturing spinal shape and appearance. The system is evaluated quantitatively on 200 CT scans, the largest dataset reported for this purpose. We obtain an overall median localization error of less than 6mm, with an identification rate of 81%.</b:BIBTEX_Abstract>
        <b:Tag>Glocker</b:Tag>
        <b:Author>
            <b:Author>
                <b:NameList>
                    <b:Person>
                        <b:Last>Glocker</b:Last>
                        <b:First>B.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Feulner</b:Last>
                        <b:First>J.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Criminisi</b:Last>
                        <b:First>A.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Haynor</b:Last>
                        <b:Middle>R.</b:Middle>
                        <b:First>D.</b:First>
                    </b:Person>
                    <b:Person>
                        <b:Last>Konukoglu</b:Last>
                        <b:First>E.</b:First>
                    </b:Person>
                </b:NameList>
            </b:Author>
        </b:Author>
        <b:ThesisType>Tech. rep.</b:ThesisType>
    </b:Source>
</b:Sources>

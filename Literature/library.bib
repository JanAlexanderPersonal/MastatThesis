Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ronneberger2015,
annote = {Original article about U-Net.

Stress on low data volume needed.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/1505.04597.pdf:pdf},
journal = {CoRR},
keywords = {Medical,Segmentation},
mendeley-tags = {Medical,Segmentation},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {abs/1505.0},
year = {2015}
}
@article{DasCLuciaMS2017,
abstract = {Multicolored proteins have allowed the color coding of cancer cells growing in vivo and enabled the distinction of host from tumor with single-cell resolution. Non-invasive imaging with fluorescent proteins enabled follow the dynamics of metastatic cancer to be followed in real time in individual animals. Non-invasive imaging of cancer cells expressing fluorescent proteins has enabled the real-time determination of efficacy of candidate antitumor and antimetastatic agents in mouse models. The use of fluorescent proteins to differentially label cancer cells in the nucleus and cytoplasm allow visualization of the nuclear–cytoplasmic dynamics of cancer cells in vivo, mitosis, apoptosis, cell-cycle position and differential behavior of nucleus and cytoplasm such as occurs during cancer-cell deformation and extravasation. Recent applications of the technology described here include linking fluorescent proteins with cell-cycle-specific proteins (FUCCI) such that the cells change color from red to green as they transit from G1 to S phases. With the macro and micro imaging technologies described here, essentially any in vivo process can be imaged, enabling the new field of in vivo cell biology using fluorescent proteins.},
author = {doi:10.1016/j.brs.2016.03.010 {Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A}, Wirecki TS. The Clinical TMS Society Consensus Review and Treatment Recommendations for TMS Therapy for Major Depressive Disorder. Brain Stimul. 2016;9(3):336-346.},
doi = {10.1088/0031-9155/61/8/3009.3D},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perera T, George MS, Grammer G, Janicak PG, Pascual-Leone A - 2017 - 3D–2D image registration for target localization in spine surgery.pdf:pdf},
isbn = {2163684814},
journal = {Physiology {\&} behavior},
keywords = {determination,protein crystallography,protein data bank,r -factor,resolution,restraints,structure,structure interpretation,structure quality,structure refinement,structure validation,ultrasound},
mendeley-tags = {ultrasound},
number = {1},
pages = {139--148},
title = {{3D–2D image registration for target localization in spine surgery: investigation of similarity metrics providing robustness to content mismatch}},
volume = {176},
year = {2017}
}
@article{Vania2019,
abstract = {There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed a fully automatic segmentation method to segment the spine from CT images, and we compared our segmentation results with reference segmentations obtained by well-known methods. We use a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94$\backslash$$\backslash${\%}), Jaccard index (93$\backslash$$\backslash${\%}), volumetric similarity (96$\backslash$$\backslash${\%}), sensitivity (97$\backslash$$\backslash${\%}), specificity (99$\backslash$$\backslash${\%}), precision (over segmentation 8.3 and under segmentation 2.6), accuracy (99$\backslash$$\backslash${\%}), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.HighlightsA method to enhance the accuracy of spine segmentation from CT data was proposed.The proposed method uses Convolutional Neural Network via redundant generation of class labels.Experiments show the segmentation accuracy has been enhanced.},
author = {Vania, Malinda and Mureja, Dawit and Lee, Deukhee},
doi = {10.1016/j.jcde.2018.05.002},
file = {:home/jan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vania, Mureja, Lee - 2019 - Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of c.pdf:pdf},
issn = {2288-5048},
journal = {Journal of Computational Design and Engineering},
number = {2},
pages = {224--232},
title = {{Automatic spine segmentation from CT images using Convolutional Neural Network via redundant generation of class labels}},
url = {https://doi.org/10.1016/j.jcde.2018.05.002},
volume = {6},
year = {2019}
}
@article{McEver2020,
abstract = {Current state of the art methods for generating semantic segmentation rely heavily on a large set of images that have each pixel labeled with a class of interest label or background. Coming up with such labels, especially in domains that require an expert to do annotations, comes at a heavy cost in time and money. Several methods have shown that we can learn semantic segmentation from less expensive image-level labels, but the effectiveness of point level labels, a healthy compromise between all pixels labelled and none, still remains largely unexplored. This paper presents a novel procedure for producing semantic segmentation from images given some point level annotations. This method includes point annotations in the training of a convolutional neural network (CNN) for producing improved localization and class activation maps. Then, we use another CNN for predicting semantic affinities in order to propagate rough class labels and create pseudo semantic segmentation labels. Finally, we propose training a CNN that is normally fully supervised using our pseudo labels in place of ground truth labels, which further improves performance and simplifies the inference process by requiring just one CNN during inference rather than two. Our method achieves state of the art results for point supervised semantic segmentation on the PASCAL VOC 2012 dataset $\backslash$cite{\{}everingham2010pascal{\}}, even outperforming state of the art methods for stronger bounding box and squiggle supervision.},
archivePrefix = {arXiv},
arxivId = {2007.05615},
author = {McEver, R. Austin and Manjunath, B. S.},
eprint = {2007.05615},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/McEver.pdf:pdf},
title = {{PCAMs: Weakly Supervised Semantic Segmentation Using Point Supervision}},
url = {http://arxiv.org/abs/2007.05615},
year = {2020}
}
@inproceedings{Nett2008,
author = {Nett, Brian and Tang, Jie and Leng, Shuai and Chen, Guang-Hong},
booktitle = {Medical Imaging 2008: Physics of Medical Imaging},
doi = {10.1117/12.771294},
editor = {Hsieh, Jiang and Samei, Ehsan},
publisher = {SPIE},
title = {{Tomosynthesis via total variation minimization reconstruction and prior image constrained compressed sensing ({\{}PICCS{\}}) on a C-arm system}},
url = {https://doi.org/10.1117/12.771294},
year = {2008}
}
@article{Chan2020,
abstract = {Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis.},
archivePrefix = {arXiv},
arxivId = {1912.11186},
author = {Chan, Lyndon and Hosseini, Mahdi S. and Plataniotis, Konstantinos N.},
doi = {10.1007/s11263-020-01373-4},
eprint = {1912.11186},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/Chan.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural network,Deep learning,Digital pathology,Natural imaging,Satellite imaging,Self-supervised Learning,Weakly supervised semantic segmentation},
title = {{A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains}},
year = {2020}
}
@article{Laradji2020,
abstract = {Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.06392v1},
author = {Laradji, Issam H. and Rostamzadeh, Negar and Pinheiro, Pedro O. and Vazquez, David and Schmidt, Mark},
doi = {10.1109/icip40778.2020.9190782},
eprint = {arXiv:1906.06392v1},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/Laradji.pdf:pdf},
number = {1},
pages = {2126--2130},
title = {{Proposal-Based Instance Segmentation With Point Supervision}},
year = {2020}
}
@article{Hyun2018,
author = {Hyun, Chang Min and Kim, Hwa Pyung and Lee, Sung Min and Lee, Sungchul and Seo, Jin Keun},
doi = {10.1088/1361-6560/aac71a},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/Hyun{\_}2018{\_}Phys.{\_}Med.{\_}Biol.{\_}63{\_}135007.pdf:pdf},
journal = {Physics in Medicine {\&} Biology},
keywords = {Reconstruction,Tomography},
mendeley-tags = {Reconstruction,Tomography},
number = {13},
pages = {135007},
publisher = {{\{}IOP{\}} Publishing},
title = {{Deep learning for undersampled {\{}MRI{\}} reconstruction}},
url = {https://doi.org/10.1088/1361-6560/aac71a},
volume = {63},
year = {2018}
}
@article{Selvaraju2016,
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
eprint = {1610.02391},
journal = {CoRR},
title = {{Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391},
volume = {abs/1610.0},
year = {2016}
}
@article{Huang2013,
author = {Huang, Jing and Zhang, Yunwan and Ma, Jianhua and Zeng, Dong and Bian, Zhaoying and Niu, Shanzhou and Feng, Qianjin and Liang, Zhengrong and Chen, Wufan},
doi = {10.1371/journal.pone.0079709},
editor = {Wang, Ge},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/journal.pone.0079709.pdf:pdf},
journal = {{\{}PLoS{\}} {\{}ONE{\}}},
keywords = {Tomography},
mendeley-tags = {Tomography},
month = {nov},
number = {11},
pages = {e79709},
publisher = {Public Library of Science ({\{}PLoS{\}})},
title = {{Iterative Image Reconstruction for Sparse-View {\{}CT{\}} Using Normal-Dose Image Induced Total Variation Prior}},
url = {https://doi.org/10.1371/journal.pone.0079709},
volume = {8},
year = {2013}
}
@article{Cicek2016,
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
eprint = {1606.06650},
file = {:home/jan/Documents/Projects/Thesis{\_}literature/3D{\_}U-Net{\_}Learning{\_}Dense{\_}Volumetric{\_}Segmentation{\_}fr.pdf:pdf},
journal = {CoRR},
keywords = {Tomography},
mendeley-tags = {Tomography},
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
url = {http://arxiv.org/abs/1606.06650},
volume = {abs/1606.0},
year = {2016}
}
